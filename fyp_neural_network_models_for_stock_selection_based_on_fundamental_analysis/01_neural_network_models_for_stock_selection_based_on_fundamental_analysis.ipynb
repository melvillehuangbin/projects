{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "\n",
    "[Paper reference](https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8861550&tag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d1c10c007e8c>:11: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:99% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "display(HTML(\"<style>.container { width:99% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_file(file_path, file_name, data_to_dump):\n",
    "    import pickle\n",
    "    with open(f'{file_path}/{file_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(data_to_dump, file)\n",
    "        \n",
    "def load_file(file_path, file_name):\n",
    "    import pickle\n",
    "    with open(f'{file_path}/{file_name}.pkl', 'rb') as file:\n",
    "        output_file = pickle.load(file)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '01_Paper_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== save files ===============\n",
    "dump_file(file_path=folder_path, file_name='stock_tickers', data_to_dump=stock_ticker)\n",
    "dump_file(file_path=folder_path, file_name='stock_tickers_formatted_before_map', data_to_dump=stock_ticker_formatted)\n",
    "dump_file(file_path=folder_path, file_name='stock_tickers_formatted', data_to_dump=stock_ticker_formatted)\n",
    "dump_file(file_path=folder_path, file_name='tickers_map', data_to_dump=tickers_map)\n",
    "dump_file(file_path=folder_path, file_name='financial_data_by_tickers', data_to_dump=financial_data_by_tickers)\n",
    "dump_file(file_path=folder_path, file_name='financial_data_by_tickers_timeframe_final', data_to_dump=financial_data_by_tickers_timeframe_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_file(file_path=folder_path, file_name='financial_data_filled', data_to_dump=financial_data_filled)\n",
    "dump_file(file_path=folder_path, file_name='financial_data_relative_return', data_to_dump=financial_data_relative_return)\n",
    "dump_file(file_path=folder_path, file_name='financial_data_trend_standardized', data_to_dump=financial_data_trend_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting the dataset required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_ticker = load_file(file_path=folder_path, file_name='stock_tickers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_ticker_formatted_before_map = []\n",
    "for st in stock_ticker:\n",
    "    stock_ticker_formatted_before_map.append(st.split('-')[0].split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing the tickers to download the datasets, map to the correct ticker in request url\n",
    "\n",
    "stock_ticker_formatted = deepcopy(stock_ticker_formatted_before_map)\n",
    "\n",
    "tickers_map = {1: 'AMR', 7: 'ABI_0', 52: 'ABK', 63: 'TSO', 68: 'WLP', 69: 'AOC', 79: 'AA', 83: 'DV', 106: 'FO', \n",
    "               108: 'BF.B', 218: 'DG_1', 240: 'DOW', 241: 'CSC', 243: 'ERTS', 261: 'EL', 293: 'FRE', 294: 'FNM', \n",
    "              296:'NWS.A', 301: 'CZN', 311: 'WPO', 356: 'SLE', 362: 'IACI', 376: 'Q_1', 397: 'LIZ', 408: 'EK', \n",
    "              414: 'LTR', 415: 'LTD', 416: 'SAI', 418: 'LEH_0', 452: 'KFT', 480: 'MOT', 488: 'MNST_0', 498: 'FPL',\n",
    "              569: 'PMTC', 636: 'MHP', 648: 'WFR', 668: 'GCI', 712: 'VIA.B', 713: 'JDSU', 732: 'WFMI', 738: 'WM', \n",
    "               739: 'WM_0', 762: 'ZMH'}\n",
    "\n",
    "for n, st in enumerate(stock_ticker_formatted):\n",
    "    if n in list(tickers_map.keys()):\n",
    "        stock_ticker_formatted[n] = tickers_map[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "financial data of 0/765 retrieved, A appended\n",
      "financial data of 1/765 retrieved, AMR appended\n",
      "financial data of 2/765 retrieved, AAN appended\n",
      "financial data of 3/765 retrieved, AAP appended\n",
      "financial data of 4/765 retrieved, AAPL appended\n",
      "financial data of 5/765 retrieved, ABBV appended\n",
      "financial data of 6/765 retrieved, ABC appended\n",
      "financial data of 7/765 retrieved, ABI_0 appended\n",
      "financial data of 8/765 retrieved, ABMD appended\n",
      "financial data of 9/765 retrieved, ABT appended\n",
      "financial data of 10/765 retrieved, ACAS appended\n",
      "financial data of 11/765 retrieved, ACC appended\n",
      "financial data of 12/765 retrieved, ACIW appended\n",
      "financial data of 13/765 retrieved, ACM appended\n",
      "financial data of 14/765 retrieved, ACN appended\n",
      "financial data of 15/765 retrieved, ACS appended\n",
      "financial data of 16/765 retrieved, ACV appended\n",
      "financial data of 17/765 retrieved, ACXM appended\n",
      "financial data of 18/765 retrieved, ADBE appended\n",
      "financial data of 19/765 retrieved, ADCT appended\n",
      "financial data of 20/765 retrieved, ADI appended\n",
      "financial data of 21/765 retrieved, ADM appended\n",
      "financial data of 22/765 retrieved, ADP appended\n",
      "financial data of 23/765 retrieved, ADS appended\n",
      "financial data of 24/765 retrieved, ADSK appended\n",
      "financial data of 25/765 retrieved, ADT appended\n",
      "financial data of 26/765 retrieved, AEE appended\n",
      "financial data of 27/765 retrieved, AEO appended\n",
      "financial data of 28/765 retrieved, AEP appended\n",
      "financial data of 29/765 retrieved, AES appended\n",
      "financial data of 30/765 retrieved, AET appended\n",
      "financial data of 31/765 retrieved, AFG appended\n",
      "financial data of 32/765 retrieved, AFL appended\n",
      "financial data of 33/765 retrieved, AGCO appended\n",
      "financial data of 34/765 retrieved, AGN appended\n",
      "financial data of 35/765 retrieved, AGN appended\n",
      "financial data of 36/765 retrieved, AHL appended\n",
      "financial data of 37/765 retrieved, AIG appended\n",
      "financial data of 38/765 retrieved, AIV appended\n",
      "financial data of 39/765 retrieved, AIZ appended\n",
      "financial data of 40/765 retrieved, AJG appended\n",
      "financial data of 41/765 retrieved, AKAM appended\n",
      "financial data of 42/765 retrieved, AKRX appended\n",
      "financial data of 43/765 retrieved, ALB appended\n",
      "financial data of 44/765 retrieved, ALEX appended\n",
      "financial data of 45/765 retrieved, ALGN appended\n",
      "financial data of 46/765 retrieved, ALK appended\n",
      "financial data of 47/765 retrieved, ALL appended\n",
      "financial data of 48/765 retrieved, ALLE appended\n",
      "financial data of 49/765 retrieved, ALTR appended\n",
      "financial data of 50/765 retrieved, ALXN appended\n",
      "financial data of 51/765 retrieved, AMAT appended\n",
      "financial data of 52/765 retrieved, ABK appended\n",
      "financial data of 53/765 retrieved, AMCX appended\n",
      "financial data of 54/765 retrieved, AMD appended\n",
      "financial data of 55/765 retrieved, AME appended\n",
      "financial data of 56/765 retrieved, AMG appended\n",
      "financial data of 57/765 retrieved, AMGN appended\n",
      "financial data of 58/765 retrieved, AMP appended\n",
      "financial data of 59/765 retrieved, AMSC appended\n",
      "financial data of 60/765 retrieved, AMT appended\n",
      "financial data of 61/765 retrieved, AMZN appended\n",
      "financial data of 62/765 retrieved, AN appended\n",
      "financial data of 63/765 retrieved, TSO appended\n",
      "financial data of 64/765 retrieved, ANDW appended\n",
      "financial data of 65/765 retrieved, ANF appended\n",
      "financial data of 66/765 retrieved, ANR appended\n",
      "financial data of 67/765 retrieved, ANSS appended\n",
      "financial data of 68/765 retrieved, WLP appended\n",
      "financial data of 69/765 retrieved, AOC appended\n",
      "financial data of 70/765 retrieved, AOS appended\n",
      "financial data of 71/765 retrieved, APA appended\n",
      "financial data of 72/765 retrieved, APC appended\n",
      "financial data of 73/765 retrieved, APD appended\n",
      "financial data of 74/765 retrieved, APH appended\n",
      "financial data of 75/765 retrieved, APOL appended\n",
      "financial data of 76/765 retrieved, APU appended\n",
      "financial data of 77/765 retrieved, ARE appended\n",
      "financial data of 78/765 retrieved, ARG appended\n",
      "financial data of 79/765 retrieved, AA appended\n",
      "financial data of 80/765 retrieved, ARRS appended\n",
      "financial data of 81/765 retrieved, ARW appended\n",
      "financial data of 82/765 retrieved, ASH appended\n",
      "financial data of 83/765 retrieved, DV appended\n",
      "financial data of 84/765 retrieved, ATI appended\n",
      "financial data of 85/765 retrieved, ATVI appended\n",
      "financial data of 86/765 retrieved, AVB appended\n",
      "financial data of 87/765 retrieved, AVGO appended\n",
      "financial data of 88/765 retrieved, AVP appended\n",
      "financial data of 89/765 retrieved, AVY appended\n",
      "financial data of 90/765 retrieved, AW appended\n",
      "financial data of 91/765 retrieved, AWK appended\n",
      "financial data of 92/765 retrieved, AXP appended\n",
      "financial data of 93/765 retrieved, AYE appended\n",
      "financial data of 94/765 retrieved, AYI appended\n",
      "financial data of 95/765 retrieved, AZO appended\n",
      "financial data of 96/765 retrieved, BA appended\n",
      "financial data of 97/765 retrieved, BAC appended\n",
      "financial data of 98/765 retrieved, BAX appended\n",
      "financial data of 99/765 retrieved, BBBY appended\n",
      "financial data of 100/765 retrieved, BBT appended\n",
      "financial data of 101/765 retrieved, BBY appended\n",
      "financial data of 102/765 retrieved, BC appended\n",
      "financial data of 103/765 retrieved, BCR appended\n",
      "financial data of 104/765 retrieved, BDK appended\n",
      "financial data of 105/765 retrieved, BDX appended\n",
      "financial data of 106/765 retrieved, FO appended\n",
      "financial data of 107/765 retrieved, BEN appended\n",
      "financial data of 108/765 retrieved, BF.B appended\n",
      "financial data of 109/765 retrieved, BHF appended\n",
      "financial data of 110/765 retrieved, BHI appended\n",
      "financial data of 111/765 retrieved, BIG appended\n",
      "financial data of 112/765 retrieved, BIIB appended\n",
      "financial data of 113/765 retrieved, BJS appended\n",
      "financial data of 114/765 retrieved, BK appended\n",
      "financial data of 115/765 retrieved, BKE appended\n",
      "financial data of 116/765 retrieved, BLK appended\n",
      "financial data of 117/765 retrieved, BLL appended\n",
      "financial data of 118/765 retrieved, BMC appended\n",
      "financial data of 119/765 retrieved, BMS appended\n",
      "financial data of 120/765 retrieved, BMY appended\n",
      "financial data of 121/765 retrieved, BNI appended\n",
      "financial data of 122/765 retrieved, BRCM appended\n",
      "financial data of 123/765 retrieved, BRK.A appended\n",
      "financial data of 124/765 retrieved, BRL appended\n",
      "financial data of 125/765 retrieved, BRLI appended\n",
      "financial data of 126/765 retrieved, BSC appended\n",
      "financial data of 127/765 retrieved, BSX appended\n",
      "financial data of 128/765 retrieved, BTU appended\n",
      "financial data of 129/765 retrieved, BUD appended\n",
      "financial data of 130/765 retrieved, BWA appended\n",
      "financial data of 131/765 retrieved, BXP appended\n",
      "financial data of 132/765 retrieved, C appended\n",
      "financial data of 133/765 retrieved, CA appended\n",
      "financial data of 134/765 retrieved, CAG appended\n",
      "financial data of 135/765 retrieved, CAH appended\n",
      "financial data of 136/765 retrieved, CAM appended\n",
      "financial data of 137/765 retrieved, CAT appended\n",
      "financial data of 138/765 retrieved, CB appended\n",
      "financial data of 139/765 retrieved, CB appended\n",
      "financial data of 140/765 retrieved, CBE appended\n",
      "financial data of 141/765 retrieved, CBG appended\n",
      "financial data of 142/765 retrieved, CBOE appended\n",
      "financial data of 143/765 retrieved, CBS appended\n",
      "financial data of 144/765 retrieved, CCE appended\n",
      "financial data of 145/765 retrieved, CCE appended\n",
      "financial data of 146/765 retrieved, CCI appended\n",
      "financial data of 147/765 retrieved, CCL appended\n",
      "financial data of 148/765 retrieved, CCU appended\n",
      "financial data of 149/765 retrieved, CDNS appended\n",
      "financial data of 150/765 retrieved, CEG appended\n",
      "financial data of 151/765 retrieved, CELG appended\n",
      "financial data of 152/765 retrieved, CERN appended\n",
      "financial data of 153/765 retrieved, CF appended\n",
      "financial data of 154/765 retrieved, CFC appended\n",
      "financial data of 155/765 retrieved, CFG appended\n",
      "financial data of 156/765 retrieved, CFN appended\n",
      "financial data of 157/765 retrieved, CHD appended\n",
      "financial data of 158/765 retrieved, CHK appended\n",
      "financial data of 159/765 retrieved, CHRW appended\n",
      "financial data of 160/765 retrieved, CI appended\n",
      "financial data of 161/765 retrieved, CIEN appended\n",
      "financial data of 162/765 retrieved, CINF appended\n",
      "financial data of 163/765 retrieved, CIT appended\n",
      "financial data of 164/765 retrieved, CL appended\n",
      "financial data of 165/765 retrieved, CLF appended\n",
      "financial data of 166/765 retrieved, CLX appended\n",
      "financial data of 167/765 retrieved, CMA appended\n",
      "financial data of 168/765 retrieved, CMCSA appended\n",
      "financial data of 169/765 retrieved, CME appended\n",
      "financial data of 170/765 retrieved, CMG appended\n",
      "financial data of 171/765 retrieved, CMI appended\n",
      "financial data of 172/765 retrieved, CMS appended\n",
      "financial data of 173/765 retrieved, CMVT appended\n",
      "financial data of 174/765 retrieved, CNC appended\n",
      "financial data of 175/765 retrieved, CNO appended\n",
      "financial data of 176/765 retrieved, CNP appended\n",
      "financial data of 177/765 retrieved, CNX appended\n",
      "financial data of 178/765 retrieved, COF appended\n",
      "financial data of 179/765 retrieved, COG appended\n",
      "financial data of 180/765 retrieved, COH appended\n",
      "financial data of 181/765 retrieved, COL appended\n",
      "financial data of 182/765 retrieved, COO appended\n",
      "financial data of 183/765 retrieved, COP appended\n",
      "financial data of 184/765 retrieved, COST appended\n",
      "financial data of 185/765 retrieved, COTY appended\n",
      "financial data of 186/765 retrieved, COV appended\n",
      "financial data of 187/765 retrieved, CPB appended\n",
      "financial data of 188/765 retrieved, CPN appended\n",
      "financial data of 189/765 retrieved, CPWR appended\n",
      "financial data of 190/765 retrieved, CR appended\n",
      "financial data of 191/765 retrieved, CRA appended\n",
      "financial data of 192/765 retrieved, CRM appended\n",
      "financial data of 193/765 retrieved, CRVL appended\n",
      "financial data of 194/765 retrieved, CSCO appended\n",
      "financial data of 195/765 retrieved, CSRA appended\n",
      "financial data of 196/765 retrieved, CSX appended\n",
      "financial data of 197/765 retrieved, CTAS appended\n",
      "financial data of 198/765 retrieved, CTL appended\n",
      "financial data of 199/765 retrieved, CTSH appended\n",
      "financial data of 200/765 retrieved, CTX appended\n",
      "financial data of 201/765 retrieved, CTXS appended\n",
      "financial data of 202/765 retrieved, CVC appended\n",
      "financial data of 203/765 retrieved, CVG appended\n",
      "financial data of 204/765 retrieved, CVH appended\n",
      "financial data of 205/765 retrieved, CVS appended\n",
      "financial data of 206/765 retrieved, CVX appended\n",
      "financial data of 207/765 retrieved, CXO appended\n",
      "financial data of 208/765 retrieved, D appended\n",
      "financial data of 209/765 retrieved, DAL appended\n",
      "financial data of 210/765 retrieved, DCI appended\n",
      "financial data of 211/765 retrieved, DD appended\n",
      "financial data of 212/765 retrieved, DDR appended\n",
      "financial data of 213/765 retrieved, DDS appended\n",
      "financial data of 214/765 retrieved, DE appended\n",
      "financial data of 215/765 retrieved, DELL appended\n",
      "financial data of 216/765 retrieved, DF appended\n",
      "financial data of 217/765 retrieved, DFS appended\n",
      "financial data of 218/765 retrieved, DG_1 appended\n",
      "financial data of 219/765 retrieved, DGX appended\n",
      "financial data of 220/765 retrieved, DHI appended\n",
      "financial data of 221/765 retrieved, DHR appended\n",
      "financial data of 222/765 retrieved, DIS appended\n",
      "financial data of 223/765 retrieved, DISCA appended\n",
      "financial data of 224/765 retrieved, DISH appended\n",
      "financial data of 225/765 retrieved, DLPH appended\n",
      "financial data of 226/765 retrieved, DLR appended\n",
      "financial data of 227/765 retrieved, DLTR appended\n",
      "financial data of 228/765 retrieved, DNB appended\n",
      "financial data of 229/765 retrieved, DNR appended\n",
      "financial data of 230/765 retrieved, DO appended\n",
      "financial data of 231/765 retrieved, DOV appended\n",
      "financial data of 232/765 retrieved, DPS appended\n",
      "financial data of 233/765 retrieved, DRE appended\n",
      "financial data of 234/765 retrieved, DRI appended\n",
      "financial data of 235/765 retrieved, DTE appended\n",
      "financial data of 236/765 retrieved, DTV appended\n",
      "financial data of 237/765 retrieved, DUK appended\n",
      "financial data of 238/765 retrieved, DVA appended\n",
      "financial data of 239/765 retrieved, DVN appended\n",
      "financial data of 240/765 retrieved, DOW appended\n",
      "financial data of 241/765 retrieved, CSC appended\n",
      "financial data of 242/765 retrieved, DYN appended\n",
      "financial data of 243/765 retrieved, ERTS appended\n",
      "financial data of 244/765 retrieved, EBAY appended\n",
      "financial data of 245/765 retrieved, ECL appended\n",
      "financial data of 246/765 retrieved, ED appended\n",
      "financial data of 247/765 retrieved, EDS appended\n",
      "financial data of 248/765 retrieved, EFX appended\n",
      "financial data of 249/765 retrieved, EIX appended\n",
      "financial data of 250/765 retrieved, EL appended\n",
      "financial data of 251/765 retrieved, EMC appended\n",
      "financial data of 252/765 retrieved, EMN appended\n",
      "financial data of 253/765 retrieved, EMR appended\n",
      "financial data of 254/765 retrieved, ENDP appended\n",
      "financial data of 255/765 retrieved, EOG appended\n",
      "financial data of 256/765 retrieved, EP appended\n",
      "financial data of 257/765 retrieved, EQ appended\n",
      "financial data of 258/765 retrieved, EQIX appended\n",
      "financial data of 259/765 retrieved, EQR appended\n",
      "financial data of 260/765 retrieved, EQT appended\n",
      "financial data of 261/765 retrieved, EL appended\n",
      "financial data of 262/765 retrieved, ESRX appended\n",
      "financial data of 263/765 retrieved, ESS appended\n",
      "financial data of 264/765 retrieved, ESV appended\n",
      "financial data of 265/765 retrieved, ETFC appended\n",
      "financial data of 266/765 retrieved, ETN appended\n",
      "financial data of 267/765 retrieved, ETR appended\n",
      "financial data of 268/765 retrieved, EW appended\n",
      "financial data of 269/765 retrieved, EXC appended\n",
      "financial data of 270/765 retrieved, EXPD appended\n",
      "financial data of 271/765 retrieved, EXPE appended\n",
      "financial data of 272/765 retrieved, EXR appended\n",
      "financial data of 273/765 retrieved, F appended\n",
      "financial data of 274/765 retrieved, FAST appended\n",
      "financial data of 275/765 retrieved, FB appended\n",
      "financial data of 276/765 retrieved, FBHS appended\n",
      "financial data of 277/765 retrieved, FCX appended\n",
      "financial data of 278/765 retrieved, FDO appended\n",
      "financial data of 279/765 retrieved, FDS appended\n",
      "financial data of 280/765 retrieved, FDX appended\n",
      "financial data of 281/765 retrieved, FE appended\n",
      "financial data of 282/765 retrieved, FFIV appended\n",
      "financial data of 283/765 retrieved, FHN appended\n",
      "financial data of 284/765 retrieved, FII appended\n",
      "financial data of 285/765 retrieved, FIS appended\n",
      "financial data of 286/765 retrieved, FISV appended\n",
      "financial data of 287/765 retrieved, FITB appended\n",
      "financial data of 288/765 retrieved, FL appended\n",
      "financial data of 289/765 retrieved, FLIR appended\n",
      "financial data of 290/765 retrieved, FLR appended\n",
      "financial data of 291/765 retrieved, FLS appended\n",
      "financial data of 292/765 retrieved, FMC appended\n",
      "financial data of 293/765 retrieved, FRE appended\n",
      "financial data of 294/765 retrieved, FNM appended\n",
      "financial data of 295/765 retrieved, FOSL appended\n",
      "financial data of 296/765 retrieved, NWS.A appended\n",
      "financial data of 297/765 retrieved, FRT appended\n",
      "financial data of 298/765 retrieved, FRX appended\n",
      "financial data of 299/765 retrieved, FSLR appended\n",
      "financial data of 300/765 retrieved, FTI appended\n",
      "financial data of 301/765 retrieved, CZN appended\n",
      "financial data of 302/765 retrieved, FTV appended\n",
      "financial data of 303/765 retrieved, GAS appended\n",
      "financial data of 304/765 retrieved, GAS appended\n",
      "financial data of 305/765 retrieved, GCI appended\n",
      "financial data of 306/765 retrieved, GD appended\n",
      "financial data of 307/765 retrieved, GE appended\n",
      "financial data of 308/765 retrieved, GENZ appended\n",
      "financial data of 309/765 retrieved, GGP appended\n",
      "financial data of 310/765 retrieved, GGP appended\n",
      "financial data of 311/765 retrieved, WPO appended\n",
      "financial data of 312/765 retrieved, GILD appended\n",
      "financial data of 313/765 retrieved, GIS appended\n",
      "financial data of 314/765 retrieved, GLAD appended\n",
      "financial data of 315/765 retrieved, GLW appended\n",
      "financial data of 316/765 retrieved, GM appended\n",
      "financial data of 317/765 retrieved, GM appended\n",
      "financial data of 318/765 retrieved, GME appended\n",
      "financial data of 319/765 retrieved, GNW appended\n",
      "financial data of 320/765 retrieved, GOOG appended\n",
      "financial data of 321/765 retrieved, GPC appended\n",
      "financial data of 322/765 retrieved, GPN appended\n",
      "financial data of 323/765 retrieved, GPS appended\n",
      "financial data of 324/765 retrieved, GR appended\n",
      "financial data of 325/765 retrieved, GRA appended\n",
      "financial data of 326/765 retrieved, GRMN appended\n",
      "financial data of 327/765 retrieved, GS appended\n",
      "financial data of 328/765 retrieved, GT appended\n",
      "financial data of 329/765 retrieved, GWW appended\n",
      "financial data of 330/765 retrieved, HAL appended\n",
      "financial data of 331/765 retrieved, HAR appended\n",
      "financial data of 332/765 retrieved, HAS appended\n",
      "financial data of 333/765 retrieved, HBAN appended\n",
      "financial data of 334/765 retrieved, HBI appended\n",
      "financial data of 335/765 retrieved, HCA appended\n",
      "financial data of 336/765 retrieved, HCBK appended\n",
      "financial data of 337/765 retrieved, HCN appended\n",
      "financial data of 338/765 retrieved, HCP appended\n",
      "financial data of 339/765 retrieved, HD appended\n",
      "financial data of 340/765 retrieved, HES appended\n",
      "financial data of 341/765 retrieved, HIBB appended\n",
      "financial data of 342/765 retrieved, HIG appended\n",
      "financial data of 343/765 retrieved, HLT appended\n",
      "financial data of 344/765 retrieved, HNZ appended\n",
      "financial data of 345/765 retrieved, HOG appended\n",
      "financial data of 346/765 retrieved, HOLX appended\n",
      "financial data of 347/765 retrieved, HON appended\n",
      "financial data of 348/765 retrieved, HOT appended\n",
      "financial data of 349/765 retrieved, HP appended\n",
      "financial data of 350/765 retrieved, HPC appended\n",
      "financial data of 351/765 retrieved, HPE appended\n",
      "financial data of 352/765 retrieved, HPQ appended\n",
      "financial data of 353/765 retrieved, HRB appended\n",
      "financial data of 354/765 retrieved, HRL appended\n",
      "financial data of 355/765 retrieved, HRS appended\n",
      "financial data of 356/765 retrieved, SLE appended\n",
      "financial data of 357/765 retrieved, HSIC appended\n",
      "financial data of 358/765 retrieved, HSP appended\n",
      "financial data of 359/765 retrieved, HST appended\n",
      "financial data of 360/765 retrieved, HSY appended\n",
      "financial data of 361/765 retrieved, HUM appended\n",
      "financial data of 362/765 retrieved, IACI appended\n",
      "financial data of 363/765 retrieved, IBM appended\n",
      "financial data of 364/765 retrieved, ICE appended\n",
      "financial data of 365/765 retrieved, ICE appended\n",
      "financial data of 366/765 retrieved, IDXX appended\n",
      "financial data of 367/765 retrieved, IFF appended\n",
      "financial data of 368/765 retrieved, IGT appended\n",
      "financial data of 369/765 retrieved, ILMN appended\n",
      "financial data of 370/765 retrieved, INCY appended\n",
      "financial data of 371/765 retrieved, INFO appended\n",
      "financial data of 372/765 retrieved, INTC appended\n",
      "financial data of 373/765 retrieved, INTU appended\n",
      "financial data of 374/765 retrieved, IP appended\n",
      "financial data of 375/765 retrieved, IPG appended\n",
      "financial data of 376/765 retrieved, Q_1 appended\n",
      "financial data of 377/765 retrieved, IR appended\n",
      "financial data of 378/765 retrieved, IRM appended\n",
      "financial data of 379/765 retrieved, ISRG appended\n",
      "financial data of 380/765 retrieved, IT appended\n",
      "financial data of 381/765 retrieved, ITT appended\n",
      "financial data of 382/765 retrieved, ITW appended\n",
      "financial data of 383/765 retrieved, IVZ appended\n",
      "financial data of 384/765 retrieved, JAVA appended\n",
      "financial data of 385/765 retrieved, JBHT appended\n",
      "financial data of 386/765 retrieved, JBL appended\n",
      "financial data of 387/765 retrieved, JCI appended\n",
      "financial data of 388/765 retrieved, JCP appended\n",
      "financial data of 389/765 retrieved, JEC appended\n",
      "financial data of 390/765 retrieved, JNJ appended\n",
      "financial data of 391/765 retrieved, JNPR appended\n",
      "financial data of 392/765 retrieved, JNS appended\n",
      "financial data of 393/765 retrieved, JNY appended\n",
      "financial data of 394/765 retrieved, JPM appended\n",
      "financial data of 395/765 retrieved, JWN appended\n",
      "financial data of 396/765 retrieved, K appended\n",
      "financial data of 397/765 retrieved, LIZ appended\n",
      "financial data of 398/765 retrieved, KBH appended\n",
      "financial data of 399/765 retrieved, KEY appended\n",
      "financial data of 400/765 retrieved, KG appended\n",
      "financial data of 401/765 retrieved, KHC appended\n",
      "financial data of 402/765 retrieved, KIM appended\n",
      "financial data of 403/765 retrieved, KLAC appended\n",
      "financial data of 404/765 retrieved, KMB appended\n",
      "financial data of 405/765 retrieved, KMI appended\n",
      "financial data of 406/765 retrieved, KMX appended\n",
      "financial data of 407/765 retrieved, KO appended\n",
      "financial data of 408/765 retrieved, EK appended\n",
      "financial data of 409/765 retrieved, KORS appended\n",
      "financial data of 410/765 retrieved, KR appended\n",
      "financial data of 411/765 retrieved, KRFT appended\n",
      "financial data of 412/765 retrieved, KSS appended\n",
      "financial data of 413/765 retrieved, KSU appended\n",
      "financial data of 414/765 retrieved, LTR appended\n",
      "financial data of 415/765 retrieved, LTD appended\n",
      "financial data of 416/765 retrieved, SAI appended\n",
      "financial data of 417/765 retrieved, LEG appended\n",
      "financial data of 418/765 retrieved, LEH_0 appended\n",
      "financial data of 419/765 retrieved, LEN appended\n",
      "financial data of 420/765 retrieved, LH appended\n",
      "financial data of 421/765 retrieved, LIFE appended\n",
      "financial data of 422/765 retrieved, LKQ appended\n",
      "financial data of 423/765 retrieved, LLL appended\n",
      "financial data of 424/765 retrieved, LLTC appended\n",
      "financial data of 425/765 retrieved, LLY appended\n",
      "financial data of 426/765 retrieved, LM appended\n",
      "financial data of 427/765 retrieved, LMT appended\n",
      "financial data of 428/765 retrieved, LNC appended\n",
      "financial data of 429/765 retrieved, LNKD appended\n",
      "financial data of 430/765 retrieved, LNT appended\n",
      "financial data of 431/765 retrieved, LOW appended\n",
      "financial data of 432/765 retrieved, LRCX appended\n",
      "financial data of 433/765 retrieved, LSI appended\n",
      "financial data of 434/765 retrieved, LSTR appended\n",
      "financial data of 435/765 retrieved, LUK appended\n",
      "financial data of 436/765 retrieved, LUV appended\n",
      "financial data of 437/765 retrieved, LVLT appended\n",
      "financial data of 438/765 retrieved, LXK appended\n",
      "financial data of 439/765 retrieved, LYB appended\n",
      "financial data of 440/765 retrieved, M appended\n",
      "financial data of 441/765 retrieved, MA appended\n",
      "financial data of 442/765 retrieved, MAA appended\n",
      "financial data of 443/765 retrieved, MAC appended\n",
      "financial data of 444/765 retrieved, MAR appended\n",
      "financial data of 445/765 retrieved, MAS appended\n",
      "financial data of 446/765 retrieved, MAT appended\n",
      "financial data of 447/765 retrieved, MBI appended\n",
      "financial data of 448/765 retrieved, MCD appended\n",
      "financial data of 449/765 retrieved, MCHP appended\n",
      "financial data of 450/765 retrieved, MCK appended\n",
      "financial data of 451/765 retrieved, MCO appended\n",
      "financial data of 452/765 retrieved, KFT appended\n",
      "financial data of 453/765 retrieved, MDP appended\n",
      "financial data of 454/765 retrieved, MDT appended\n",
      "financial data of 455/765 retrieved, MED appended\n",
      "financial data of 456/765 retrieved, MER appended\n",
      "financial data of 457/765 retrieved, MET appended\n",
      "financial data of 458/765 retrieved, MFE appended\n",
      "financial data of 459/765 retrieved, MGM appended\n",
      "financial data of 460/765 retrieved, MHK appended\n",
      "financial data of 461/765 retrieved, MHS appended\n",
      "financial data of 462/765 retrieved, MI appended\n",
      "financial data of 463/765 retrieved, MIL appended\n",
      "financial data of 464/765 retrieved, MJN appended\n",
      "financial data of 465/765 retrieved, MKC appended\n",
      "financial data of 466/765 retrieved, MLM appended\n",
      "financial data of 467/765 retrieved, MMC appended\n",
      "financial data of 468/765 retrieved, MMM appended\n",
      "financial data of 469/765 retrieved, MNK appended\n",
      "financial data of 470/765 retrieved, MNST appended\n",
      "financial data of 471/765 retrieved, MO appended\n",
      "financial data of 472/765 retrieved, MOLX appended\n",
      "financial data of 473/765 retrieved, MON appended\n",
      "financial data of 474/765 retrieved, MOS appended\n",
      "financial data of 475/765 retrieved, MPC appended\n",
      "financial data of 476/765 retrieved, MRK appended\n",
      "financial data of 477/765 retrieved, MRO appended\n",
      "financial data of 478/765 retrieved, MS appended\n",
      "financial data of 479/765 retrieved, MSFT appended\n",
      "financial data of 480/765 retrieved, MOT appended\n",
      "financial data of 481/765 retrieved, MTB appended\n",
      "financial data of 482/765 retrieved, MTD appended\n",
      "financial data of 483/765 retrieved, MTG appended\n",
      "financial data of 484/765 retrieved, MTW appended\n",
      "financial data of 485/765 retrieved, MU appended\n",
      "financial data of 486/765 retrieved, MUR appended\n",
      "financial data of 487/765 retrieved, MWV appended\n",
      "financial data of 488/765 retrieved, MNST_0 appended\n",
      "financial data of 489/765 retrieved, MYL appended\n",
      "financial data of 490/765 retrieved, NAV appended\n",
      "financial data of 491/765 retrieved, NAVI appended\n",
      "financial data of 492/765 retrieved, NBL appended\n",
      "financial data of 493/765 retrieved, NBR appended\n",
      "financial data of 494/765 retrieved, NCC appended\n",
      "financial data of 495/765 retrieved, NCLH appended\n",
      "financial data of 496/765 retrieved, NDAQ appended\n",
      "financial data of 497/765 retrieved, NE appended\n",
      "financial data of 498/765 retrieved, FPL appended\n",
      "financial data of 499/765 retrieved, NEM appended\n",
      "financial data of 500/765 retrieved, NFLX appended\n",
      "financial data of 501/765 retrieved, NFX appended\n",
      "financial data of 502/765 retrieved, NI appended\n",
      "financial data of 503/765 retrieved, NILE appended\n",
      "financial data of 504/765 retrieved, NKE appended\n",
      "financial data of 505/765 retrieved, NLSN appended\n",
      "financial data of 506/765 retrieved, NOC appended\n",
      "financial data of 507/765 retrieved, NOV appended\n",
      "financial data of 508/765 retrieved, NOVL appended\n",
      "financial data of 509/765 retrieved, NRG appended\n",
      "financial data of 510/765 retrieved, NSC appended\n",
      "financial data of 511/765 retrieved, NSM appended\n",
      "financial data of 512/765 retrieved, NTAP appended\n",
      "financial data of 513/765 retrieved, NTRS appended\n",
      "financial data of 514/765 retrieved, NUE appended\n",
      "financial data of 515/765 retrieved, NUS appended\n",
      "financial data of 516/765 retrieved, NVDA appended\n",
      "financial data of 517/765 retrieved, NVLS appended\n",
      "financial data of 518/765 retrieved, NWL appended\n",
      "financial data of 519/765 retrieved, NWS appended\n",
      "financial data of 520/765 retrieved, NYT appended\n",
      "financial data of 521/765 retrieved, O appended\n",
      "financial data of 522/765 retrieved, ODP appended\n",
      "financial data of 523/765 retrieved, OFLX appended\n",
      "financial data of 524/765 retrieved, OI appended\n",
      "financial data of 525/765 retrieved, OKE appended\n",
      "financial data of 526/765 retrieved, OMC appended\n",
      "financial data of 527/765 retrieved, OMX appended\n",
      "financial data of 528/765 retrieved, ORCL appended\n",
      "financial data of 529/765 retrieved, ORLY appended\n",
      "financial data of 530/765 retrieved, OXY appended\n",
      "financial data of 531/765 retrieved, PAYX appended\n",
      "financial data of 532/765 retrieved, PBCT appended\n",
      "financial data of 533/765 retrieved, PBG appended\n",
      "financial data of 534/765 retrieved, PBI appended\n",
      "financial data of 535/765 retrieved, PCAR appended\n",
      "financial data of 536/765 retrieved, PCG appended\n",
      "financial data of 537/765 retrieved, PCL appended\n",
      "financial data of 538/765 retrieved, PCLN appended\n",
      "financial data of 539/765 retrieved, PCP appended\n",
      "financial data of 540/765 retrieved, PDCO appended\n",
      "financial data of 541/765 retrieved, PEG appended\n",
      "financial data of 542/765 retrieved, PEP appended\n",
      "financial data of 543/765 retrieved, PETM appended\n",
      "financial data of 544/765 retrieved, PETS appended\n",
      "financial data of 545/765 retrieved, PFE appended\n",
      "financial data of 546/765 retrieved, PFG appended\n",
      "financial data of 547/765 retrieved, PFIE appended\n",
      "financial data of 548/765 retrieved, PG appended\n",
      "financial data of 549/765 retrieved, PGN appended\n",
      "financial data of 550/765 retrieved, PGR appended\n",
      "financial data of 551/765 retrieved, PH appended\n",
      "financial data of 552/765 retrieved, PHM appended\n",
      "financial data of 553/765 retrieved, PKG appended\n",
      "financial data of 554/765 retrieved, PKI appended\n",
      "financial data of 555/765 retrieved, PLD appended\n",
      "financial data of 556/765 retrieved, PLD appended\n",
      "financial data of 557/765 retrieved, PLL appended\n",
      "financial data of 558/765 retrieved, PM appended\n",
      "financial data of 559/765 retrieved, PNC appended\n",
      "financial data of 560/765 retrieved, PNR appended\n",
      "financial data of 561/765 retrieved, PNW appended\n",
      "financial data of 562/765 retrieved, POM appended\n",
      "financial data of 563/765 retrieved, PPG appended\n",
      "financial data of 564/765 retrieved, PPL appended\n",
      "financial data of 565/765 retrieved, PRGO appended\n",
      "financial data of 566/765 retrieved, PRU appended\n",
      "financial data of 567/765 retrieved, PSA appended\n",
      "financial data of 568/765 retrieved, PSX appended\n",
      "financial data of 569/765 retrieved, PMTC appended\n",
      "financial data of 570/765 retrieved, PTV appended\n",
      "financial data of 571/765 retrieved, PVH appended\n",
      "financial data of 572/765 retrieved, PWR appended\n",
      "financial data of 573/765 retrieved, PX appended\n",
      "financial data of 574/765 retrieved, PXD appended\n",
      "financial data of 575/765 retrieved, PYPL appended\n",
      "financial data of 576/765 retrieved, Q appended\n",
      "financial data of 577/765 retrieved, QCOM appended\n",
      "financial data of 578/765 retrieved, QLGC appended\n",
      "financial data of 579/765 retrieved, QRVO appended\n",
      "financial data of 580/765 retrieved, QSII appended\n",
      "financial data of 581/765 retrieved, R appended\n",
      "financial data of 582/765 retrieved, RAD appended\n",
      "financial data of 583/765 retrieved, RAI appended\n",
      "financial data of 584/765 retrieved, RAVN appended\n",
      "financial data of 585/765 retrieved, RCL appended\n",
      "financial data of 586/765 retrieved, RDC appended\n",
      "financial data of 587/765 retrieved, RE appended\n",
      "financial data of 588/765 retrieved, REG appended\n",
      "financial data of 589/765 retrieved, REGN appended\n",
      "financial data of 590/765 retrieved, RF appended\n",
      "financial data of 591/765 retrieved, RHI appended\n",
      "financial data of 592/765 retrieved, RHT appended\n",
      "financial data of 593/765 retrieved, RIG appended\n",
      "financial data of 594/765 retrieved, RJF appended\n",
      "financial data of 595/765 retrieved, RL appended\n",
      "financial data of 596/765 retrieved, RMD appended\n",
      "financial data of 597/765 retrieved, ROH appended\n",
      "financial data of 598/765 retrieved, ROK appended\n",
      "financial data of 599/765 retrieved, ROP appended\n",
      "financial data of 600/765 retrieved, ROST appended\n",
      "financial data of 601/765 retrieved, RRC appended\n",
      "financial data of 602/765 retrieved, RRD appended\n",
      "financial data of 603/765 retrieved, RSG appended\n",
      "financial data of 604/765 retrieved, RSH appended\n",
      "financial data of 605/765 retrieved, RTN appended\n",
      "financial data of 606/765 retrieved, RX appended\n",
      "financial data of 607/765 retrieved, S appended\n",
      "financial data of 608/765 retrieved, SAF appended\n",
      "financial data of 609/765 retrieved, SAM appended\n",
      "financial data of 610/765 retrieved, SANM appended\n",
      "financial data of 611/765 retrieved, SAPE appended\n",
      "financial data of 612/765 retrieved, SBAC appended\n",
      "financial data of 613/765 retrieved, SBL appended\n",
      "financial data of 614/765 retrieved, SBUX appended\n",
      "financial data of 615/765 retrieved, SCG appended\n",
      "financial data of 616/765 retrieved, SCHW appended\n",
      "financial data of 617/765 retrieved, SE appended\n",
      "financial data of 618/765 retrieved, SEE appended\n",
      "financial data of 619/765 retrieved, SGP appended\n",
      "financial data of 620/765 retrieved, SHLD appended\n",
      "financial data of 621/765 retrieved, SHW appended\n",
      "financial data of 622/765 retrieved, SIAL appended\n",
      "financial data of 623/765 retrieved, SIG appended\n",
      "financial data of 624/765 retrieved, SII appended\n",
      "financial data of 625/765 retrieved, SJM appended\n",
      "financial data of 626/765 retrieved, SLB appended\n",
      "financial data of 627/765 retrieved, SLG appended\n",
      "financial data of 628/765 retrieved, SLM appended\n",
      "financial data of 629/765 retrieved, SNA appended\n",
      "financial data of 630/765 retrieved, SNDK appended\n",
      "financial data of 631/765 retrieved, SNI appended\n",
      "financial data of 632/765 retrieved, SNPS appended\n",
      "financial data of 633/765 retrieved, SO appended\n",
      "financial data of 634/765 retrieved, SOV appended\n",
      "financial data of 635/765 retrieved, SPG appended\n",
      "financial data of 636/765 retrieved, MHP appended\n",
      "financial data of 637/765 retrieved, SPLS appended\n",
      "financial data of 638/765 retrieved, SRCL appended\n",
      "financial data of 639/765 retrieved, SRE appended\n",
      "financial data of 640/765 retrieved, SSP appended\n",
      "financial data of 641/765 retrieved, STI appended\n",
      "financial data of 642/765 retrieved, STJ appended\n",
      "financial data of 643/765 retrieved, STR appended\n",
      "financial data of 644/765 retrieved, STT appended\n",
      "financial data of 645/765 retrieved, STX appended\n",
      "financial data of 646/765 retrieved, STZ appended\n",
      "financial data of 647/765 retrieved, SUN appended\n",
      "financial data of 648/765 retrieved, WFR appended\n",
      "financial data of 649/765 retrieved, SVU appended\n",
      "financial data of 650/765 retrieved, SWK appended\n",
      "financial data of 651/765 retrieved, SWKS appended\n",
      "financial data of 652/765 retrieved, SWN appended\n",
      "financial data of 653/765 retrieved, SWY appended\n",
      "financial data of 654/765 retrieved, SYF appended\n",
      "financial data of 655/765 retrieved, SYK appended\n",
      "financial data of 656/765 retrieved, SYMC appended\n",
      "financial data of 657/765 retrieved, SYNT appended\n",
      "financial data of 658/765 retrieved, SYY appended\n",
      "financial data of 659/765 retrieved, T appended\n",
      "financial data of 660/765 retrieved, TAP appended\n",
      "financial data of 661/765 retrieved, TDC appended\n",
      "financial data of 662/765 retrieved, TDG appended\n",
      "financial data of 663/765 retrieved, TE appended\n",
      "financial data of 664/765 retrieved, TEG appended\n",
      "financial data of 665/765 retrieved, TEL appended\n",
      "financial data of 666/765 retrieved, TER appended\n",
      "financial data of 667/765 retrieved, TEX appended\n",
      "financial data of 668/765 retrieved, GCI appended\n",
      "financial data of 669/765 retrieved, TGT appended\n",
      "financial data of 670/765 retrieved, THC appended\n",
      "financial data of 671/765 retrieved, TIE appended\n",
      "financial data of 672/765 retrieved, TIF appended\n",
      "financial data of 673/765 retrieved, TIN appended\n",
      "financial data of 674/765 retrieved, TJX appended\n",
      "financial data of 675/765 retrieved, TKR appended\n",
      "financial data of 676/765 retrieved, TLAB appended\n",
      "financial data of 677/765 retrieved, TMK appended\n",
      "financial data of 678/765 retrieved, TMO appended\n",
      "financial data of 679/765 retrieved, TRIP appended\n",
      "financial data of 680/765 retrieved, TROW appended\n",
      "financial data of 681/765 retrieved, TRV appended\n",
      "financial data of 682/765 retrieved, TSCO appended\n",
      "financial data of 683/765 retrieved, TSLA appended\n",
      "financial data of 684/765 retrieved, TSN appended\n",
      "financial data of 685/765 retrieved, TSS appended\n",
      "financial data of 686/765 retrieved, TT appended\n",
      "financial data of 687/765 retrieved, TWC appended\n",
      "financial data of 688/765 retrieved, TWTR appended\n",
      "financial data of 689/765 retrieved, TWX appended\n",
      "financial data of 690/765 retrieved, TXN appended\n",
      "financial data of 691/765 retrieved, TXT appended\n",
      "financial data of 692/765 retrieved, TYC appended\n",
      "financial data of 693/765 retrieved, UA appended\n",
      "financial data of 694/765 retrieved, UAL appended\n",
      "financial data of 695/765 retrieved, UDR appended\n",
      "financial data of 696/765 retrieved, UG appended\n",
      "financial data of 697/765 retrieved, UHS appended\n",
      "financial data of 698/765 retrieved, UIS appended\n",
      "financial data of 699/765 retrieved, ULTA appended\n",
      "financial data of 700/765 retrieved, UNH appended\n",
      "financial data of 701/765 retrieved, UNM appended\n",
      "financial data of 702/765 retrieved, UNP appended\n",
      "financial data of 703/765 retrieved, UPS appended\n",
      "financial data of 704/765 retrieved, URBN appended\n",
      "financial data of 705/765 retrieved, URI appended\n",
      "financial data of 706/765 retrieved, USB appended\n",
      "financial data of 707/765 retrieved, UST appended\n",
      "financial data of 708/765 retrieved, UTX appended\n",
      "financial data of 709/765 retrieved, V appended\n",
      "financial data of 710/765 retrieved, VAR appended\n",
      "financial data of 711/765 retrieved, VFC appended\n",
      "financial data of 712/765 retrieved, VIA.B appended\n",
      "financial data of 713/765 retrieved, JDSU appended\n",
      "financial data of 714/765 retrieved, VIVO appended\n",
      "financial data of 715/765 retrieved, VLO appended\n",
      "financial data of 716/765 retrieved, VMC appended\n",
      "financial data of 717/765 retrieved, VNO appended\n",
      "financial data of 718/765 retrieved, VRSK appended\n",
      "financial data of 719/765 retrieved, VRSN appended\n",
      "financial data of 720/765 retrieved, VRTX appended\n",
      "financial data of 721/765 retrieved, VTR appended\n",
      "financial data of 722/765 retrieved, VZ appended\n",
      "financial data of 723/765 retrieved, WAG appended\n",
      "financial data of 724/765 retrieved, WAT appended\n",
      "financial data of 725/765 retrieved, WB appended\n",
      "financial data of 726/765 retrieved, WBA appended\n",
      "financial data of 727/765 retrieved, WDC appended\n",
      "financial data of 728/765 retrieved, WDR appended\n",
      "financial data of 729/765 retrieved, WEC appended\n",
      "financial data of 730/765 retrieved, WEN appended\n",
      "financial data of 731/765 retrieved, WFC appended\n",
      "financial data of 732/765 retrieved, WFMI appended\n",
      "financial data of 733/765 retrieved, WFT appended\n",
      "financial data of 734/765 retrieved, WHR appended\n",
      "financial data of 735/765 retrieved, WIN appended\n",
      "financial data of 736/765 retrieved, WINA appended\n",
      "financial data of 737/765 retrieved, WLTW appended\n",
      "financial data of 738/765 retrieved, WM appended\n",
      "financial data of 739/765 retrieved, WM_0 appended\n",
      "financial data of 740/765 retrieved, WMB appended\n",
      "financial data of 741/765 retrieved, WMT appended\n",
      "financial data of 742/765 retrieved, WPX appended\n",
      "financial data of 743/765 retrieved, WRK appended\n",
      "financial data of 744/765 retrieved, WU appended\n",
      "financial data of 745/765 retrieved, WWY appended\n",
      "financial data of 746/765 retrieved, WY appended\n",
      "financial data of 747/765 retrieved, WYE appended\n",
      "financial data of 748/765 retrieved, WYN appended\n",
      "financial data of 749/765 retrieved, WYNN appended\n",
      "financial data of 750/765 retrieved, X appended\n",
      "financial data of 751/765 retrieved, XEC appended\n",
      "financial data of 752/765 retrieved, XEL appended\n",
      "financial data of 753/765 retrieved, XL appended\n",
      "financial data of 754/765 retrieved, XLNX appended\n",
      "financial data of 755/765 retrieved, XOM appended\n",
      "financial data of 756/765 retrieved, XRAY appended\n",
      "financial data of 757/765 retrieved, XRX appended\n",
      "financial data of 758/765 retrieved, XTO appended\n",
      "financial data of 759/765 retrieved, XYL appended\n",
      "financial data of 760/765 retrieved, YHOO appended\n",
      "financial data of 761/765 retrieved, YUM appended\n",
      "financial data of 762/765 retrieved, ZMH appended\n",
      "financial data of 763/765 retrieved, ZION appended\n",
      "financial data of 764/765 retrieved, ZTS appended\n"
     ]
    }
   ],
   "source": [
    "# automate downloading of files using requests library\n",
    "import requests\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "tickers_not_retrieved = [] # may be file name error\n",
    "financial_data_by_tickers = []\n",
    "total_tickers = len(stock_ticker_formatted)\n",
    "\n",
    "for n, st in enumerate(stock_ticker_formatted):\n",
    "    url = f\"https://web.archive.org/web/20180809015717/http://www.stockpup.com/data/{st}_quarterly_financial_data.csv\"\n",
    "\n",
    "    with requests.Session() as s:\n",
    "        try:\n",
    "            download = s.get(url)\n",
    "            decoded_content = download.content.decode('utf-8')\n",
    "            csv_lines = list(csv.reader(decoded_content.splitlines(), delimiter=','))\n",
    "            df_financial_data_per_ticker = pd.DataFrame(csv_lines, columns=csv_lines[0]).drop(index=0)\n",
    "            df_financial_data_per_ticker['ticker'] = st\n",
    "            financial_data_by_tickers.append(df_financial_data_per_ticker)\n",
    "            print(f'financial data of {n}/{total_tickers} retrieved, {st} appended')\n",
    "        except Exception as e:\n",
    "            print(f'ticker not retrieved {st} financial data of {n}/{total_tickers} NOT RETRIEVED', 'error:', e)\n",
    "            tickers_not_retrieved.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data_by_tickers = load_file(file_path=folder_path, file_name='financial_data_by_tickers')\n",
    "stock_ticker_formatted_before_map = load_file(file_path=folder_path, file_name='stock_tickers_formatted_before_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 102 leading stocks chosen. Assumption to be made that we find top 102 stocks that represents about 51% of market, then get the required timeline\n",
    "df_top_200 = pd.read_csv(f'{folder_path}/top_200_stocks_by_components.csv')\n",
    "top_200_list = list(df_top_200['Symbol'])\n",
    "\n",
    "\n",
    "# check stocks in list\n",
    "top_200_idx = []\n",
    "for n, st in enumerate(stock_ticker_formatted_before_map):\n",
    "    if st in top_200_list:\n",
    "        top_200_idx.append(n)\n",
    "        \n",
    "# 95 stocks in top 102 in dataframes list\n",
    "financial_data_by_tickers_final = []\n",
    "for idx in top_200_idx:\n",
    "    financial_data_by_tickers_final.append(financial_data_by_tickers[idx])\n",
    "    \n",
    "# out of 95 stocks take only the relevant quarters from Q1 1996 to Q4 2017\n",
    "financial_data_by_tickers_timeframe = []\n",
    "for df in financial_data_by_tickers_final:\n",
    "    df_temp = df[(df['Quarter end'] >= '1996-03-29') & (df['Quarter end'] <= '2017-12-31')]\n",
    "    financial_data_by_tickers_timeframe.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only tickers with all 88 observations from Q1 1996 to Q4 2017 present, else drop them\n",
    "for n, df in enumerate(financial_data_by_tickers_timeframe):\n",
    "    if df.shape[0] != 88:\n",
    "        del financial_data_by_tickers_timeframe[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n",
      "(88, 42)\n"
     ]
    }
   ],
   "source": [
    "# check shapes of dfs\n",
    "for df in financial_data_by_tickers_timeframe:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataframes that have all 88 observations, we select the top 70 then do feature dropping\n",
    "\n",
    "# merge dataframe, get rank, sort them and select top 70\n",
    "\n",
    "df_financial_data_top_97 = pd.concat(financial_data_by_tickers_timeframe). \\\n",
    "merge(df_top_200, left_on='ticker', right_on='Symbol'). \\\n",
    "sort_values(by='#', ascending=True). \\\n",
    "drop(columns = list(df_top_200.columns))\n",
    "\n",
    "financial_data_by_tickers_timeframe_sorted = []\n",
    "ticker_list = list(df_financial_data_top_97['ticker'].unique())\n",
    "for t in ticker_list:\n",
    "    df_temp = df_financial_data_top_97[df_financial_data_top_97['ticker'] == t]. \\\n",
    "    sort_values(by='Quarter end', ascending=True). \\\n",
    "    reset_index(drop=True)\n",
    "    \n",
    "    financial_data_by_tickers_timeframe_sorted.append(df_temp)\n",
    "    \n",
    "financial_data_by_tickers_timeframe_final = deepcopy(financial_data_by_tickers_timeframe_sorted[0:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data_by_tickers_timeframe_final = load_file(file_path=folder_path, file_name='financial_data_by_tickers_timeframe_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An assumption is to select 21 features that have all values present for 88 data points or are good features by logical reasoning and per research on important fundamental features\n",
    "\n",
    "cols_wanted = ['Quarter end', 'Assets', 'Current Assets', 'Liabilities', 'Current Liabilities', 'Shareholders equity','Revenue', 'Earnings', 'EPS basic', 'EPS diluted', 'Price', 'Price high', 'Price low', 'ROE', \n",
    "               'P/B ratio', 'P/E ratio', 'Cumulative dividends per share', 'Dividend payout ratio', 'Long-term debt to equity ratio','Net margin', 'Asset turnover', 'Free cash flow per share', 'ticker'] \n",
    "financial_data_by_tickers_timeframe_cols_selected = []\n",
    "for df in financial_data_by_tickers_timeframe_final:\n",
    "    df_temp = df[cols_wanted]\n",
    "    financial_data_by_tickers_timeframe_cols_selected.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data_cols_converted = []\n",
    "\n",
    "for df in financial_data_by_tickers_timeframe_cols_selected:\n",
    "    cols_convert = list(df.drop(columns=['Quarter end', 'ticker']).columns)\n",
    "    df = df.replace({'0': 0, 'None':0, None: 0})\n",
    "    df[cols_convert] = df[cols_convert].astype(float)\n",
    "    financial_data_cols_converted.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88 entries, 0 to 87\n",
      "Data columns (total 23 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   Quarter end                     88 non-null     object \n",
      " 1   Assets                          88 non-null     float64\n",
      " 2   Current Assets                  88 non-null     float64\n",
      " 3   Liabilities                     88 non-null     float64\n",
      " 4   Current Liabilities             88 non-null     float64\n",
      " 5   Shareholders equity             88 non-null     float64\n",
      " 6   Revenue                         88 non-null     float64\n",
      " 7   Earnings                        88 non-null     float64\n",
      " 8   EPS basic                       88 non-null     float64\n",
      " 9   EPS diluted                     88 non-null     float64\n",
      " 10  Price                           88 non-null     float64\n",
      " 11  Price high                      88 non-null     float64\n",
      " 12  Price low                       88 non-null     float64\n",
      " 13  ROE                             88 non-null     float64\n",
      " 14  P/B ratio                       88 non-null     float64\n",
      " 15  P/E ratio                       88 non-null     float64\n",
      " 16  Cumulative dividends per share  88 non-null     float64\n",
      " 17  Dividend payout ratio           88 non-null     float64\n",
      " 18  Long-term debt to equity ratio  88 non-null     float64\n",
      " 19  Net margin                      88 non-null     float64\n",
      " 20  Asset turnover                  88 non-null     float64\n",
      " 21  Free cash flow per share        88 non-null     float64\n",
      " 22  ticker                          88 non-null     object \n",
      "dtypes: float64(21), object(2)\n",
      "memory usage: 15.9+ KB\n"
     ]
    }
   ],
   "source": [
    "financial_data_cols_converted[0].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Relative Return of Stocks\n",
    "\n",
    "`absolute quarterly return of stock price`: (this quarter closing price) - (last quarter closing price) / (last quarter closing price)\n",
    "\n",
    "1. find out price of dija stocks and its absolute quarterly return per quarter\n",
    "2. calculate the absolute quarterly return of stocks per quarter \n",
    "3. get relative return w.r.t Dija\n",
    "4. relative_return = absolute_quarterly_return_of_stock - absolute_quarterly_return_of_dija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find absolute quarterly return in price for each ticker\n",
    "\n",
    "def relative_return_djia(df):\n",
    "    \"\"\"\n",
    "    This functions gets the relative return of quarterly price of each ticker with respect to the Dow Jones Industrial Average\n",
    "    \"\"\"\n",
    "\n",
    "    df_djia = pd.read_csv(f\"{folder_path}/DIJA_absolute_return.csv\")\n",
    "    np_djia_return = df_djia['Absolute Return'].to_numpy()\n",
    "\n",
    "    df_relative_return = deepcopy(df)\n",
    "    np_relative_return = np.zeros(len(df_relative_return))\n",
    "    np_absolute_return = np.zeros(len(df_relative_return))\n",
    "    np_price = df_relative_return['Price'].to_numpy()\n",
    "    \n",
    "    for n, price in enumerate(np_price):\n",
    "        if n==0:\n",
    "            np_relative_return[0] = 0\n",
    "        else:\n",
    "            absolute_return = (np_price[n] - np_price[n-1])/np_price[n-1]\n",
    "            np_absolute_return[n] = absolute_return\n",
    "    \n",
    "    for i in range(88):\n",
    "        np_relative_return[i] = np_absolute_return[i] - np_djia_return[i]\n",
    "        \n",
    "    # mean substitution for first quarter of 1996\n",
    "    np_relative_return[0] = np_relative_return.mean()\n",
    "\n",
    "    df_relative_return['Relative Return DJIA'] = np_relative_return\n",
    "    \n",
    "    return df_relative_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-182-74c00f9e9a5f>:20: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  absolute_return = (np_price[n] - np_price[n-1])/np_price[n-1]\n"
     ]
    }
   ],
   "source": [
    "financial_data_relative_return = []\n",
    "for df in financial_data_cols_converted:\n",
    "    financial_data_relative_return.append(relative_return_djia(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter end</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Current Assets</th>\n",
       "      <th>Liabilities</th>\n",
       "      <th>Current Liabilities</th>\n",
       "      <th>Shareholders equity</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>EPS basic</th>\n",
       "      <th>EPS diluted</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price high</th>\n",
       "      <th>Price low</th>\n",
       "      <th>ROE</th>\n",
       "      <th>P/B ratio</th>\n",
       "      <th>P/E ratio</th>\n",
       "      <th>Cumulative dividends per share</th>\n",
       "      <th>Dividend payout ratio</th>\n",
       "      <th>Long-term debt to equity ratio</th>\n",
       "      <th>Net margin</th>\n",
       "      <th>Asset turnover</th>\n",
       "      <th>Free cash flow per share</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Relative Return DJIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-03-29</td>\n",
       "      <td>5.234000e+09</td>\n",
       "      <td>4.277000e+09</td>\n",
       "      <td>3.178000e+09</td>\n",
       "      <td>2.273000e+09</td>\n",
       "      <td>2.056000e+09</td>\n",
       "      <td>2.185000e+09</td>\n",
       "      <td>-7.400000e+08</td>\n",
       "      <td>-5.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.2443</td>\n",
       "      <td>1.28</td>\n",
       "      <td>21.57</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1474</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.456728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-06-28</td>\n",
       "      <td>5.345000e+09</td>\n",
       "      <td>4.454000e+09</td>\n",
       "      <td>3.325000e+09</td>\n",
       "      <td>1.926000e+09</td>\n",
       "      <td>2.020000e+09</td>\n",
       "      <td>2.179000e+09</td>\n",
       "      <td>-3.200000e+07</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.3197</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4698</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.020458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-09-27</td>\n",
       "      <td>5.364000e+09</td>\n",
       "      <td>4.515000e+09</td>\n",
       "      <td>3.306000e+09</td>\n",
       "      <td>2.003000e+09</td>\n",
       "      <td>2.058000e+09</td>\n",
       "      <td>2.321000e+09</td>\n",
       "      <td>2.500000e+07</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.3656</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4611</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.11</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.717138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-12-27</td>\n",
       "      <td>5.272000e+09</td>\n",
       "      <td>4.419000e+09</td>\n",
       "      <td>3.330000e+09</td>\n",
       "      <td>2.044000e+09</td>\n",
       "      <td>1.942000e+09</td>\n",
       "      <td>2.129000e+09</td>\n",
       "      <td>-1.200000e+08</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.4294</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4892</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>8.984578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-03-28</td>\n",
       "      <td>4.487000e+09</td>\n",
       "      <td>3.642000e+09</td>\n",
       "      <td>3.245000e+09</td>\n",
       "      <td>2.011000e+09</td>\n",
       "      <td>1.242000e+09</td>\n",
       "      <td>1.601000e+09</td>\n",
       "      <td>-7.080000e+08</td>\n",
       "      <td>-5.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.4599</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7665</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.837868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997-06-27</td>\n",
       "      <td>4.341000e+09</td>\n",
       "      <td>3.493000e+09</td>\n",
       "      <td>3.145000e+09</td>\n",
       "      <td>1.910000e+09</td>\n",
       "      <td>1.196000e+09</td>\n",
       "      <td>1.737000e+09</td>\n",
       "      <td>-5.600000e+07</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.5337</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>14.081110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997-09-26</td>\n",
       "      <td>4.233000e+09</td>\n",
       "      <td>3.424000e+09</td>\n",
       "      <td>3.033000e+09</td>\n",
       "      <td>1.818000e+09</td>\n",
       "      <td>1.200000e+09</td>\n",
       "      <td>1.614000e+09</td>\n",
       "      <td>-1.610000e+08</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.7491</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7925</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.675242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1997-12-26</td>\n",
       "      <td>4.126000e+09</td>\n",
       "      <td>3.373000e+09</td>\n",
       "      <td>2.882000e+09</td>\n",
       "      <td>1.669000e+09</td>\n",
       "      <td>1.244000e+09</td>\n",
       "      <td>1.578000e+09</td>\n",
       "      <td>4.700000e+07</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.7194</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7653</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.586413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1998-03-27</td>\n",
       "      <td>3.963000e+09</td>\n",
       "      <td>3.213000e+09</td>\n",
       "      <td>2.575000e+09</td>\n",
       "      <td>1.384000e+09</td>\n",
       "      <td>1.388000e+09</td>\n",
       "      <td>1.405000e+09</td>\n",
       "      <td>5.500000e+07</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.0915</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6866</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>10.221135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1998-06-26</td>\n",
       "      <td>4.041000e+09</td>\n",
       "      <td>3.375000e+09</td>\n",
       "      <td>2.555000e+09</td>\n",
       "      <td>1.389000e+09</td>\n",
       "      <td>1.486000e+09</td>\n",
       "      <td>1.402000e+09</td>\n",
       "      <td>1.010000e+08</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6413</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.070150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1998-09-25</td>\n",
       "      <td>4.289000e+09</td>\n",
       "      <td>3.698000e+09</td>\n",
       "      <td>2.647000e+09</td>\n",
       "      <td>1.520000e+09</td>\n",
       "      <td>1.642000e+09</td>\n",
       "      <td>1.556000e+09</td>\n",
       "      <td>1.060000e+08</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2146</td>\n",
       "      <td>3.28</td>\n",
       "      <td>358.40</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-13.865783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1998-12-26</td>\n",
       "      <td>4.592000e+09</td>\n",
       "      <td>3.867000e+09</td>\n",
       "      <td>2.669000e+09</td>\n",
       "      <td>1.484000e+09</td>\n",
       "      <td>1.923000e+09</td>\n",
       "      <td>1.710000e+09</td>\n",
       "      <td>1.520000e+08</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>2.91</td>\n",
       "      <td>17.41</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>14.558278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1999-03-27</td>\n",
       "      <td>4.935000e+09</td>\n",
       "      <td>4.092000e+09</td>\n",
       "      <td>2.760000e+09</td>\n",
       "      <td>1.544000e+09</td>\n",
       "      <td>2.175000e+09</td>\n",
       "      <td>1.530000e+09</td>\n",
       "      <td>1.350000e+08</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.2735</td>\n",
       "      <td>2.84</td>\n",
       "      <td>15.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.0797</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.315441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999-06-26</td>\n",
       "      <td>5.019000e+09</td>\n",
       "      <td>4.298000e+09</td>\n",
       "      <td>2.049000e+09</td>\n",
       "      <td>1.539000e+09</td>\n",
       "      <td>2.970000e+09</td>\n",
       "      <td>1.558000e+09</td>\n",
       "      <td>2.030000e+08</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.2737</td>\n",
       "      <td>2.61</td>\n",
       "      <td>13.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>10.847414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1999-09-25</td>\n",
       "      <td>5.161000e+09</td>\n",
       "      <td>4.285000e+09</td>\n",
       "      <td>2.057000e+09</td>\n",
       "      <td>1.549000e+09</td>\n",
       "      <td>3.104000e+09</td>\n",
       "      <td>1.336000e+09</td>\n",
       "      <td>1.110000e+08</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.19</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>3.32</td>\n",
       "      <td>16.85</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-5.662087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>7.586000e+09</td>\n",
       "      <td>4.909000e+09</td>\n",
       "      <td>3.172000e+09</td>\n",
       "      <td>1.965000e+09</td>\n",
       "      <td>4.414000e+09</td>\n",
       "      <td>2.343000e+09</td>\n",
       "      <td>1.830000e+08</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.13</td>\n",
       "      <td>4.21</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>4.54</td>\n",
       "      <td>24.28</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0680</td>\n",
       "      <td>0.0934</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>10.520186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>7.007000e+09</td>\n",
       "      <td>4.912000e+09</td>\n",
       "      <td>2.792000e+09</td>\n",
       "      <td>1.853000e+09</td>\n",
       "      <td>4.215000e+09</td>\n",
       "      <td>1.945000e+09</td>\n",
       "      <td>2.330000e+08</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.23</td>\n",
       "      <td>5.37</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>4.32</td>\n",
       "      <td>32.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0712</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.915036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>6.932000e+09</td>\n",
       "      <td>5.157000e+09</td>\n",
       "      <td>2.756000e+09</td>\n",
       "      <td>1.873000e+09</td>\n",
       "      <td>4.176000e+09</td>\n",
       "      <td>1.825000e+09</td>\n",
       "      <td>2.000000e+08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.55</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2.87</td>\n",
       "      <td>0.1828</td>\n",
       "      <td>4.27</td>\n",
       "      <td>26.64</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>0.0976</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.608010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-09-30</td>\n",
       "      <td>6.803000e+09</td>\n",
       "      <td>5.427000e+09</td>\n",
       "      <td>2.696000e+09</td>\n",
       "      <td>1.933000e+09</td>\n",
       "      <td>4.107000e+09</td>\n",
       "      <td>1.870000e+09</td>\n",
       "      <td>1.700000e+08</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.1859</td>\n",
       "      <td>3.48</td>\n",
       "      <td>22.23</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0730</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.720470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-12-30</td>\n",
       "      <td>5.986000e+09</td>\n",
       "      <td>4.926000e+09</td>\n",
       "      <td>2.274000e+09</td>\n",
       "      <td>1.637000e+09</td>\n",
       "      <td>3.712000e+09</td>\n",
       "      <td>1.007000e+09</td>\n",
       "      <td>-1.950000e+08</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>1.66</td>\n",
       "      <td>9.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0838</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.710145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>6.130000e+09</td>\n",
       "      <td>5.345000e+09</td>\n",
       "      <td>2.392000e+09</td>\n",
       "      <td>1.795000e+09</td>\n",
       "      <td>3.738000e+09</td>\n",
       "      <td>1.431000e+09</td>\n",
       "      <td>4.300000e+07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>1.77</td>\n",
       "      <td>17.63</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-9.247683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001-06-30</td>\n",
       "      <td>6.071000e+09</td>\n",
       "      <td>5.248000e+09</td>\n",
       "      <td>2.213000e+09</td>\n",
       "      <td>1.614000e+09</td>\n",
       "      <td>3.858000e+09</td>\n",
       "      <td>1.475000e+09</td>\n",
       "      <td>6.100000e+07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>2.16</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0822</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.143763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2001-09-29</td>\n",
       "      <td>6.021000e+09</td>\n",
       "      <td>5.143000e+09</td>\n",
       "      <td>2.101000e+09</td>\n",
       "      <td>1.518000e+09</td>\n",
       "      <td>3.920000e+09</td>\n",
       "      <td>1.450000e+09</td>\n",
       "      <td>6.600000e+07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.0066</td>\n",
       "      <td>1.81</td>\n",
       "      <td>111.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-18.831963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2001-12-29</td>\n",
       "      <td>6.122000e+09</td>\n",
       "      <td>5.272000e+09</td>\n",
       "      <td>2.156000e+09</td>\n",
       "      <td>1.579000e+09</td>\n",
       "      <td>3.966000e+09</td>\n",
       "      <td>1.375000e+09</td>\n",
       "      <td>3.800000e+07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>11.686242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2002-03-30</td>\n",
       "      <td>6.264000e+09</td>\n",
       "      <td>5.414000e+09</td>\n",
       "      <td>2.250000e+09</td>\n",
       "      <td>1.709000e+09</td>\n",
       "      <td>4.014000e+09</td>\n",
       "      <td>1.495000e+09</td>\n",
       "      <td>4.000000e+07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>2.06</td>\n",
       "      <td>38.50</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.862965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2002-06-29</td>\n",
       "      <td>6.293000e+09</td>\n",
       "      <td>5.443000e+09</td>\n",
       "      <td>2.227000e+09</td>\n",
       "      <td>1.718000e+09</td>\n",
       "      <td>4.066000e+09</td>\n",
       "      <td>1.429000e+09</td>\n",
       "      <td>3.200000e+07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>1.88</td>\n",
       "      <td>36.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0777</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-12.635829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2002-09-28</td>\n",
       "      <td>6.298000e+09</td>\n",
       "      <td>5.388000e+09</td>\n",
       "      <td>2.203000e+09</td>\n",
       "      <td>1.658000e+09</td>\n",
       "      <td>4.095000e+09</td>\n",
       "      <td>1.443000e+09</td>\n",
       "      <td>-4.500000e+07</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>1.44</td>\n",
       "      <td>32.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-21.981386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2002-12-28</td>\n",
       "      <td>6.269000e+09</td>\n",
       "      <td>5.384000e+09</td>\n",
       "      <td>2.153000e+09</td>\n",
       "      <td>1.595000e+09</td>\n",
       "      <td>4.116000e+09</td>\n",
       "      <td>1.472000e+09</td>\n",
       "      <td>-8.000000e+06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.36</td>\n",
       "      <td>85.56</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0777</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>8.927623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2003-03-29</td>\n",
       "      <td>6.361000e+09</td>\n",
       "      <td>5.468000e+09</td>\n",
       "      <td>2.222000e+09</td>\n",
       "      <td>2.007000e+09</td>\n",
       "      <td>4.139000e+09</td>\n",
       "      <td>1.475000e+09</td>\n",
       "      <td>1.400000e+07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.0017</td>\n",
       "      <td>1.27</td>\n",
       "      <td>291.20</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.427597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2003-06-28</td>\n",
       "      <td>6.441000e+09</td>\n",
       "      <td>5.546000e+09</td>\n",
       "      <td>2.243000e+09</td>\n",
       "      <td>2.025000e+09</td>\n",
       "      <td>4.198000e+09</td>\n",
       "      <td>1.545000e+09</td>\n",
       "      <td>1.900000e+07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>11.170046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2003-09-27</td>\n",
       "      <td>6.815000e+09</td>\n",
       "      <td>5.887000e+09</td>\n",
       "      <td>2.592000e+09</td>\n",
       "      <td>2.357000e+09</td>\n",
       "      <td>4.223000e+09</td>\n",
       "      <td>1.715000e+09</td>\n",
       "      <td>4.400000e+07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0111</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.415671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2003-12-27</td>\n",
       "      <td>6.971000e+09</td>\n",
       "      <td>6.029000e+09</td>\n",
       "      <td>2.641000e+09</td>\n",
       "      <td>2.382000e+09</td>\n",
       "      <td>4.330000e+09</td>\n",
       "      <td>2.006000e+09</td>\n",
       "      <td>6.300000e+07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>1.94</td>\n",
       "      <td>117.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>11.336727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2004-03-27</td>\n",
       "      <td>6.735000e+09</td>\n",
       "      <td>5.784000e+09</td>\n",
       "      <td>2.245000e+09</td>\n",
       "      <td>1.980000e+09</td>\n",
       "      <td>4.490000e+09</td>\n",
       "      <td>1.909000e+09</td>\n",
       "      <td>4.600000e+07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>2.08</td>\n",
       "      <td>64.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.828342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2004-06-26</td>\n",
       "      <td>7.223000e+09</td>\n",
       "      <td>6.286000e+09</td>\n",
       "      <td>2.411000e+09</td>\n",
       "      <td>2.155000e+09</td>\n",
       "      <td>4.812000e+09</td>\n",
       "      <td>2.014000e+09</td>\n",
       "      <td>6.100000e+07</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>2.54</td>\n",
       "      <td>64.83</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.962485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2004-09-25</td>\n",
       "      <td>8.050000e+09</td>\n",
       "      <td>7.055000e+09</td>\n",
       "      <td>2.974000e+09</td>\n",
       "      <td>2.680000e+09</td>\n",
       "      <td>5.076000e+09</td>\n",
       "      <td>2.350000e+09</td>\n",
       "      <td>1.060000e+08</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>2.70</td>\n",
       "      <td>58.95</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.397054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2004-12-25</td>\n",
       "      <td>9.362000e+09</td>\n",
       "      <td>8.322000e+09</td>\n",
       "      <td>3.572000e+09</td>\n",
       "      <td>3.224000e+09</td>\n",
       "      <td>5.790000e+09</td>\n",
       "      <td>3.490000e+09</td>\n",
       "      <td>2.950000e+08</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.77</td>\n",
       "      <td>4.89</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>4.19</td>\n",
       "      <td>75.40</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.087938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2005-03-26</td>\n",
       "      <td>1.011100e+10</td>\n",
       "      <td>9.007000e+09</td>\n",
       "      <td>3.725000e+09</td>\n",
       "      <td>3.352000e+09</td>\n",
       "      <td>6.386000e+09</td>\n",
       "      <td>3.243000e+09</td>\n",
       "      <td>2.900000e+08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>5.47</td>\n",
       "      <td>6.44</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>5.42</td>\n",
       "      <td>62.26</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.207643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2005-06-25</td>\n",
       "      <td>1.048800e+10</td>\n",
       "      <td>9.376000e+09</td>\n",
       "      <td>3.667000e+09</td>\n",
       "      <td>3.123000e+09</td>\n",
       "      <td>6.821000e+09</td>\n",
       "      <td>3.520000e+09</td>\n",
       "      <td>3.200000e+08</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>5.56</td>\n",
       "      <td>6.25</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>5.01</td>\n",
       "      <td>43.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.210220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2005-09-24</td>\n",
       "      <td>1.155100e+10</td>\n",
       "      <td>1.030000e+10</td>\n",
       "      <td>4.085000e+09</td>\n",
       "      <td>3.484000e+09</td>\n",
       "      <td>7.466000e+09</td>\n",
       "      <td>3.678000e+09</td>\n",
       "      <td>4.300000e+08</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>6.40</td>\n",
       "      <td>7.60</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>5.47</td>\n",
       "      <td>37.81</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.11</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.930324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>1.418100e+10</td>\n",
       "      <td>1.216200e+10</td>\n",
       "      <td>5.801000e+09</td>\n",
       "      <td>5.060000e+09</td>\n",
       "      <td>8.380000e+09</td>\n",
       "      <td>5.749000e+09</td>\n",
       "      <td>5.650000e+08</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.65</td>\n",
       "      <td>8.81</td>\n",
       "      <td>10.78</td>\n",
       "      <td>6.84</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>6.94</td>\n",
       "      <td>39.53</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.764946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2006-04-01</td>\n",
       "      <td>1.391100e+10</td>\n",
       "      <td>1.128600e+10</td>\n",
       "      <td>5.229000e+09</td>\n",
       "      <td>4.456000e+09</td>\n",
       "      <td>8.682000e+09</td>\n",
       "      <td>4.359000e+09</td>\n",
       "      <td>4.100000e+08</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.29</td>\n",
       "      <td>12.34</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.2201</td>\n",
       "      <td>7.30</td>\n",
       "      <td>38.73</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>1.38</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.694939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>1.511400e+10</td>\n",
       "      <td>1.249100e+10</td>\n",
       "      <td>5.784000e+09</td>\n",
       "      <td>5.023000e+09</td>\n",
       "      <td>9.330000e+09</td>\n",
       "      <td>4.370000e+09</td>\n",
       "      <td>4.720000e+08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.23</td>\n",
       "      <td>10.54</td>\n",
       "      <td>7.92</td>\n",
       "      <td>0.2217</td>\n",
       "      <td>6.32</td>\n",
       "      <td>32.47</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.13</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.263796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2006-09-30</td>\n",
       "      <td>1.720500e+10</td>\n",
       "      <td>1.450900e+10</td>\n",
       "      <td>7.221000e+09</td>\n",
       "      <td>6.471000e+09</td>\n",
       "      <td>9.984000e+09</td>\n",
       "      <td>4.837000e+09</td>\n",
       "      <td>5.420000e+08</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.14</td>\n",
       "      <td>11.11</td>\n",
       "      <td>7.17</td>\n",
       "      <td>0.2187</td>\n",
       "      <td>5.90</td>\n",
       "      <td>29.62</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.518435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2006-12-30</td>\n",
       "      <td>1.946100e+10</td>\n",
       "      <td>1.666400e+10</td>\n",
       "      <td>8.233000e+09</td>\n",
       "      <td>7.337000e+09</td>\n",
       "      <td>1.122800e+10</td>\n",
       "      <td>7.115000e+09</td>\n",
       "      <td>1.004000e+09</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.14</td>\n",
       "      <td>11.84</td>\n",
       "      <td>13.31</td>\n",
       "      <td>10.37</td>\n",
       "      <td>0.2476</td>\n",
       "      <td>7.13</td>\n",
       "      <td>36.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1174</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.28</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.586591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-03-31</td>\n",
       "      <td>1.871100e+10</td>\n",
       "      <td>1.602900e+10</td>\n",
       "      <td>6.450000e+09</td>\n",
       "      <td>5.485000e+09</td>\n",
       "      <td>1.226100e+10</td>\n",
       "      <td>5.264000e+09</td>\n",
       "      <td>7.700000e+08</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.87</td>\n",
       "      <td>12.84</td>\n",
       "      <td>13.97</td>\n",
       "      <td>11.70</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>6.90</td>\n",
       "      <td>32.57</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.10</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.796202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2007-06-30</td>\n",
       "      <td>2.164700e+10</td>\n",
       "      <td>1.874500e+10</td>\n",
       "      <td>8.243000e+09</td>\n",
       "      <td>6.992000e+09</td>\n",
       "      <td>1.340400e+10</td>\n",
       "      <td>5.410000e+09</td>\n",
       "      <td>8.180000e+08</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>15.52</td>\n",
       "      <td>18.23</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.2674</td>\n",
       "      <td>7.65</td>\n",
       "      <td>34.38</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>8.071351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2007-09-29</td>\n",
       "      <td>2.534700e+10</td>\n",
       "      <td>2.195600e+10</td>\n",
       "      <td>1.081500e+10</td>\n",
       "      <td>9.299000e+09</td>\n",
       "      <td>1.453200e+10</td>\n",
       "      <td>6.217000e+09</td>\n",
       "      <td>9.040000e+08</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>19.05</td>\n",
       "      <td>22.14</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.2719</td>\n",
       "      <td>8.66</td>\n",
       "      <td>37.67</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.732219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2007-12-29</td>\n",
       "      <td>3.003900e+10</td>\n",
       "      <td>2.618900e+10</td>\n",
       "      <td>1.323500e+10</td>\n",
       "      <td>1.053500e+10</td>\n",
       "      <td>1.680400e+10</td>\n",
       "      <td>9.608000e+09</td>\n",
       "      <td>1.581000e+09</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.76</td>\n",
       "      <td>25.25</td>\n",
       "      <td>28.99</td>\n",
       "      <td>21.52</td>\n",
       "      <td>0.2858</td>\n",
       "      <td>10.65</td>\n",
       "      <td>44.97</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1537</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.42</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.430052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008-03-29</td>\n",
       "      <td>3.047100e+10</td>\n",
       "      <td>2.673600e+10</td>\n",
       "      <td>1.241800e+10</td>\n",
       "      <td>9.634000e+09</td>\n",
       "      <td>1.805300e+10</td>\n",
       "      <td>7.512000e+09</td>\n",
       "      <td>1.045000e+09</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.16</td>\n",
       "      <td>22.56</td>\n",
       "      <td>28.64</td>\n",
       "      <td>16.49</td>\n",
       "      <td>0.2770</td>\n",
       "      <td>8.26</td>\n",
       "      <td>34.71</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-8.276958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2008-06-28</td>\n",
       "      <td>3.170900e+10</td>\n",
       "      <td>2.799800e+10</td>\n",
       "      <td>1.208700e+10</td>\n",
       "      <td>9.218000e+09</td>\n",
       "      <td>1.962200e+10</td>\n",
       "      <td>7.464000e+09</td>\n",
       "      <td>1.072000e+09</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.19</td>\n",
       "      <td>23.91</td>\n",
       "      <td>27.46</td>\n",
       "      <td>20.36</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>8.16</td>\n",
       "      <td>34.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1494</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-7.983148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2008-09-27</td>\n",
       "      <td>3.957200e+10</td>\n",
       "      <td>3.469000e+10</td>\n",
       "      <td>1.854200e+10</td>\n",
       "      <td>1.409200e+10</td>\n",
       "      <td>2.103000e+10</td>\n",
       "      <td>7.895000e+09</td>\n",
       "      <td>1.136000e+09</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.25</td>\n",
       "      <td>21.54</td>\n",
       "      <td>25.84</td>\n",
       "      <td>17.24</td>\n",
       "      <td>0.2561</td>\n",
       "      <td>6.82</td>\n",
       "      <td>29.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.63</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.701146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2008-12-27</td>\n",
       "      <td>4.278700e+10</td>\n",
       "      <td>3.516300e+10</td>\n",
       "      <td>1.987800e+10</td>\n",
       "      <td>1.475700e+10</td>\n",
       "      <td>2.290900e+10</td>\n",
       "      <td>1.016700e+10</td>\n",
       "      <td>1.605000e+09</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.78</td>\n",
       "      <td>14.21</td>\n",
       "      <td>17.10</td>\n",
       "      <td>11.31</td>\n",
       "      <td>0.2381</td>\n",
       "      <td>4.20</td>\n",
       "      <td>18.56</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.58</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-23.974958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2009-03-28</td>\n",
       "      <td>4.323700e+10</td>\n",
       "      <td>3.385300e+10</td>\n",
       "      <td>1.892600e+10</td>\n",
       "      <td>1.375100e+10</td>\n",
       "      <td>2.431100e+10</td>\n",
       "      <td>8.163000e+09</td>\n",
       "      <td>1.205000e+09</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.33</td>\n",
       "      <td>13.44</td>\n",
       "      <td>15.71</td>\n",
       "      <td>11.17</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>3.66</td>\n",
       "      <td>17.49</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.12</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-15.397626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2009-06-27</td>\n",
       "      <td>4.814000e+10</td>\n",
       "      <td>3.517000e+10</td>\n",
       "      <td>2.225200e+10</td>\n",
       "      <td>1.666100e+10</td>\n",
       "      <td>2.588800e+10</td>\n",
       "      <td>8.337000e+09</td>\n",
       "      <td>1.229000e+09</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.35</td>\n",
       "      <td>17.79</td>\n",
       "      <td>20.91</td>\n",
       "      <td>14.66</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>4.57</td>\n",
       "      <td>22.44</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>10.245290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2009-09-26</td>\n",
       "      <td>4.750100e+10</td>\n",
       "      <td>3.155500e+10</td>\n",
       "      <td>1.586100e+10</td>\n",
       "      <td>1.150600e+10</td>\n",
       "      <td>3.164000e+10</td>\n",
       "      <td>1.623800e+10</td>\n",
       "      <td>4.196000e+09</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.60</td>\n",
       "      <td>23.09</td>\n",
       "      <td>26.99</td>\n",
       "      <td>19.20</td>\n",
       "      <td>0.3145</td>\n",
       "      <td>5.59</td>\n",
       "      <td>28.31</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1919</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.42</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>13.325551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2009-12-26</td>\n",
       "      <td>5.392600e+10</td>\n",
       "      <td>3.333200e+10</td>\n",
       "      <td>1.815800e+10</td>\n",
       "      <td>1.309700e+10</td>\n",
       "      <td>3.576800e+10</td>\n",
       "      <td>1.568300e+10</td>\n",
       "      <td>3.378000e+09</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.67</td>\n",
       "      <td>27.86</td>\n",
       "      <td>29.91</td>\n",
       "      <td>25.81</td>\n",
       "      <td>0.3404</td>\n",
       "      <td>5.55</td>\n",
       "      <td>21.53</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2067</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.070474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2010-03-27</td>\n",
       "      <td>5.705700e+10</td>\n",
       "      <td>3.233600e+10</td>\n",
       "      <td>1.770900e+10</td>\n",
       "      <td>1.222900e+10</td>\n",
       "      <td>3.934800e+10</td>\n",
       "      <td>1.349900e+10</td>\n",
       "      <td>3.074000e+09</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.33</td>\n",
       "      <td>30.16</td>\n",
       "      <td>33.14</td>\n",
       "      <td>27.18</td>\n",
       "      <td>0.3582</td>\n",
       "      <td>5.36</td>\n",
       "      <td>19.28</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.32</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.030189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2010-06-26</td>\n",
       "      <td>6.472500e+10</td>\n",
       "      <td>3.603300e+10</td>\n",
       "      <td>2.161400e+10</td>\n",
       "      <td>1.561200e+10</td>\n",
       "      <td>4.311100e+10</td>\n",
       "      <td>1.570000e+10</td>\n",
       "      <td>3.253000e+09</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.51</td>\n",
       "      <td>34.16</td>\n",
       "      <td>39.86</td>\n",
       "      <td>28.46</td>\n",
       "      <td>0.3710</td>\n",
       "      <td>5.53</td>\n",
       "      <td>18.46</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.65</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-10.943779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2010-09-25</td>\n",
       "      <td>7.518300e+10</td>\n",
       "      <td>4.167800e+10</td>\n",
       "      <td>2.739200e+10</td>\n",
       "      <td>2.072200e+10</td>\n",
       "      <td>4.779100e+10</td>\n",
       "      <td>2.034300e+10</td>\n",
       "      <td>4.308000e+09</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.64</td>\n",
       "      <td>37.79</td>\n",
       "      <td>41.93</td>\n",
       "      <td>33.65</td>\n",
       "      <td>0.3376</td>\n",
       "      <td>5.61</td>\n",
       "      <td>17.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2148</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.76</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>9.505832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2010-12-25</td>\n",
       "      <td>8.674200e+10</td>\n",
       "      <td>4.392700e+10</td>\n",
       "      <td>3.207600e+10</td>\n",
       "      <td>2.379500e+10</td>\n",
       "      <td>5.466600e+10</td>\n",
       "      <td>2.674100e+10</td>\n",
       "      <td>6.004000e+09</td>\n",
       "      <td>6.53</td>\n",
       "      <td>6.43</td>\n",
       "      <td>42.91</td>\n",
       "      <td>46.53</td>\n",
       "      <td>39.29</td>\n",
       "      <td>0.3599</td>\n",
       "      <td>5.77</td>\n",
       "      <td>19.83</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2181</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.32</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.954396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2011-03-26</td>\n",
       "      <td>9.490400e+10</td>\n",
       "      <td>4.699700e+10</td>\n",
       "      <td>3.342700e+10</td>\n",
       "      <td>2.432700e+10</td>\n",
       "      <td>6.147700e+10</td>\n",
       "      <td>2.466700e+10</td>\n",
       "      <td>5.987000e+09</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6.40</td>\n",
       "      <td>49.02</td>\n",
       "      <td>52.13</td>\n",
       "      <td>45.90</td>\n",
       "      <td>0.3777</td>\n",
       "      <td>5.78</td>\n",
       "      <td>19.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2236</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.86</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.167036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2011-06-25</td>\n",
       "      <td>1.067580e+11</td>\n",
       "      <td>4.689800e+10</td>\n",
       "      <td>3.741500e+10</td>\n",
       "      <td>2.685900e+10</td>\n",
       "      <td>6.934300e+10</td>\n",
       "      <td>2.857100e+10</td>\n",
       "      <td>7.308000e+09</td>\n",
       "      <td>7.89</td>\n",
       "      <td>7.79</td>\n",
       "      <td>47.55</td>\n",
       "      <td>50.73</td>\n",
       "      <td>44.36</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>5.01</td>\n",
       "      <td>15.87</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.56</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.732115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2011-09-24</td>\n",
       "      <td>1.163710e+11</td>\n",
       "      <td>4.498800e+10</td>\n",
       "      <td>3.975600e+10</td>\n",
       "      <td>2.797000e+10</td>\n",
       "      <td>7.661500e+10</td>\n",
       "      <td>2.827000e+10</td>\n",
       "      <td>6.623000e+09</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.06</td>\n",
       "      <td>53.58</td>\n",
       "      <td>60.41</td>\n",
       "      <td>46.75</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>5.01</td>\n",
       "      <td>14.85</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.90</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-13.626579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>1.386810e+11</td>\n",
       "      <td>5.477100e+10</td>\n",
       "      <td>4.862700e+10</td>\n",
       "      <td>3.460700e+10</td>\n",
       "      <td>9.005400e+10</td>\n",
       "      <td>4.633300e+10</td>\n",
       "      <td>1.306400e+10</td>\n",
       "      <td>14.03</td>\n",
       "      <td>13.87</td>\n",
       "      <td>55.78</td>\n",
       "      <td>60.96</td>\n",
       "      <td>50.61</td>\n",
       "      <td>0.4435</td>\n",
       "      <td>4.74</td>\n",
       "      <td>14.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2580</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.47</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>10.715696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>1.509340e+11</td>\n",
       "      <td>5.071200e+10</td>\n",
       "      <td>4.843600e+10</td>\n",
       "      <td>3.203600e+10</td>\n",
       "      <td>1.024980e+11</td>\n",
       "      <td>3.918600e+10</td>\n",
       "      <td>1.162200e+10</td>\n",
       "      <td>12.45</td>\n",
       "      <td>12.30</td>\n",
       "      <td>73.61</td>\n",
       "      <td>88.78</td>\n",
       "      <td>58.43</td>\n",
       "      <td>0.4563</td>\n",
       "      <td>5.33</td>\n",
       "      <td>14.67</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.90</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.846722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>1.628960e+11</td>\n",
       "      <td>5.194300e+10</td>\n",
       "      <td>5.115000e+10</td>\n",
       "      <td>3.306000e+10</td>\n",
       "      <td>1.117460e+11</td>\n",
       "      <td>3.502300e+10</td>\n",
       "      <td>8.824000e+09</td>\n",
       "      <td>9.42</td>\n",
       "      <td>9.32</td>\n",
       "      <td>83.30</td>\n",
       "      <td>92.00</td>\n",
       "      <td>74.60</td>\n",
       "      <td>0.4214</td>\n",
       "      <td>5.32</td>\n",
       "      <td>14.22</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2697</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.10</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.445594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2012-09-29</td>\n",
       "      <td>1.760640e+11</td>\n",
       "      <td>5.765300e+10</td>\n",
       "      <td>5.785400e+10</td>\n",
       "      <td>3.854200e+10</td>\n",
       "      <td>1.182100e+11</td>\n",
       "      <td>3.596600e+10</td>\n",
       "      <td>8.223000e+09</td>\n",
       "      <td>8.76</td>\n",
       "      <td>8.68</td>\n",
       "      <td>91.08</td>\n",
       "      <td>100.72</td>\n",
       "      <td>81.43</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>5.35</td>\n",
       "      <td>14.98</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.238925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2012-12-29</td>\n",
       "      <td>1.960880e+11</td>\n",
       "      <td>7.234800e+10</td>\n",
       "      <td>6.874200e+10</td>\n",
       "      <td>4.687900e+10</td>\n",
       "      <td>1.273460e+11</td>\n",
       "      <td>5.451200e+10</td>\n",
       "      <td>1.307800e+10</td>\n",
       "      <td>13.93</td>\n",
       "      <td>13.81</td>\n",
       "      <td>84.14</td>\n",
       "      <td>96.68</td>\n",
       "      <td>71.60</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>4.69</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.1191</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.19</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.617302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2013-03-30</td>\n",
       "      <td>1.947430e+11</td>\n",
       "      <td>6.333700e+10</td>\n",
       "      <td>5.925300e+10</td>\n",
       "      <td>3.550800e+10</td>\n",
       "      <td>1.354900e+11</td>\n",
       "      <td>4.360300e+10</td>\n",
       "      <td>9.547000e+09</td>\n",
       "      <td>10.16</td>\n",
       "      <td>10.09</td>\n",
       "      <td>69.58</td>\n",
       "      <td>79.29</td>\n",
       "      <td>59.86</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>3.59</td>\n",
       "      <td>11.04</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2346</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.55</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>9.940451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2013-06-29</td>\n",
       "      <td>1.998560e+11</td>\n",
       "      <td>6.821900e+10</td>\n",
       "      <td>7.650200e+10</td>\n",
       "      <td>3.631900e+10</td>\n",
       "      <td>1.233540e+11</td>\n",
       "      <td>3.532300e+10</td>\n",
       "      <td>6.900000e+09</td>\n",
       "      <td>7.51</td>\n",
       "      <td>7.47</td>\n",
       "      <td>60.78</td>\n",
       "      <td>66.54</td>\n",
       "      <td>55.01</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>2.95</td>\n",
       "      <td>10.15</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.2052</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.2228</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.093975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2013-09-28</td>\n",
       "      <td>2.070000e+11</td>\n",
       "      <td>7.328600e+10</td>\n",
       "      <td>8.345100e+10</td>\n",
       "      <td>4.365800e+10</td>\n",
       "      <td>1.235490e+11</td>\n",
       "      <td>3.747200e+10</td>\n",
       "      <td>7.512000e+09</td>\n",
       "      <td>8.31</td>\n",
       "      <td>8.26</td>\n",
       "      <td>65.36</td>\n",
       "      <td>73.39</td>\n",
       "      <td>57.32</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>3.37</td>\n",
       "      <td>11.42</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.2833</td>\n",
       "      <td>0.1373</td>\n",
       "      <td>0.2167</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.21</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.529913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2013-12-28</td>\n",
       "      <td>2.251840e+11</td>\n",
       "      <td>8.034700e+10</td>\n",
       "      <td>9.550000e+10</td>\n",
       "      <td>5.376900e+10</td>\n",
       "      <td>1.296840e+11</td>\n",
       "      <td>5.759400e+10</td>\n",
       "      <td>1.307200e+10</td>\n",
       "      <td>14.59</td>\n",
       "      <td>14.50</td>\n",
       "      <td>74.97</td>\n",
       "      <td>82.16</td>\n",
       "      <td>67.77</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>3.82</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.2896</td>\n",
       "      <td>0.1308</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3.30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>8.876112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2014-03-29</td>\n",
       "      <td>2.059890e+11</td>\n",
       "      <td>7.054100e+10</td>\n",
       "      <td>8.581000e+10</td>\n",
       "      <td>4.320800e+10</td>\n",
       "      <td>1.201790e+11</td>\n",
       "      <td>4.564600e+10</td>\n",
       "      <td>1.022300e+10</td>\n",
       "      <td>11.69</td>\n",
       "      <td>11.62</td>\n",
       "      <td>75.34</td>\n",
       "      <td>80.18</td>\n",
       "      <td>70.51</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>3.63</td>\n",
       "      <td>13.08</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.2142</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.718132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2014-06-28</td>\n",
       "      <td>2.225200e+11</td>\n",
       "      <td>6.794900e+10</td>\n",
       "      <td>1.015800e+11</td>\n",
       "      <td>4.620500e+10</td>\n",
       "      <td>1.209400e+11</td>\n",
       "      <td>3.743200e+10</td>\n",
       "      <td>7.748000e+09</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>84.05</td>\n",
       "      <td>95.05</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.3120</td>\n",
       "      <td>4.22</td>\n",
       "      <td>14.06</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.2829</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.308209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2014-09-27</td>\n",
       "      <td>2.318390e+11</td>\n",
       "      <td>6.853100e+10</td>\n",
       "      <td>1.202920e+11</td>\n",
       "      <td>6.344800e+10</td>\n",
       "      <td>1.115470e+11</td>\n",
       "      <td>4.212300e+10</td>\n",
       "      <td>8.467000e+09</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.42</td>\n",
       "      <td>97.91</td>\n",
       "      <td>103.74</td>\n",
       "      <td>92.09</td>\n",
       "      <td>0.3276</td>\n",
       "      <td>4.85</td>\n",
       "      <td>15.81</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.2778</td>\n",
       "      <td>0.2599</td>\n",
       "      <td>0.2161</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.60</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.434052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2014-12-27</td>\n",
       "      <td>2.618940e+11</td>\n",
       "      <td>8.340300e+10</td>\n",
       "      <td>1.385660e+11</td>\n",
       "      <td>7.361100e+10</td>\n",
       "      <td>1.233280e+11</td>\n",
       "      <td>7.459900e+10</td>\n",
       "      <td>1.802400e+10</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.06</td>\n",
       "      <td>107.47</td>\n",
       "      <td>119.75</td>\n",
       "      <td>95.18</td>\n",
       "      <td>0.3736</td>\n",
       "      <td>5.65</td>\n",
       "      <td>16.71</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.2473</td>\n",
       "      <td>0.2636</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5.23</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.474945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2015-03-28</td>\n",
       "      <td>2.611940e+11</td>\n",
       "      <td>6.789100e+10</td>\n",
       "      <td>1.321880e+11</td>\n",
       "      <td>5.872900e+10</td>\n",
       "      <td>1.290060e+11</td>\n",
       "      <td>5.801000e+10</td>\n",
       "      <td>1.356900e+10</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.33</td>\n",
       "      <td>119.11</td>\n",
       "      <td>133.60</td>\n",
       "      <td>104.63</td>\n",
       "      <td>0.3944</td>\n",
       "      <td>5.63</td>\n",
       "      <td>16.05</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.2317</td>\n",
       "      <td>0.3106</td>\n",
       "      <td>0.2253</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.88</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.155809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2015-06-27</td>\n",
       "      <td>2.731510e+11</td>\n",
       "      <td>7.095300e+10</td>\n",
       "      <td>1.474740e+11</td>\n",
       "      <td>6.528500e+10</td>\n",
       "      <td>1.256770e+11</td>\n",
       "      <td>4.960500e+10</td>\n",
       "      <td>1.067700e+10</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.85</td>\n",
       "      <td>128.82</td>\n",
       "      <td>134.54</td>\n",
       "      <td>123.10</td>\n",
       "      <td>0.4146</td>\n",
       "      <td>5.75</td>\n",
       "      <td>15.92</td>\n",
       "      <td>5.32</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.26</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.807323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2015-09-26</td>\n",
       "      <td>2.904790e+11</td>\n",
       "      <td>8.937800e+10</td>\n",
       "      <td>1.711240e+11</td>\n",
       "      <td>8.061000e+10</td>\n",
       "      <td>1.193550e+11</td>\n",
       "      <td>5.150100e+10</td>\n",
       "      <td>1.112400e+10</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.96</td>\n",
       "      <td>112.48</td>\n",
       "      <td>132.97</td>\n",
       "      <td>92.00</td>\n",
       "      <td>0.4294</td>\n",
       "      <td>5.10</td>\n",
       "      <td>12.99</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.2118</td>\n",
       "      <td>0.4479</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.76</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-8.323556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2015-12-26</td>\n",
       "      <td>2.932840e+11</td>\n",
       "      <td>7.621900e+10</td>\n",
       "      <td>1.650170e+11</td>\n",
       "      <td>7.609200e+10</td>\n",
       "      <td>1.282670e+11</td>\n",
       "      <td>7.587200e+10</td>\n",
       "      <td>1.836100e+10</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.28</td>\n",
       "      <td>114.69</td>\n",
       "      <td>123.82</td>\n",
       "      <td>105.57</td>\n",
       "      <td>0.4279</td>\n",
       "      <td>5.36</td>\n",
       "      <td>12.47</td>\n",
       "      <td>6.36</td>\n",
       "      <td>0.2132</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.2287</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4.23</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.563855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>3.052770e+11</td>\n",
       "      <td>8.759200e+10</td>\n",
       "      <td>1.748200e+11</td>\n",
       "      <td>6.826500e+10</td>\n",
       "      <td>1.304570e+11</td>\n",
       "      <td>5.055700e+10</td>\n",
       "      <td>1.051600e+10</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.90</td>\n",
       "      <td>100.91</td>\n",
       "      <td>109.43</td>\n",
       "      <td>92.39</td>\n",
       "      <td>0.4024</td>\n",
       "      <td>4.36</td>\n",
       "      <td>10.71</td>\n",
       "      <td>6.88</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.5318</td>\n",
       "      <td>0.2227</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.64</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.350354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2016-06-25</td>\n",
       "      <td>3.056020e+11</td>\n",
       "      <td>9.376100e+10</td>\n",
       "      <td>1.790610e+11</td>\n",
       "      <td>7.148600e+10</td>\n",
       "      <td>1.265410e+11</td>\n",
       "      <td>4.235800e+10</td>\n",
       "      <td>7.796000e+09</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.42</td>\n",
       "      <td>100.93</td>\n",
       "      <td>112.39</td>\n",
       "      <td>89.47</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11.23</td>\n",
       "      <td>7.45</td>\n",
       "      <td>0.2448</td>\n",
       "      <td>0.5448</td>\n",
       "      <td>0.2170</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.43</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.366066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>3.216860e+11</td>\n",
       "      <td>1.068690e+11</td>\n",
       "      <td>1.934370e+11</td>\n",
       "      <td>7.900600e+10</td>\n",
       "      <td>1.282490e+11</td>\n",
       "      <td>4.685200e+10</td>\n",
       "      <td>9.014000e+09</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.68</td>\n",
       "      <td>103.84</td>\n",
       "      <td>116.18</td>\n",
       "      <td>91.50</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12.13</td>\n",
       "      <td>8.02</td>\n",
       "      <td>0.2592</td>\n",
       "      <td>0.5881</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.27</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.094360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>3.311410e+11</td>\n",
       "      <td>1.033320e+11</td>\n",
       "      <td>1.987510e+11</td>\n",
       "      <td>8.413000e+10</td>\n",
       "      <td>1.323900e+11</td>\n",
       "      <td>7.835100e+10</td>\n",
       "      <td>1.789100e+10</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.36</td>\n",
       "      <td>111.38</td>\n",
       "      <td>118.69</td>\n",
       "      <td>104.08</td>\n",
       "      <td>0.3494</td>\n",
       "      <td>4.63</td>\n",
       "      <td>13.45</td>\n",
       "      <td>8.59</td>\n",
       "      <td>0.2643</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.69</td>\n",
       "      <td>4.51</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.432220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>3.345320e+11</td>\n",
       "      <td>1.019900e+11</td>\n",
       "      <td>2.004500e+11</td>\n",
       "      <td>7.334200e+10</td>\n",
       "      <td>1.340820e+11</td>\n",
       "      <td>5.289600e+10</td>\n",
       "      <td>1.102900e+10</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.10</td>\n",
       "      <td>129.63</td>\n",
       "      <td>144.50</td>\n",
       "      <td>114.76</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>5.14</td>\n",
       "      <td>15.51</td>\n",
       "      <td>9.16</td>\n",
       "      <td>0.2640</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.82</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.522419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>3.451730e+11</td>\n",
       "      <td>1.128750e+11</td>\n",
       "      <td>2.127480e+11</td>\n",
       "      <td>8.130200e+10</td>\n",
       "      <td>1.324250e+11</td>\n",
       "      <td>4.540800e+10</td>\n",
       "      <td>8.717000e+09</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.67</td>\n",
       "      <td>148.36</td>\n",
       "      <td>156.65</td>\n",
       "      <td>140.06</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>5.77</td>\n",
       "      <td>17.33</td>\n",
       "      <td>9.79</td>\n",
       "      <td>0.2627</td>\n",
       "      <td>0.6786</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.16</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.359579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>3.753190e+11</td>\n",
       "      <td>1.286450e+11</td>\n",
       "      <td>2.412720e+11</td>\n",
       "      <td>1.008140e+11</td>\n",
       "      <td>1.340470e+11</td>\n",
       "      <td>5.257900e+10</td>\n",
       "      <td>1.071400e+10</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.06</td>\n",
       "      <td>153.68</td>\n",
       "      <td>164.94</td>\n",
       "      <td>142.41</td>\n",
       "      <td>0.3629</td>\n",
       "      <td>5.99</td>\n",
       "      <td>17.44</td>\n",
       "      <td>10.42</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.7252</td>\n",
       "      <td>0.2109</td>\n",
       "      <td>0.66</td>\n",
       "      <td>2.27</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.746663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>4.067940e+11</td>\n",
       "      <td>1.438100e+11</td>\n",
       "      <td>2.665950e+11</td>\n",
       "      <td>1.157880e+11</td>\n",
       "      <td>1.401990e+11</td>\n",
       "      <td>8.829300e+10</td>\n",
       "      <td>2.006500e+10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.89</td>\n",
       "      <td>164.83</td>\n",
       "      <td>177.20</td>\n",
       "      <td>152.46</td>\n",
       "      <td>0.3737</td>\n",
       "      <td>6.31</td>\n",
       "      <td>17.94</td>\n",
       "      <td>11.05</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.7412</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.99</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>9.434216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quarter end        Assets  Current Assets   Liabilities  \\\n",
       "0   1996-03-29  5.234000e+09    4.277000e+09  3.178000e+09   \n",
       "1   1996-06-28  5.345000e+09    4.454000e+09  3.325000e+09   \n",
       "2   1996-09-27  5.364000e+09    4.515000e+09  3.306000e+09   \n",
       "3   1996-12-27  5.272000e+09    4.419000e+09  3.330000e+09   \n",
       "4   1997-03-28  4.487000e+09    3.642000e+09  3.245000e+09   \n",
       "5   1997-06-27  4.341000e+09    3.493000e+09  3.145000e+09   \n",
       "6   1997-09-26  4.233000e+09    3.424000e+09  3.033000e+09   \n",
       "7   1997-12-26  4.126000e+09    3.373000e+09  2.882000e+09   \n",
       "8   1998-03-27  3.963000e+09    3.213000e+09  2.575000e+09   \n",
       "9   1998-06-26  4.041000e+09    3.375000e+09  2.555000e+09   \n",
       "10  1998-09-25  4.289000e+09    3.698000e+09  2.647000e+09   \n",
       "11  1998-12-26  4.592000e+09    3.867000e+09  2.669000e+09   \n",
       "12  1999-03-27  4.935000e+09    4.092000e+09  2.760000e+09   \n",
       "13  1999-06-26  5.019000e+09    4.298000e+09  2.049000e+09   \n",
       "14  1999-09-25  5.161000e+09    4.285000e+09  2.057000e+09   \n",
       "15  2000-01-01  7.586000e+09    4.909000e+09  3.172000e+09   \n",
       "16  2000-04-01  7.007000e+09    4.912000e+09  2.792000e+09   \n",
       "17  2000-07-01  6.932000e+09    5.157000e+09  2.756000e+09   \n",
       "18  2000-09-30  6.803000e+09    5.427000e+09  2.696000e+09   \n",
       "19  2000-12-30  5.986000e+09    4.926000e+09  2.274000e+09   \n",
       "20  2001-03-31  6.130000e+09    5.345000e+09  2.392000e+09   \n",
       "21  2001-06-30  6.071000e+09    5.248000e+09  2.213000e+09   \n",
       "22  2001-09-29  6.021000e+09    5.143000e+09  2.101000e+09   \n",
       "23  2001-12-29  6.122000e+09    5.272000e+09  2.156000e+09   \n",
       "24  2002-03-30  6.264000e+09    5.414000e+09  2.250000e+09   \n",
       "25  2002-06-29  6.293000e+09    5.443000e+09  2.227000e+09   \n",
       "26  2002-09-28  6.298000e+09    5.388000e+09  2.203000e+09   \n",
       "27  2002-12-28  6.269000e+09    5.384000e+09  2.153000e+09   \n",
       "28  2003-03-29  6.361000e+09    5.468000e+09  2.222000e+09   \n",
       "29  2003-06-28  6.441000e+09    5.546000e+09  2.243000e+09   \n",
       "30  2003-09-27  6.815000e+09    5.887000e+09  2.592000e+09   \n",
       "31  2003-12-27  6.971000e+09    6.029000e+09  2.641000e+09   \n",
       "32  2004-03-27  6.735000e+09    5.784000e+09  2.245000e+09   \n",
       "33  2004-06-26  7.223000e+09    6.286000e+09  2.411000e+09   \n",
       "34  2004-09-25  8.050000e+09    7.055000e+09  2.974000e+09   \n",
       "35  2004-12-25  9.362000e+09    8.322000e+09  3.572000e+09   \n",
       "36  2005-03-26  1.011100e+10    9.007000e+09  3.725000e+09   \n",
       "37  2005-06-25  1.048800e+10    9.376000e+09  3.667000e+09   \n",
       "38  2005-09-24  1.155100e+10    1.030000e+10  4.085000e+09   \n",
       "39  2005-12-31  1.418100e+10    1.216200e+10  5.801000e+09   \n",
       "40  2006-04-01  1.391100e+10    1.128600e+10  5.229000e+09   \n",
       "41  2006-07-01  1.511400e+10    1.249100e+10  5.784000e+09   \n",
       "42  2006-09-30  1.720500e+10    1.450900e+10  7.221000e+09   \n",
       "43  2006-12-30  1.946100e+10    1.666400e+10  8.233000e+09   \n",
       "44  2007-03-31  1.871100e+10    1.602900e+10  6.450000e+09   \n",
       "45  2007-06-30  2.164700e+10    1.874500e+10  8.243000e+09   \n",
       "46  2007-09-29  2.534700e+10    2.195600e+10  1.081500e+10   \n",
       "47  2007-12-29  3.003900e+10    2.618900e+10  1.323500e+10   \n",
       "48  2008-03-29  3.047100e+10    2.673600e+10  1.241800e+10   \n",
       "49  2008-06-28  3.170900e+10    2.799800e+10  1.208700e+10   \n",
       "50  2008-09-27  3.957200e+10    3.469000e+10  1.854200e+10   \n",
       "51  2008-12-27  4.278700e+10    3.516300e+10  1.987800e+10   \n",
       "52  2009-03-28  4.323700e+10    3.385300e+10  1.892600e+10   \n",
       "53  2009-06-27  4.814000e+10    3.517000e+10  2.225200e+10   \n",
       "54  2009-09-26  4.750100e+10    3.155500e+10  1.586100e+10   \n",
       "55  2009-12-26  5.392600e+10    3.333200e+10  1.815800e+10   \n",
       "56  2010-03-27  5.705700e+10    3.233600e+10  1.770900e+10   \n",
       "57  2010-06-26  6.472500e+10    3.603300e+10  2.161400e+10   \n",
       "58  2010-09-25  7.518300e+10    4.167800e+10  2.739200e+10   \n",
       "59  2010-12-25  8.674200e+10    4.392700e+10  3.207600e+10   \n",
       "60  2011-03-26  9.490400e+10    4.699700e+10  3.342700e+10   \n",
       "61  2011-06-25  1.067580e+11    4.689800e+10  3.741500e+10   \n",
       "62  2011-09-24  1.163710e+11    4.498800e+10  3.975600e+10   \n",
       "63  2011-12-31  1.386810e+11    5.477100e+10  4.862700e+10   \n",
       "64  2012-03-31  1.509340e+11    5.071200e+10  4.843600e+10   \n",
       "65  2012-06-30  1.628960e+11    5.194300e+10  5.115000e+10   \n",
       "66  2012-09-29  1.760640e+11    5.765300e+10  5.785400e+10   \n",
       "67  2012-12-29  1.960880e+11    7.234800e+10  6.874200e+10   \n",
       "68  2013-03-30  1.947430e+11    6.333700e+10  5.925300e+10   \n",
       "69  2013-06-29  1.998560e+11    6.821900e+10  7.650200e+10   \n",
       "70  2013-09-28  2.070000e+11    7.328600e+10  8.345100e+10   \n",
       "71  2013-12-28  2.251840e+11    8.034700e+10  9.550000e+10   \n",
       "72  2014-03-29  2.059890e+11    7.054100e+10  8.581000e+10   \n",
       "73  2014-06-28  2.225200e+11    6.794900e+10  1.015800e+11   \n",
       "74  2014-09-27  2.318390e+11    6.853100e+10  1.202920e+11   \n",
       "75  2014-12-27  2.618940e+11    8.340300e+10  1.385660e+11   \n",
       "76  2015-03-28  2.611940e+11    6.789100e+10  1.321880e+11   \n",
       "77  2015-06-27  2.731510e+11    7.095300e+10  1.474740e+11   \n",
       "78  2015-09-26  2.904790e+11    8.937800e+10  1.711240e+11   \n",
       "79  2015-12-26  2.932840e+11    7.621900e+10  1.650170e+11   \n",
       "80  2016-03-26  3.052770e+11    8.759200e+10  1.748200e+11   \n",
       "81  2016-06-25  3.056020e+11    9.376100e+10  1.790610e+11   \n",
       "82  2016-09-24  3.216860e+11    1.068690e+11  1.934370e+11   \n",
       "83  2016-12-31  3.311410e+11    1.033320e+11  1.987510e+11   \n",
       "84  2017-04-01  3.345320e+11    1.019900e+11  2.004500e+11   \n",
       "85  2017-07-01  3.451730e+11    1.128750e+11  2.127480e+11   \n",
       "86  2017-09-30  3.753190e+11    1.286450e+11  2.412720e+11   \n",
       "87  2017-12-30  4.067940e+11    1.438100e+11  2.665950e+11   \n",
       "\n",
       "    Current Liabilities  Shareholders equity       Revenue      Earnings  \\\n",
       "0          2.273000e+09         2.056000e+09  2.185000e+09 -7.400000e+08   \n",
       "1          1.926000e+09         2.020000e+09  2.179000e+09 -3.200000e+07   \n",
       "2          2.003000e+09         2.058000e+09  2.321000e+09  2.500000e+07   \n",
       "3          2.044000e+09         1.942000e+09  2.129000e+09 -1.200000e+08   \n",
       "4          2.011000e+09         1.242000e+09  1.601000e+09 -7.080000e+08   \n",
       "5          1.910000e+09         1.196000e+09  1.737000e+09 -5.600000e+07   \n",
       "6          1.818000e+09         1.200000e+09  1.614000e+09 -1.610000e+08   \n",
       "7          1.669000e+09         1.244000e+09  1.578000e+09  4.700000e+07   \n",
       "8          1.384000e+09         1.388000e+09  1.405000e+09  5.500000e+07   \n",
       "9          1.389000e+09         1.486000e+09  1.402000e+09  1.010000e+08   \n",
       "10         1.520000e+09         1.642000e+09  1.556000e+09  1.060000e+08   \n",
       "11         1.484000e+09         1.923000e+09  1.710000e+09  1.520000e+08   \n",
       "12         1.544000e+09         2.175000e+09  1.530000e+09  1.350000e+08   \n",
       "13         1.539000e+09         2.970000e+09  1.558000e+09  2.030000e+08   \n",
       "14         1.549000e+09         3.104000e+09  1.336000e+09  1.110000e+08   \n",
       "15         1.965000e+09         4.414000e+09  2.343000e+09  1.830000e+08   \n",
       "16         1.853000e+09         4.215000e+09  1.945000e+09  2.330000e+08   \n",
       "17         1.873000e+09         4.176000e+09  1.825000e+09  2.000000e+08   \n",
       "18         1.933000e+09         4.107000e+09  1.870000e+09  1.700000e+08   \n",
       "19         1.637000e+09         3.712000e+09  1.007000e+09 -1.950000e+08   \n",
       "20         1.795000e+09         3.738000e+09  1.431000e+09  4.300000e+07   \n",
       "21         1.614000e+09         3.858000e+09  1.475000e+09  6.100000e+07   \n",
       "22         1.518000e+09         3.920000e+09  1.450000e+09  6.600000e+07   \n",
       "23         1.579000e+09         3.966000e+09  1.375000e+09  3.800000e+07   \n",
       "24         1.709000e+09         4.014000e+09  1.495000e+09  4.000000e+07   \n",
       "25         1.718000e+09         4.066000e+09  1.429000e+09  3.200000e+07   \n",
       "26         1.658000e+09         4.095000e+09  1.443000e+09 -4.500000e+07   \n",
       "27         1.595000e+09         4.116000e+09  1.472000e+09 -8.000000e+06   \n",
       "28         2.007000e+09         4.139000e+09  1.475000e+09  1.400000e+07   \n",
       "29         2.025000e+09         4.198000e+09  1.545000e+09  1.900000e+07   \n",
       "30         2.357000e+09         4.223000e+09  1.715000e+09  4.400000e+07   \n",
       "31         2.382000e+09         4.330000e+09  2.006000e+09  6.300000e+07   \n",
       "32         1.980000e+09         4.490000e+09  1.909000e+09  4.600000e+07   \n",
       "33         2.155000e+09         4.812000e+09  2.014000e+09  6.100000e+07   \n",
       "34         2.680000e+09         5.076000e+09  2.350000e+09  1.060000e+08   \n",
       "35         3.224000e+09         5.790000e+09  3.490000e+09  2.950000e+08   \n",
       "36         3.352000e+09         6.386000e+09  3.243000e+09  2.900000e+08   \n",
       "37         3.123000e+09         6.821000e+09  3.520000e+09  3.200000e+08   \n",
       "38         3.484000e+09         7.466000e+09  3.678000e+09  4.300000e+08   \n",
       "39         5.060000e+09         8.380000e+09  5.749000e+09  5.650000e+08   \n",
       "40         4.456000e+09         8.682000e+09  4.359000e+09  4.100000e+08   \n",
       "41         5.023000e+09         9.330000e+09  4.370000e+09  4.720000e+08   \n",
       "42         6.471000e+09         9.984000e+09  4.837000e+09  5.420000e+08   \n",
       "43         7.337000e+09         1.122800e+10  7.115000e+09  1.004000e+09   \n",
       "44         5.485000e+09         1.226100e+10  5.264000e+09  7.700000e+08   \n",
       "45         6.992000e+09         1.340400e+10  5.410000e+09  8.180000e+08   \n",
       "46         9.299000e+09         1.453200e+10  6.217000e+09  9.040000e+08   \n",
       "47         1.053500e+10         1.680400e+10  9.608000e+09  1.581000e+09   \n",
       "48         9.634000e+09         1.805300e+10  7.512000e+09  1.045000e+09   \n",
       "49         9.218000e+09         1.962200e+10  7.464000e+09  1.072000e+09   \n",
       "50         1.409200e+10         2.103000e+10  7.895000e+09  1.136000e+09   \n",
       "51         1.475700e+10         2.290900e+10  1.016700e+10  1.605000e+09   \n",
       "52         1.375100e+10         2.431100e+10  8.163000e+09  1.205000e+09   \n",
       "53         1.666100e+10         2.588800e+10  8.337000e+09  1.229000e+09   \n",
       "54         1.150600e+10         3.164000e+10  1.623800e+10  4.196000e+09   \n",
       "55         1.309700e+10         3.576800e+10  1.568300e+10  3.378000e+09   \n",
       "56         1.222900e+10         3.934800e+10  1.349900e+10  3.074000e+09   \n",
       "57         1.561200e+10         4.311100e+10  1.570000e+10  3.253000e+09   \n",
       "58         2.072200e+10         4.779100e+10  2.034300e+10  4.308000e+09   \n",
       "59         2.379500e+10         5.466600e+10  2.674100e+10  6.004000e+09   \n",
       "60         2.432700e+10         6.147700e+10  2.466700e+10  5.987000e+09   \n",
       "61         2.685900e+10         6.934300e+10  2.857100e+10  7.308000e+09   \n",
       "62         2.797000e+10         7.661500e+10  2.827000e+10  6.623000e+09   \n",
       "63         3.460700e+10         9.005400e+10  4.633300e+10  1.306400e+10   \n",
       "64         3.203600e+10         1.024980e+11  3.918600e+10  1.162200e+10   \n",
       "65         3.306000e+10         1.117460e+11  3.502300e+10  8.824000e+09   \n",
       "66         3.854200e+10         1.182100e+11  3.596600e+10  8.223000e+09   \n",
       "67         4.687900e+10         1.273460e+11  5.451200e+10  1.307800e+10   \n",
       "68         3.550800e+10         1.354900e+11  4.360300e+10  9.547000e+09   \n",
       "69         3.631900e+10         1.233540e+11  3.532300e+10  6.900000e+09   \n",
       "70         4.365800e+10         1.235490e+11  3.747200e+10  7.512000e+09   \n",
       "71         5.376900e+10         1.296840e+11  5.759400e+10  1.307200e+10   \n",
       "72         4.320800e+10         1.201790e+11  4.564600e+10  1.022300e+10   \n",
       "73         4.620500e+10         1.209400e+11  3.743200e+10  7.748000e+09   \n",
       "74         6.344800e+10         1.115470e+11  4.212300e+10  8.467000e+09   \n",
       "75         7.361100e+10         1.233280e+11  7.459900e+10  1.802400e+10   \n",
       "76         5.872900e+10         1.290060e+11  5.801000e+10  1.356900e+10   \n",
       "77         6.528500e+10         1.256770e+11  4.960500e+10  1.067700e+10   \n",
       "78         8.061000e+10         1.193550e+11  5.150100e+10  1.112400e+10   \n",
       "79         7.609200e+10         1.282670e+11  7.587200e+10  1.836100e+10   \n",
       "80         6.826500e+10         1.304570e+11  5.055700e+10  1.051600e+10   \n",
       "81         7.148600e+10         1.265410e+11  4.235800e+10  7.796000e+09   \n",
       "82         7.900600e+10         1.282490e+11  4.685200e+10  9.014000e+09   \n",
       "83         8.413000e+10         1.323900e+11  7.835100e+10  1.789100e+10   \n",
       "84         7.334200e+10         1.340820e+11  5.289600e+10  1.102900e+10   \n",
       "85         8.130200e+10         1.324250e+11  4.540800e+10  8.717000e+09   \n",
       "86         1.008140e+11         1.340470e+11  5.257900e+10  1.071400e+10   \n",
       "87         1.157880e+11         1.401990e+11  8.829300e+10  2.006500e+10   \n",
       "\n",
       "    EPS basic  EPS diluted   Price  Price high  Price low     ROE  P/B ratio  \\\n",
       "0       -5.99         0.00    1.04        1.27       0.82 -0.2443       1.28   \n",
       "1       -0.26         0.00    0.86        1.03       0.70 -0.3197       1.46   \n",
       "2        0.21         0.00    0.73        0.89       0.57 -0.3656       1.26   \n",
       "3       -0.96         0.00    0.88        0.99       0.76 -0.4294       1.49   \n",
       "4       -5.64         0.00    0.69        0.83       0.54 -0.4599       1.23   \n",
       "5       -0.44         0.00    0.61        0.71       0.52 -0.5337       1.74   \n",
       "6       -1.26         0.00    0.76        1.06       0.46 -0.7491       2.24   \n",
       "7        0.37         0.33    0.67        0.88       0.46 -0.7194       2.03   \n",
       "8        0.42         0.38    0.73        1.00       0.46 -0.0915       2.21   \n",
       "9        0.76         0.65    1.00        1.13       0.88  0.0316       2.70   \n",
       "10       0.78         0.65    1.28        1.56       1.00  0.2146       3.28   \n",
       "11       1.12         0.95    1.25        1.48       1.02  0.2572       2.91   \n",
       "12       0.99         0.84    1.42        1.69       1.14  0.2735       2.84   \n",
       "13       1.41         1.20    1.49        1.79       1.19  0.2737       2.61   \n",
       "14       0.71         0.62    2.19        2.86       1.51  0.2363       3.32   \n",
       "15       1.14         1.03    3.13        4.21       2.05  0.1996       4.54   \n",
       "16       1.44         1.28    4.23        5.37       3.09  0.1986       4.32   \n",
       "17       0.62         0.55    3.93        4.98       2.87  0.1828       4.27   \n",
       "18       0.51         0.47    3.20        4.58       1.81  0.1859       3.48   \n",
       "19      -0.58        -0.58    1.44        1.91       0.97  0.1007       1.66   \n",
       "20       0.12         0.12    1.36        1.70       1.03  0.0554       1.77   \n",
       "21       0.17         0.17    1.64        1.94       1.34  0.0205       2.16   \n",
       "22       0.20         0.20    1.43        1.80       1.05 -0.0066       1.81   \n",
       "23       0.11         0.11    1.39        1.70       1.07  0.0537       1.74   \n",
       "24       0.11         0.11    1.65        1.82       1.48  0.0520       2.06   \n",
       "25       0.09         0.09    1.52        1.86       1.18  0.0441       1.88   \n",
       "26      -0.13        -0.13    1.17        1.34       1.00  0.0161       1.44   \n",
       "27      -0.02        -0.02    1.10        1.23       0.97  0.0047       1.36   \n",
       "28       0.04         0.04    1.04        1.09       0.99 -0.0017       1.27   \n",
       "29       0.05         0.05    1.16        1.38       0.94 -0.0048       1.43   \n",
       "30       0.12         0.12    1.50        1.65       1.36  0.0166       1.81   \n",
       "31       0.17         0.17    1.59        1.77       1.41  0.0332       1.94   \n",
       "32       0.13         0.12    1.75        1.98       1.51  0.0399       2.08   \n",
       "33       0.16         0.16    2.13        2.41       1.84  0.0479       2.54   \n",
       "34       0.28         0.25    2.40        2.71       2.08  0.0590       2.70   \n",
       "35       0.75         0.70    3.77        4.89       2.66  0.1008       4.19   \n",
       "36       0.36         0.34    5.47        6.44       4.51  0.1363       5.42   \n",
       "37       0.39         0.37    5.56        6.25       4.88  0.1680       5.01   \n",
       "38       0.52         0.50    6.40        7.60       5.20  0.2018       5.47   \n",
       "39       0.68         0.65    8.81       10.78       6.84  0.2210       6.94   \n",
       "40       0.49         0.47   10.29       12.34       8.24  0.2201       7.30   \n",
       "41       0.55         0.54    9.23       10.54       7.92  0.2217       6.32   \n",
       "42       0.64         0.61    9.14       11.11       7.17  0.2187       5.90   \n",
       "43       1.17         1.14   11.84       13.31      10.37  0.2476       7.13   \n",
       "44       0.89         0.87   12.84       13.97      11.70  0.2605       6.90   \n",
       "45       0.94         0.92   15.52       18.23      12.80  0.2674       7.65   \n",
       "46       1.04         1.00   19.05       22.14      15.95  0.2719       8.66   \n",
       "47       1.81         1.76   25.25       28.99      21.52  0.2858      10.65   \n",
       "48       1.19         1.16   22.56       28.64      16.49  0.2770       8.26   \n",
       "49       1.21         1.19   23.91       27.46      20.36  0.2667       8.16   \n",
       "50       1.27         1.25   21.54       25.84      17.24  0.2561       6.82   \n",
       "51       1.81         1.78   14.21       17.10      11.31  0.2381       4.20   \n",
       "52       1.35         1.33   13.44       15.71      11.17  0.2284       3.66   \n",
       "53       1.38         1.35   17.79       20.91      14.66  0.2199       4.57   \n",
       "54       4.66         4.60   23.09       26.99      19.20  0.3145       5.59   \n",
       "55       3.74         3.67   27.86       29.91      25.81  0.3404       5.55   \n",
       "56       3.39         3.33   30.16       33.14      27.18  0.3582       5.36   \n",
       "57       3.57         3.51   34.16       39.86      28.46  0.3710       5.53   \n",
       "58       4.70         4.64   37.79       41.93      33.65  0.3376       5.61   \n",
       "59       6.53         6.43   42.91       46.53      39.29  0.3599       5.77   \n",
       "60       6.49         6.40   49.02       52.13      45.90  0.3777       5.78   \n",
       "61       7.89         7.79   47.55       50.73      44.36  0.4048       5.01   \n",
       "62       7.13         7.06   53.58       60.41      46.75  0.3956       5.01   \n",
       "63      14.03        13.87   55.78       60.96      50.61  0.4435       4.74   \n",
       "64      12.45        12.30   73.61       88.78      58.43  0.4563       5.33   \n",
       "65       9.42         9.32   83.30       92.00      74.60  0.4214       5.32   \n",
       "66       8.76         8.68   91.08      100.72      81.43  0.3951       5.35   \n",
       "67      13.93        13.81   84.14       96.68      71.60  0.3632       4.69   \n",
       "68      10.16        10.09   69.58       79.29      59.86  0.3220       3.59   \n",
       "69       7.51         7.47   60.78       66.54      55.01  0.2993       2.95   \n",
       "70       8.31         8.26   65.36       73.39      57.32  0.2906       3.37   \n",
       "71      14.59        14.50   74.97       82.16      67.77  0.2893       3.82   \n",
       "72      11.69        11.62   75.34       80.18      70.51  0.3036       3.63   \n",
       "73       1.29         1.28   84.05       95.05      73.05  0.3120       4.22   \n",
       "74       1.42         1.42   97.91      103.74      92.09  0.3276       4.85   \n",
       "75       3.08         3.06  107.47      119.75      95.18  0.3736       5.65   \n",
       "76       2.34         2.33  119.11      133.60     104.63  0.3944       5.63   \n",
       "77       1.86         1.85  128.82      134.54     123.10  0.4146       5.75   \n",
       "78       1.98         1.96  112.48      132.97      92.00  0.4294       5.10   \n",
       "79       3.30         3.28  114.69      123.82     105.57  0.4279       5.36   \n",
       "80       1.91         1.90  100.91      109.43      92.39  0.4024       4.36   \n",
       "81       1.43         1.42  100.93      112.39      89.47  0.3789       4.24   \n",
       "82       1.68         1.68  103.84      116.18      91.50  0.3559       4.42   \n",
       "83       3.38         3.36  111.38      118.69     104.08  0.3494       4.63   \n",
       "84       2.11         2.10  129.63      144.50     114.76  0.3509       5.14   \n",
       "85       1.68         1.67  148.36      156.65     140.06  0.3540       5.77   \n",
       "86       2.08         2.06  153.68      164.94     142.41  0.3629       5.99   \n",
       "87       3.92         3.89  164.83      177.20     152.46  0.3737       6.31   \n",
       "\n",
       "    P/E ratio  Cumulative dividends per share  Dividend payout ratio  \\\n",
       "0       21.57                            0.03                 0.0000   \n",
       "1        0.00                            0.03                 0.0000   \n",
       "2        0.00                            0.03                 0.0000   \n",
       "3        0.00                            0.03                 0.0000   \n",
       "4        0.00                            0.03                 0.0000   \n",
       "5        0.00                            0.03                 0.0000   \n",
       "6        0.00                            0.03                 0.0000   \n",
       "7        0.00                            0.03                 0.0000   \n",
       "8        0.00                            0.03                 0.0000   \n",
       "9        0.00                            0.03                 0.0000   \n",
       "10     358.40                            0.03                 0.0000   \n",
       "11      17.41                            0.03                 0.0000   \n",
       "12      15.12                            0.03                 0.0000   \n",
       "13      13.50                            0.03                 0.0000   \n",
       "14      16.85                            0.03                 0.0000   \n",
       "15      24.28                            0.03                 0.0000   \n",
       "16      32.10                            0.03                 0.0000   \n",
       "17      26.64                            0.03                 0.0000   \n",
       "18      22.23                            0.03                 0.0000   \n",
       "19       9.27                            0.03                 0.0000   \n",
       "20      17.63                            0.03                 0.0000   \n",
       "21      41.00                            0.03                 0.0000   \n",
       "22     111.22                            0.03                 0.0000   \n",
       "23       0.00                            0.03                 0.0000   \n",
       "24      38.50                            0.03                 0.0000   \n",
       "25      36.07                            0.03                 0.0000   \n",
       "26      32.12                            0.03                 0.0000   \n",
       "27      85.56                            0.03                 0.0000   \n",
       "28     291.20                            0.03                 0.0000   \n",
       "29       0.00                            0.03                 0.0000   \n",
       "30       0.00                            0.03                 0.0000   \n",
       "31     117.16                            0.03                 0.0000   \n",
       "32      64.47                            0.03                 0.0000   \n",
       "33      64.83                            0.03                 0.0000   \n",
       "34      58.95                            0.03                 0.0000   \n",
       "35      75.40                            0.03                 0.0000   \n",
       "36      62.26                            0.03                 0.0000   \n",
       "37      43.49                            0.03                 0.0000   \n",
       "38      37.81                            0.03                 0.0000   \n",
       "39      39.53                            0.03                 0.0000   \n",
       "40      38.73                            0.03                 0.0000   \n",
       "41      32.47                            0.03                 0.0000   \n",
       "42      29.62                            0.03                 0.0000   \n",
       "43      36.51                            0.03                 0.0000   \n",
       "44      32.57                            0.03                 0.0000   \n",
       "45      34.38                            0.03                 0.0000   \n",
       "46      37.67                            0.03                 0.0000   \n",
       "47      44.97                            0.03                 0.0000   \n",
       "48      34.71                            0.03                 0.0000   \n",
       "49      34.58                            0.03                 0.0000   \n",
       "50      29.51                            0.03                 0.0000   \n",
       "51      18.56                            0.03                 0.0000   \n",
       "52      17.49                            0.03                 0.0000   \n",
       "53      22.44                            0.03                 0.0000   \n",
       "54      28.31                            0.03                 0.0000   \n",
       "55      21.53                            0.03                 0.0000   \n",
       "56      19.28                            0.03                 0.0000   \n",
       "57      18.46                            0.03                 0.0000   \n",
       "58      17.51                            0.03                 0.0000   \n",
       "59      19.83                            0.03                 0.0000   \n",
       "60      19.16                            0.03                 0.0000   \n",
       "61      15.87                            0.03                 0.0000   \n",
       "62      14.85                            0.03                 0.0000   \n",
       "63      14.11                            0.03                 0.0000   \n",
       "64      14.67                            0.03                 0.0000   \n",
       "65      14.22                            0.41                 0.0619   \n",
       "66      14.98                            0.41                 0.0595   \n",
       "67      13.33                            0.79                 0.1191   \n",
       "68      11.04                            1.17                 0.1880   \n",
       "69      10.15                            1.60                 0.2052   \n",
       "70      11.42                            2.04                 0.2833   \n",
       "71      13.24                            2.47                 0.2896   \n",
       "72      13.08                            2.91                 0.2881   \n",
       "73      14.06                            3.38                 0.2829   \n",
       "74      15.81                            3.86                 0.2778   \n",
       "75      16.71                            4.33                 0.2473   \n",
       "76      16.05                            4.80                 0.2317   \n",
       "77      15.92                            5.32                 0.2213   \n",
       "78      12.99                            5.84                 0.2118   \n",
       "79      12.47                            6.36                 0.2132   \n",
       "80      10.71                            6.88                 0.2288   \n",
       "81      11.23                            7.45                 0.2448   \n",
       "82      12.13                            8.02                 0.2592   \n",
       "83      13.45                            8.59                 0.2643   \n",
       "84      15.51                            9.16                 0.2640   \n",
       "85      17.33                            9.79                 0.2627   \n",
       "86      17.44                           10.42                 0.2575   \n",
       "87      17.94                           11.05                 0.2505   \n",
       "\n",
       "    Long-term debt to equity ratio  Net margin  Asset turnover  \\\n",
       "0                           0.1474      0.0000            1.81   \n",
       "1                           0.4698      0.0000            1.80   \n",
       "2                           0.4611      0.0000            1.75   \n",
       "3                           0.4892      0.0000            1.66   \n",
       "4                           0.7665      0.0000            1.61   \n",
       "5                           0.7952      0.0000            1.60   \n",
       "6                           0.7925      0.0000            1.54   \n",
       "7                           0.7653      0.0000            1.52   \n",
       "8                           0.6866      0.0000            1.52   \n",
       "9                           0.6413      0.0070            1.47   \n",
       "10                          0.5810      0.0520            1.45   \n",
       "11                          0.4961      0.0682            1.44   \n",
       "12                          0.4391      0.0797            1.39   \n",
       "13                          0.1010      0.0938            1.35   \n",
       "14                          0.0966      0.0980            1.25   \n",
       "15                          0.0680      0.0934            1.19   \n",
       "16                          0.0712      0.1016            1.16   \n",
       "17                          0.0718      0.0976            1.12   \n",
       "18                          0.0730      0.0985            1.13   \n",
       "19                          0.0838      0.0614            0.99   \n",
       "20                          0.0848      0.0355            0.95   \n",
       "21                          0.0822      0.0137            0.93   \n",
       "22                          0.0809      0.0000            0.89   \n",
       "23                          0.0794      0.0363            0.94   \n",
       "24                          0.0775      0.0354            0.95   \n",
       "25                          0.0777      0.0306            0.93   \n",
       "26                          0.0772      0.0113            0.92   \n",
       "27                          0.0777      0.0033            0.93   \n",
       "28                          0.0000      0.0000            0.92   \n",
       "29                          0.0000      0.0000            0.94   \n",
       "30                          0.0000      0.0111            0.96   \n",
       "31                          0.0000      0.0208            1.01   \n",
       "32                          0.0000      0.0240            1.06   \n",
       "33                          0.0000      0.0280            1.10   \n",
       "34                          0.0000      0.0333            1.14   \n",
       "35                          0.0000      0.0520            1.24   \n",
       "36                          0.0000      0.0678            1.28   \n",
       "37                          0.0000      0.0802            1.33   \n",
       "38                          0.0000      0.0958            1.34   \n",
       "39                          0.0000      0.0991            1.40   \n",
       "40                          0.0000      0.0997            1.38   \n",
       "41                          0.0000      0.1034            1.33   \n",
       "42                          0.0000      0.1030            1.28   \n",
       "43                          0.0000      0.1174            1.26   \n",
       "44                          0.0000      0.1292            1.22   \n",
       "45                          0.0000      0.1385            1.18   \n",
       "46                          0.0000      0.1456            1.13   \n",
       "47                          0.0000      0.1537            1.11   \n",
       "48                          0.0000      0.1513            1.07   \n",
       "49                          0.0000      0.1494            1.05   \n",
       "50                          0.0000      0.1488            0.99   \n",
       "51                          0.0000      0.1470            0.91   \n",
       "52                          0.0000      0.1490            0.86   \n",
       "53                          0.0000      0.1497            0.80   \n",
       "54                          0.0000      0.1919            0.94   \n",
       "55                          0.0000      0.2067            1.00   \n",
       "56                          0.0000      0.2209            1.04   \n",
       "57                          0.0000      0.2274            1.10   \n",
       "58                          0.0000      0.2148            1.04   \n",
       "59                          0.0000      0.2181            1.08   \n",
       "60                          0.0000      0.2236            1.09   \n",
       "61                          0.0000      0.2353            1.10   \n",
       "62                          0.0000      0.2395            1.07   \n",
       "63                          0.0000      0.2580            1.12   \n",
       "64                          0.0000      0.2713            1.11   \n",
       "65                          0.0000      0.2697            1.05   \n",
       "66                          0.0000      0.2667            1.00   \n",
       "67                          0.0000      0.2535            0.96   \n",
       "68                          0.0000      0.2346            0.93   \n",
       "69                          0.1375      0.2228            0.88   \n",
       "70                          0.1373      0.2167            0.86   \n",
       "71                          0.1308      0.2128            0.84   \n",
       "72                          0.1411      0.2142            0.84   \n",
       "73                          0.2400      0.2164            0.83   \n",
       "74                          0.2599      0.2161            0.83   \n",
       "75                          0.2636      0.2225            0.87   \n",
       "76                          0.3106      0.2253            0.87   \n",
       "77                          0.3773      0.2262            0.87   \n",
       "78                          0.4479      0.2285            0.86   \n",
       "79                          0.4148      0.2287            0.84   \n",
       "80                          0.5318      0.2227            0.78   \n",
       "81                          0.5448      0.2170            0.74   \n",
       "82                          0.5881      0.2119            0.70   \n",
       "83                          0.5556      0.2073            0.69   \n",
       "84                          0.6304      0.2074            0.68   \n",
       "85                          0.6786      0.2087            0.67   \n",
       "86                          0.7252      0.2109            0.66   \n",
       "87                          0.7412      0.2112            0.65   \n",
       "\n",
       "    Free cash flow per share ticker  Relative Return DJIA  \n",
       "0                      -0.11   AAPL              1.456728  \n",
       "1                       0.08   AAPL              1.020458  \n",
       "2                       0.11   AAPL              3.717138  \n",
       "3                       0.02   AAPL              8.984578  \n",
       "4                       0.02   AAPL              1.837868  \n",
       "5                      -0.06   AAPL             14.081110  \n",
       "6                       0.05   AAPL              3.675242  \n",
       "7                       0.04   AAPL             -0.586413  \n",
       "8                       0.04   AAPL             10.221135  \n",
       "9                       0.04   AAPL              2.070150  \n",
       "10                      0.08   AAPL            -13.865783  \n",
       "11                      0.06   AAPL             14.558278  \n",
       "12                      0.07   AAPL              6.315441  \n",
       "13                      0.02   AAPL             10.847414  \n",
       "14                      0.04   AAPL             -5.662087  \n",
       "15                      0.07   AAPL             10.520186  \n",
       "16                      0.01   AAPL             -4.915036  \n",
       "17                      0.05   AAPL             -4.608010  \n",
       "18                      0.03   AAPL              1.720470  \n",
       "19                     -0.01   AAPL              0.710145  \n",
       "20                     -0.01   AAPL             -9.247683  \n",
       "21                     -0.01   AAPL              6.143763  \n",
       "22                      0.02   AAPL            -18.831963  \n",
       "23                      0.00   AAPL             11.686242  \n",
       "24                     -0.01   AAPL              3.862965  \n",
       "25                     -0.01   AAPL            -12.635829  \n",
       "26                      0.00   AAPL            -21.981386  \n",
       "27                      0.02   AAPL              8.927623  \n",
       "28                      0.01   AAPL             -4.427597  \n",
       "29                     -0.01   AAPL             11.170046  \n",
       "30                      0.00   AAPL              3.415671  \n",
       "31                      0.03   AAPL             11.336727  \n",
       "32                      0.00   AAPL             -0.828342  \n",
       "33                      0.03   AAPL              0.962485  \n",
       "34                      0.07   AAPL             -3.397054  \n",
       "35                      0.13   AAPL              7.087938  \n",
       "36                      0.09   AAPL             -2.207643  \n",
       "37                      0.07   AAPL             -2.210220  \n",
       "38                      0.11   AAPL              2.930324  \n",
       "39                      0.03   AAPL              1.764946  \n",
       "40                     -0.05   AAPL              3.694939  \n",
       "41                      0.13   AAPL              0.263796  \n",
       "42                      0.15   AAPL              4.518435  \n",
       "43                      0.28   AAPL              6.586591  \n",
       "44                      0.10   AAPL             -0.796202  \n",
       "45                      0.16   AAPL              8.071351  \n",
       "46                      0.24   AAPL              3.732219  \n",
       "47                      0.42   AAPL             -4.430052  \n",
       "48                      0.17   AAPL             -8.276958  \n",
       "49                      0.16   AAPL             -7.983148  \n",
       "50                      0.63   AAPL             -4.701146  \n",
       "51                      0.58   AAPL            -23.974958  \n",
       "52                      0.12   AAPL            -15.397626  \n",
       "53                      0.31   AAPL             10.245290  \n",
       "54                      0.42   AAPL             13.325551  \n",
       "55                      0.85   AAPL              7.070474  \n",
       "56                      0.32   AAPL              4.030189  \n",
       "57                      0.65   AAPL            -10.943779  \n",
       "58                      0.76   AAPL              9.505832  \n",
       "59                      1.32   AAPL              6.954396  \n",
       "60                      0.86   AAPL              6.167036  \n",
       "61                      1.56   AAPL              0.732115  \n",
       "62                      0.90   AAPL            -13.626579  \n",
       "63                      2.47   AAPL             10.715696  \n",
       "64                      1.90   AAPL              7.846722  \n",
       "65                      1.10   AAPL             -2.445594  \n",
       "66                      0.86   AAPL              4.238925  \n",
       "67                      3.19   AAPL             -2.617302  \n",
       "68                      1.55   AAPL              9.940451  \n",
       "69                      0.91   AAPL              2.093975  \n",
       "70                      1.21   AAPL              1.529913  \n",
       "71                      3.30   AAPL              8.876112  \n",
       "72                      2.00   AAPL             -0.718132  \n",
       "73                      1.31   AAPL              2.308209  \n",
       "74                      1.60   AAPL              1.434052  \n",
       "75                      5.23   AAPL              4.474945  \n",
       "76                      2.88   AAPL             -0.155809  \n",
       "77                      2.26   AAPL             -0.807323  \n",
       "78                      1.76   AAPL             -8.323556  \n",
       "79                      4.23   AAPL              6.563855  \n",
       "80                      1.64   AAPL              1.350354  \n",
       "81                      1.43   AAPL              1.366066  \n",
       "82                      2.27   AAPL              2.094360  \n",
       "83                      4.51   AAPL              7.432220  \n",
       "84                      1.82   AAPL              4.522419  \n",
       "85                      1.16   AAPL              3.359579  \n",
       "86                      2.27   AAPL              4.746663  \n",
       "87                      4.99   AAPL              9.434216  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_data_relative_return[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Missing Entries\n",
    "\n",
    "Done before `Trend standardization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_entries(df):\n",
    "    \"\"\"\n",
    "    1. change each series to a numpy array\n",
    "    2. check if all data points present (88 data points), if yes pass, if no then do mean substitution iterating in reverse\n",
    "    2. if value == 0, val(idx+1) + val(idx-1) / 2 , else value == val(idx)\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    df_filled = deepcopy(df)\n",
    "    cols = list(df_filled.drop(columns=['Quarter end', 'ticker']).columns)\n",
    "    for c in cols:\n",
    "        df_temp = df_filled[c]\n",
    "        df_temp_check = df_temp[df_temp != 0]\n",
    "        if df_temp_check.shape[0] != 88:\n",
    "            np_series = df_temp.to_numpy()\n",
    "#             np_series_before = deepcopy(np_series)\n",
    "            for n, val in reversed(list(enumerate(np_series))):\n",
    "                if ((n == 0) and (val == 0)) or ((n == 87) and (val == 0)):\n",
    "                    np_series[n] = (np_series[n] + np_series[n-1]) / 2\n",
    "                elif (val == 0):\n",
    "                    np_series[n] = (np_series[n-1] + np_series[n+1]) / 2\n",
    "                else:\n",
    "                    np_series[n] = np_series[n]\n",
    "            df_filled[c] = np_series\n",
    "    #         print(c, np_series, '\\n', np_series_before)\n",
    "    \n",
    "    return df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing entries for all dfs\n",
    "financial_data_filled = []\n",
    "for df in financial_data_relative_return:\n",
    "    financial_data_filled.append(missing_entries(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter end</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Current Assets</th>\n",
       "      <th>Liabilities</th>\n",
       "      <th>Current Liabilities</th>\n",
       "      <th>Shareholders equity</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>EPS basic</th>\n",
       "      <th>EPS diluted</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price high</th>\n",
       "      <th>Price low</th>\n",
       "      <th>ROE</th>\n",
       "      <th>P/B ratio</th>\n",
       "      <th>P/E ratio</th>\n",
       "      <th>Cumulative dividends per share</th>\n",
       "      <th>Dividend payout ratio</th>\n",
       "      <th>Long-term debt to equity ratio</th>\n",
       "      <th>Net margin</th>\n",
       "      <th>Asset turnover</th>\n",
       "      <th>Free cash flow per share</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Relative Return DJIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-03-29</td>\n",
       "      <td>5.234000e+09</td>\n",
       "      <td>4.277000e+09</td>\n",
       "      <td>3.178000e+09</td>\n",
       "      <td>2.273000e+09</td>\n",
       "      <td>2.056000e+09</td>\n",
       "      <td>2.185000e+09</td>\n",
       "      <td>-7.400000e+08</td>\n",
       "      <td>-5.99</td>\n",
       "      <td>1.945000</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.2443</td>\n",
       "      <td>1.28</td>\n",
       "      <td>21.570</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.252500e-01</td>\n",
       "      <td>1.474000e-01</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>1.81</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.828901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-06-28</td>\n",
       "      <td>5.345000e+09</td>\n",
       "      <td>4.454000e+09</td>\n",
       "      <td>3.325000e+09</td>\n",
       "      <td>1.926000e+09</td>\n",
       "      <td>2.020000e+09</td>\n",
       "      <td>2.179000e+09</td>\n",
       "      <td>-3.200000e+07</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.3197</td>\n",
       "      <td>1.46</td>\n",
       "      <td>11.485</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.355606e-21</td>\n",
       "      <td>4.698000e-01</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.080</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.381029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-09-27</td>\n",
       "      <td>5.364000e+09</td>\n",
       "      <td>4.515000e+09</td>\n",
       "      <td>3.306000e+09</td>\n",
       "      <td>2.003000e+09</td>\n",
       "      <td>2.058000e+09</td>\n",
       "      <td>2.321000e+09</td>\n",
       "      <td>2.500000e+07</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.3656</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.400</td>\n",
       "      <td>0.03</td>\n",
       "      <td>6.711211e-21</td>\n",
       "      <td>4.611000e-01</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.110</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.175122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-12-27</td>\n",
       "      <td>5.272000e+09</td>\n",
       "      <td>4.419000e+09</td>\n",
       "      <td>3.330000e+09</td>\n",
       "      <td>2.044000e+09</td>\n",
       "      <td>1.942000e+09</td>\n",
       "      <td>2.129000e+09</td>\n",
       "      <td>-1.200000e+08</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.020625</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-0.4294</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.800</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.342242e-20</td>\n",
       "      <td>4.892000e-01</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.020</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-9.418520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-03-28</td>\n",
       "      <td>4.487000e+09</td>\n",
       "      <td>3.642000e+09</td>\n",
       "      <td>3.245000e+09</td>\n",
       "      <td>2.011000e+09</td>\n",
       "      <td>1.242000e+09</td>\n",
       "      <td>1.601000e+09</td>\n",
       "      <td>-7.080000e+08</td>\n",
       "      <td>-5.64</td>\n",
       "      <td>0.041250</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.4599</td>\n",
       "      <td>1.23</td>\n",
       "      <td>5.600</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.684485e-20</td>\n",
       "      <td>7.665000e-01</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.020</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.312751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997-06-27</td>\n",
       "      <td>4.341000e+09</td>\n",
       "      <td>3.493000e+09</td>\n",
       "      <td>3.145000e+09</td>\n",
       "      <td>1.910000e+09</td>\n",
       "      <td>1.196000e+09</td>\n",
       "      <td>1.737000e+09</td>\n",
       "      <td>-5.600000e+07</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.5337</td>\n",
       "      <td>1.74</td>\n",
       "      <td>11.200</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.368969e-20</td>\n",
       "      <td>7.952000e-01</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>1.60</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-16.662054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997-09-26</td>\n",
       "      <td>4.233000e+09</td>\n",
       "      <td>3.424000e+09</td>\n",
       "      <td>3.033000e+09</td>\n",
       "      <td>1.818000e+09</td>\n",
       "      <td>1.200000e+09</td>\n",
       "      <td>1.614000e+09</td>\n",
       "      <td>-1.610000e+08</td>\n",
       "      <td>-1.26</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.7491</td>\n",
       "      <td>2.24</td>\n",
       "      <td>22.400</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.073794e-19</td>\n",
       "      <td>7.925000e-01</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.050</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.305219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1997-12-26</td>\n",
       "      <td>4.126000e+09</td>\n",
       "      <td>3.373000e+09</td>\n",
       "      <td>2.882000e+09</td>\n",
       "      <td>1.669000e+09</td>\n",
       "      <td>1.244000e+09</td>\n",
       "      <td>1.578000e+09</td>\n",
       "      <td>4.700000e+07</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.7194</td>\n",
       "      <td>2.03</td>\n",
       "      <td>44.800</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.147588e-19</td>\n",
       "      <td>7.653000e-01</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.040</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.347391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1998-03-27</td>\n",
       "      <td>3.963000e+09</td>\n",
       "      <td>3.213000e+09</td>\n",
       "      <td>2.575000e+09</td>\n",
       "      <td>1.384000e+09</td>\n",
       "      <td>1.388000e+09</td>\n",
       "      <td>1.405000e+09</td>\n",
       "      <td>5.500000e+07</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.0915</td>\n",
       "      <td>2.21</td>\n",
       "      <td>89.600</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.295175e-19</td>\n",
       "      <td>6.866000e-01</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.040</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-11.184244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1998-06-26</td>\n",
       "      <td>4.041000e+09</td>\n",
       "      <td>3.375000e+09</td>\n",
       "      <td>2.555000e+09</td>\n",
       "      <td>1.389000e+09</td>\n",
       "      <td>1.486000e+09</td>\n",
       "      <td>1.402000e+09</td>\n",
       "      <td>1.010000e+08</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>2.70</td>\n",
       "      <td>179.200</td>\n",
       "      <td>0.03</td>\n",
       "      <td>8.590351e-19</td>\n",
       "      <td>6.413000e-01</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.040</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.359833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1998-09-25</td>\n",
       "      <td>4.289000e+09</td>\n",
       "      <td>3.698000e+09</td>\n",
       "      <td>2.647000e+09</td>\n",
       "      <td>1.520000e+09</td>\n",
       "      <td>1.642000e+09</td>\n",
       "      <td>1.556000e+09</td>\n",
       "      <td>1.060000e+08</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2146</td>\n",
       "      <td>3.28</td>\n",
       "      <td>358.400</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.718070e-18</td>\n",
       "      <td>5.810000e-01</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.080</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>12.672734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1998-12-26</td>\n",
       "      <td>4.592000e+09</td>\n",
       "      <td>3.867000e+09</td>\n",
       "      <td>2.669000e+09</td>\n",
       "      <td>1.484000e+09</td>\n",
       "      <td>1.923000e+09</td>\n",
       "      <td>1.710000e+09</td>\n",
       "      <td>1.520000e+08</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>2.91</td>\n",
       "      <td>17.410</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.436140e-18</td>\n",
       "      <td>4.961000e-01</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.060</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-17.094391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1999-03-27</td>\n",
       "      <td>4.935000e+09</td>\n",
       "      <td>4.092000e+09</td>\n",
       "      <td>2.760000e+09</td>\n",
       "      <td>1.544000e+09</td>\n",
       "      <td>2.175000e+09</td>\n",
       "      <td>1.530000e+09</td>\n",
       "      <td>1.350000e+08</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.2735</td>\n",
       "      <td>2.84</td>\n",
       "      <td>15.120</td>\n",
       "      <td>0.03</td>\n",
       "      <td>6.872281e-18</td>\n",
       "      <td>4.391000e-01</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.070</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-6.450447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999-06-26</td>\n",
       "      <td>5.019000e+09</td>\n",
       "      <td>4.298000e+09</td>\n",
       "      <td>2.049000e+09</td>\n",
       "      <td>1.539000e+09</td>\n",
       "      <td>2.970000e+09</td>\n",
       "      <td>1.558000e+09</td>\n",
       "      <td>2.030000e+08</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.2737</td>\n",
       "      <td>2.61</td>\n",
       "      <td>13.500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.374456e-17</td>\n",
       "      <td>1.010000e-01</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.020</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-12.055963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1999-09-25</td>\n",
       "      <td>5.161000e+09</td>\n",
       "      <td>4.285000e+09</td>\n",
       "      <td>2.057000e+09</td>\n",
       "      <td>1.549000e+09</td>\n",
       "      <td>3.104000e+09</td>\n",
       "      <td>1.336000e+09</td>\n",
       "      <td>1.110000e+08</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>2.19</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.2363</td>\n",
       "      <td>3.32</td>\n",
       "      <td>16.850</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.748912e-17</td>\n",
       "      <td>9.660000e-02</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.040</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.247408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>7.586000e+09</td>\n",
       "      <td>4.909000e+09</td>\n",
       "      <td>3.172000e+09</td>\n",
       "      <td>1.965000e+09</td>\n",
       "      <td>4.414000e+09</td>\n",
       "      <td>2.343000e+09</td>\n",
       "      <td>1.830000e+08</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>3.13</td>\n",
       "      <td>4.21</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.1996</td>\n",
       "      <td>4.54</td>\n",
       "      <td>24.280</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.497824e-17</td>\n",
       "      <td>6.800000e-02</td>\n",
       "      <td>0.093400</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.070</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-10.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>7.007000e+09</td>\n",
       "      <td>4.912000e+09</td>\n",
       "      <td>2.792000e+09</td>\n",
       "      <td>1.853000e+09</td>\n",
       "      <td>4.215000e+09</td>\n",
       "      <td>1.945000e+09</td>\n",
       "      <td>2.330000e+08</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>4.23</td>\n",
       "      <td>5.37</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>4.32</td>\n",
       "      <td>32.100</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.099565e-16</td>\n",
       "      <td>7.120000e-02</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>5.354430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>6.932000e+09</td>\n",
       "      <td>5.157000e+09</td>\n",
       "      <td>2.756000e+09</td>\n",
       "      <td>1.873000e+09</td>\n",
       "      <td>4.176000e+09</td>\n",
       "      <td>1.825000e+09</td>\n",
       "      <td>2.000000e+08</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.98</td>\n",
       "      <td>2.87</td>\n",
       "      <td>0.1828</td>\n",
       "      <td>4.27</td>\n",
       "      <td>26.640</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.199130e-16</td>\n",
       "      <td>7.180000e-02</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.050</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.269249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-09-30</td>\n",
       "      <td>6.803000e+09</td>\n",
       "      <td>5.427000e+09</td>\n",
       "      <td>2.696000e+09</td>\n",
       "      <td>1.933000e+09</td>\n",
       "      <td>4.107000e+09</td>\n",
       "      <td>1.870000e+09</td>\n",
       "      <td>1.700000e+08</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.1859</td>\n",
       "      <td>3.48</td>\n",
       "      <td>22.230</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.398260e-16</td>\n",
       "      <td>7.300000e-02</td>\n",
       "      <td>0.098500</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.030</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.129014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-12-30</td>\n",
       "      <td>5.986000e+09</td>\n",
       "      <td>4.926000e+09</td>\n",
       "      <td>2.274000e+09</td>\n",
       "      <td>1.637000e+09</td>\n",
       "      <td>3.712000e+09</td>\n",
       "      <td>1.007000e+09</td>\n",
       "      <td>-1.950000e+08</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.580000</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>1.66</td>\n",
       "      <td>9.270</td>\n",
       "      <td>0.03</td>\n",
       "      <td>8.796519e-16</td>\n",
       "      <td>8.380000e-02</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.826228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>6.130000e+09</td>\n",
       "      <td>5.345000e+09</td>\n",
       "      <td>2.392000e+09</td>\n",
       "      <td>1.795000e+09</td>\n",
       "      <td>3.738000e+09</td>\n",
       "      <td>1.431000e+09</td>\n",
       "      <td>4.300000e+07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.0554</td>\n",
       "      <td>1.77</td>\n",
       "      <td>17.630</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.759304e-15</td>\n",
       "      <td>8.480000e-02</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>8.362750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001-06-30</td>\n",
       "      <td>6.071000e+09</td>\n",
       "      <td>5.248000e+09</td>\n",
       "      <td>2.213000e+09</td>\n",
       "      <td>1.614000e+09</td>\n",
       "      <td>3.858000e+09</td>\n",
       "      <td>1.475000e+09</td>\n",
       "      <td>6.100000e+07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>2.16</td>\n",
       "      <td>41.000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.518608e-15</td>\n",
       "      <td>8.220000e-02</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-6.106840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2001-09-29</td>\n",
       "      <td>6.021000e+09</td>\n",
       "      <td>5.143000e+09</td>\n",
       "      <td>2.101000e+09</td>\n",
       "      <td>1.518000e+09</td>\n",
       "      <td>3.920000e+09</td>\n",
       "      <td>1.450000e+09</td>\n",
       "      <td>6.600000e+07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.0066</td>\n",
       "      <td>1.81</td>\n",
       "      <td>111.220</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.037215e-15</td>\n",
       "      <td>8.090000e-02</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.020</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>15.628731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2001-12-29</td>\n",
       "      <td>6.122000e+09</td>\n",
       "      <td>5.272000e+09</td>\n",
       "      <td>2.156000e+09</td>\n",
       "      <td>1.579000e+09</td>\n",
       "      <td>3.966000e+09</td>\n",
       "      <td>1.375000e+09</td>\n",
       "      <td>3.800000e+07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>1.74</td>\n",
       "      <td>74.860</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.407443e-14</td>\n",
       "      <td>7.940000e-02</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.005</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-13.296489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2002-03-30</td>\n",
       "      <td>6.264000e+09</td>\n",
       "      <td>5.414000e+09</td>\n",
       "      <td>2.250000e+09</td>\n",
       "      <td>1.709000e+09</td>\n",
       "      <td>4.014000e+09</td>\n",
       "      <td>1.495000e+09</td>\n",
       "      <td>4.000000e+07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>2.06</td>\n",
       "      <td>38.500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.814886e-14</td>\n",
       "      <td>7.750000e-02</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.629145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2002-06-29</td>\n",
       "      <td>6.293000e+09</td>\n",
       "      <td>5.443000e+09</td>\n",
       "      <td>2.227000e+09</td>\n",
       "      <td>1.718000e+09</td>\n",
       "      <td>4.066000e+09</td>\n",
       "      <td>1.429000e+09</td>\n",
       "      <td>3.200000e+07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>1.88</td>\n",
       "      <td>36.070</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.629772e-14</td>\n",
       "      <td>7.770000e-02</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>11.077370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2002-09-28</td>\n",
       "      <td>6.298000e+09</td>\n",
       "      <td>5.388000e+09</td>\n",
       "      <td>2.203000e+09</td>\n",
       "      <td>1.658000e+09</td>\n",
       "      <td>4.095000e+09</td>\n",
       "      <td>1.443000e+09</td>\n",
       "      <td>-4.500000e+07</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.130000</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>1.44</td>\n",
       "      <td>32.120</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.125954e-13</td>\n",
       "      <td>7.720000e-02</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.005</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>17.634971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2002-12-28</td>\n",
       "      <td>6.269000e+09</td>\n",
       "      <td>5.384000e+09</td>\n",
       "      <td>2.153000e+09</td>\n",
       "      <td>1.595000e+09</td>\n",
       "      <td>4.116000e+09</td>\n",
       "      <td>1.472000e+09</td>\n",
       "      <td>-8.000000e+06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>1.36</td>\n",
       "      <td>85.560</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.251909e-13</td>\n",
       "      <td>7.770000e-02</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.020</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-9.934788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2003-03-29</td>\n",
       "      <td>6.361000e+09</td>\n",
       "      <td>5.468000e+09</td>\n",
       "      <td>2.222000e+09</td>\n",
       "      <td>2.007000e+09</td>\n",
       "      <td>4.139000e+09</td>\n",
       "      <td>1.475000e+09</td>\n",
       "      <td>1.400000e+07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.0017</td>\n",
       "      <td>1.27</td>\n",
       "      <td>291.200</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.503818e-13</td>\n",
       "      <td>3.885000e-02</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.135283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2003-06-28</td>\n",
       "      <td>6.441000e+09</td>\n",
       "      <td>5.546000e+09</td>\n",
       "      <td>2.243000e+09</td>\n",
       "      <td>2.025000e+09</td>\n",
       "      <td>4.198000e+09</td>\n",
       "      <td>1.545000e+09</td>\n",
       "      <td>1.900000e+07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>1.43</td>\n",
       "      <td>174.890</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.007636e-13</td>\n",
       "      <td>1.250555e-13</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-12.313217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2003-09-27</td>\n",
       "      <td>6.815000e+09</td>\n",
       "      <td>5.887000e+09</td>\n",
       "      <td>2.592000e+09</td>\n",
       "      <td>2.357000e+09</td>\n",
       "      <td>4.223000e+09</td>\n",
       "      <td>1.715000e+09</td>\n",
       "      <td>4.400000e+07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>1.81</td>\n",
       "      <td>58.580</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.801527e-12</td>\n",
       "      <td>2.501110e-13</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.010</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.930111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2003-12-27</td>\n",
       "      <td>6.971000e+09</td>\n",
       "      <td>6.029000e+09</td>\n",
       "      <td>2.641000e+09</td>\n",
       "      <td>2.382000e+09</td>\n",
       "      <td>4.330000e+09</td>\n",
       "      <td>2.006000e+09</td>\n",
       "      <td>6.300000e+07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>1.94</td>\n",
       "      <td>117.160</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.603054e-12</td>\n",
       "      <td>5.002221e-13</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.030</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-12.649999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2004-03-27</td>\n",
       "      <td>6.735000e+09</td>\n",
       "      <td>5.784000e+09</td>\n",
       "      <td>2.245000e+09</td>\n",
       "      <td>1.980000e+09</td>\n",
       "      <td>4.490000e+09</td>\n",
       "      <td>1.909000e+09</td>\n",
       "      <td>4.600000e+07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>2.08</td>\n",
       "      <td>64.470</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.206108e-12</td>\n",
       "      <td>1.000444e-12</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.030</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.021049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2004-06-26</td>\n",
       "      <td>7.223000e+09</td>\n",
       "      <td>6.286000e+09</td>\n",
       "      <td>2.411000e+09</td>\n",
       "      <td>2.155000e+09</td>\n",
       "      <td>4.812000e+09</td>\n",
       "      <td>2.014000e+09</td>\n",
       "      <td>6.100000e+07</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.41</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>2.54</td>\n",
       "      <td>64.830</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.441222e-11</td>\n",
       "      <td>2.000888e-12</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.030</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.533796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2004-09-25</td>\n",
       "      <td>8.050000e+09</td>\n",
       "      <td>7.055000e+09</td>\n",
       "      <td>2.974000e+09</td>\n",
       "      <td>2.680000e+09</td>\n",
       "      <td>5.076000e+09</td>\n",
       "      <td>2.350000e+09</td>\n",
       "      <td>1.060000e+08</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>2.70</td>\n",
       "      <td>58.950</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.882443e-11</td>\n",
       "      <td>4.001777e-12</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.070</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.530629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2004-12-25</td>\n",
       "      <td>9.362000e+09</td>\n",
       "      <td>8.322000e+09</td>\n",
       "      <td>3.572000e+09</td>\n",
       "      <td>3.224000e+09</td>\n",
       "      <td>5.790000e+09</td>\n",
       "      <td>3.490000e+09</td>\n",
       "      <td>2.950000e+08</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>3.77</td>\n",
       "      <td>4.89</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>4.19</td>\n",
       "      <td>75.400</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.764887e-11</td>\n",
       "      <td>8.003553e-12</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.130</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-6.400607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2005-03-26</td>\n",
       "      <td>1.011100e+10</td>\n",
       "      <td>9.007000e+09</td>\n",
       "      <td>3.725000e+09</td>\n",
       "      <td>3.352000e+09</td>\n",
       "      <td>6.386000e+09</td>\n",
       "      <td>3.243000e+09</td>\n",
       "      <td>2.900000e+08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>5.47</td>\n",
       "      <td>6.44</td>\n",
       "      <td>4.51</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>5.42</td>\n",
       "      <td>62.260</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.152977e-10</td>\n",
       "      <td>1.600711e-11</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.090</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.040651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2005-06-25</td>\n",
       "      <td>1.048800e+10</td>\n",
       "      <td>9.376000e+09</td>\n",
       "      <td>3.667000e+09</td>\n",
       "      <td>3.123000e+09</td>\n",
       "      <td>6.821000e+09</td>\n",
       "      <td>3.520000e+09</td>\n",
       "      <td>3.200000e+08</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>5.56</td>\n",
       "      <td>6.25</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>5.01</td>\n",
       "      <td>43.490</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.305955e-10</td>\n",
       "      <td>3.201421e-11</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.070</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.194626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2005-09-24</td>\n",
       "      <td>1.155100e+10</td>\n",
       "      <td>1.030000e+10</td>\n",
       "      <td>4.085000e+09</td>\n",
       "      <td>3.484000e+09</td>\n",
       "      <td>7.466000e+09</td>\n",
       "      <td>3.678000e+09</td>\n",
       "      <td>4.300000e+08</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.40</td>\n",
       "      <td>7.60</td>\n",
       "      <td>5.20</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>5.47</td>\n",
       "      <td>37.810</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.611909e-10</td>\n",
       "      <td>6.402843e-11</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.110</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.707615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>1.418100e+10</td>\n",
       "      <td>1.216200e+10</td>\n",
       "      <td>5.801000e+09</td>\n",
       "      <td>5.060000e+09</td>\n",
       "      <td>8.380000e+09</td>\n",
       "      <td>5.749000e+09</td>\n",
       "      <td>5.650000e+08</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>8.81</td>\n",
       "      <td>10.78</td>\n",
       "      <td>6.84</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>6.94</td>\n",
       "      <td>39.530</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.223819e-10</td>\n",
       "      <td>1.280569e-10</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.030</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.031368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2006-04-01</td>\n",
       "      <td>1.391100e+10</td>\n",
       "      <td>1.128600e+10</td>\n",
       "      <td>5.229000e+09</td>\n",
       "      <td>4.456000e+09</td>\n",
       "      <td>8.682000e+09</td>\n",
       "      <td>4.359000e+09</td>\n",
       "      <td>4.100000e+08</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.29</td>\n",
       "      <td>12.34</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.2201</td>\n",
       "      <td>7.30</td>\n",
       "      <td>38.730</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.844764e-09</td>\n",
       "      <td>2.561137e-10</td>\n",
       "      <td>0.099700</td>\n",
       "      <td>1.38</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.487899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>1.511400e+10</td>\n",
       "      <td>1.249100e+10</td>\n",
       "      <td>5.784000e+09</td>\n",
       "      <td>5.023000e+09</td>\n",
       "      <td>9.330000e+09</td>\n",
       "      <td>4.370000e+09</td>\n",
       "      <td>4.720000e+08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>9.23</td>\n",
       "      <td>10.54</td>\n",
       "      <td>7.92</td>\n",
       "      <td>0.2217</td>\n",
       "      <td>6.32</td>\n",
       "      <td>32.470</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.689528e-09</td>\n",
       "      <td>5.122274e-10</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.130</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.471172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2006-09-30</td>\n",
       "      <td>1.720500e+10</td>\n",
       "      <td>1.450900e+10</td>\n",
       "      <td>7.221000e+09</td>\n",
       "      <td>6.471000e+09</td>\n",
       "      <td>9.984000e+09</td>\n",
       "      <td>4.837000e+09</td>\n",
       "      <td>5.420000e+08</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.14</td>\n",
       "      <td>11.11</td>\n",
       "      <td>7.17</td>\n",
       "      <td>0.2187</td>\n",
       "      <td>5.90</td>\n",
       "      <td>29.620</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.379055e-09</td>\n",
       "      <td>1.024455e-09</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.150</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.752707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2006-12-30</td>\n",
       "      <td>1.946100e+10</td>\n",
       "      <td>1.666400e+10</td>\n",
       "      <td>8.233000e+09</td>\n",
       "      <td>7.337000e+09</td>\n",
       "      <td>1.122800e+10</td>\n",
       "      <td>7.115000e+09</td>\n",
       "      <td>1.004000e+09</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>11.84</td>\n",
       "      <td>13.31</td>\n",
       "      <td>10.37</td>\n",
       "      <td>0.2476</td>\n",
       "      <td>7.13</td>\n",
       "      <td>36.510</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.475811e-08</td>\n",
       "      <td>2.048910e-09</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.280</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-6.418143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-03-31</td>\n",
       "      <td>1.871100e+10</td>\n",
       "      <td>1.602900e+10</td>\n",
       "      <td>6.450000e+09</td>\n",
       "      <td>5.485000e+09</td>\n",
       "      <td>1.226100e+10</td>\n",
       "      <td>5.264000e+09</td>\n",
       "      <td>7.700000e+08</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>12.84</td>\n",
       "      <td>13.97</td>\n",
       "      <td>11.70</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>6.90</td>\n",
       "      <td>32.570</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.951622e-08</td>\n",
       "      <td>4.097819e-09</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.100</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.957433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2007-06-30</td>\n",
       "      <td>2.164700e+10</td>\n",
       "      <td>1.874500e+10</td>\n",
       "      <td>8.243000e+09</td>\n",
       "      <td>6.992000e+09</td>\n",
       "      <td>1.340400e+10</td>\n",
       "      <td>5.410000e+09</td>\n",
       "      <td>8.180000e+08</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>15.52</td>\n",
       "      <td>18.23</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.2674</td>\n",
       "      <td>7.65</td>\n",
       "      <td>34.380</td>\n",
       "      <td>0.03</td>\n",
       "      <td>5.903244e-08</td>\n",
       "      <td>8.195639e-09</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.160</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-8.324871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2007-09-29</td>\n",
       "      <td>2.534700e+10</td>\n",
       "      <td>2.195600e+10</td>\n",
       "      <td>1.081500e+10</td>\n",
       "      <td>9.299000e+09</td>\n",
       "      <td>1.453200e+10</td>\n",
       "      <td>6.217000e+09</td>\n",
       "      <td>9.040000e+08</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.05</td>\n",
       "      <td>22.14</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.2719</td>\n",
       "      <td>8.66</td>\n",
       "      <td>37.670</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.180649e-07</td>\n",
       "      <td>1.639128e-08</td>\n",
       "      <td>0.145600</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.240</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.404618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2007-12-29</td>\n",
       "      <td>3.003900e+10</td>\n",
       "      <td>2.618900e+10</td>\n",
       "      <td>1.323500e+10</td>\n",
       "      <td>1.053500e+10</td>\n",
       "      <td>1.680400e+10</td>\n",
       "      <td>9.608000e+09</td>\n",
       "      <td>1.581000e+09</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>25.25</td>\n",
       "      <td>28.99</td>\n",
       "      <td>21.52</td>\n",
       "      <td>0.2858</td>\n",
       "      <td>10.65</td>\n",
       "      <td>44.970</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.361298e-07</td>\n",
       "      <td>3.278255e-08</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.420</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.865088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008-03-29</td>\n",
       "      <td>3.047100e+10</td>\n",
       "      <td>2.673600e+10</td>\n",
       "      <td>1.241800e+10</td>\n",
       "      <td>9.634000e+09</td>\n",
       "      <td>1.805300e+10</td>\n",
       "      <td>7.512000e+09</td>\n",
       "      <td>1.045000e+09</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>22.56</td>\n",
       "      <td>28.64</td>\n",
       "      <td>16.49</td>\n",
       "      <td>0.2770</td>\n",
       "      <td>8.26</td>\n",
       "      <td>34.710</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.722595e-07</td>\n",
       "      <td>6.556511e-08</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.170</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.446753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2008-06-28</td>\n",
       "      <td>3.170900e+10</td>\n",
       "      <td>2.799800e+10</td>\n",
       "      <td>1.208700e+10</td>\n",
       "      <td>9.218000e+09</td>\n",
       "      <td>1.962200e+10</td>\n",
       "      <td>7.464000e+09</td>\n",
       "      <td>1.072000e+09</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>23.91</td>\n",
       "      <td>27.46</td>\n",
       "      <td>20.36</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>8.16</td>\n",
       "      <td>34.580</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.445190e-07</td>\n",
       "      <td>1.311302e-07</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.160</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.504089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2008-09-27</td>\n",
       "      <td>3.957200e+10</td>\n",
       "      <td>3.469000e+10</td>\n",
       "      <td>1.854200e+10</td>\n",
       "      <td>1.409200e+10</td>\n",
       "      <td>2.103000e+10</td>\n",
       "      <td>7.895000e+09</td>\n",
       "      <td>1.136000e+09</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>21.54</td>\n",
       "      <td>25.84</td>\n",
       "      <td>17.24</td>\n",
       "      <td>0.2561</td>\n",
       "      <td>6.82</td>\n",
       "      <td>29.510</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.889038e-06</td>\n",
       "      <td>2.622604e-07</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.630</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.300434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2008-12-27</td>\n",
       "      <td>4.278700e+10</td>\n",
       "      <td>3.516300e+10</td>\n",
       "      <td>1.987800e+10</td>\n",
       "      <td>1.475700e+10</td>\n",
       "      <td>2.290900e+10</td>\n",
       "      <td>1.016700e+10</td>\n",
       "      <td>1.605000e+09</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.780000</td>\n",
       "      <td>14.21</td>\n",
       "      <td>17.10</td>\n",
       "      <td>11.31</td>\n",
       "      <td>0.2381</td>\n",
       "      <td>4.20</td>\n",
       "      <td>18.560</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.778076e-06</td>\n",
       "      <td>5.245209e-07</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.580</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>18.776236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2009-03-28</td>\n",
       "      <td>4.323700e+10</td>\n",
       "      <td>3.385300e+10</td>\n",
       "      <td>1.892600e+10</td>\n",
       "      <td>1.375100e+10</td>\n",
       "      <td>2.431100e+10</td>\n",
       "      <td>8.163000e+09</td>\n",
       "      <td>1.205000e+09</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>13.44</td>\n",
       "      <td>15.71</td>\n",
       "      <td>11.17</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>3.66</td>\n",
       "      <td>17.490</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.556152e-06</td>\n",
       "      <td>1.049042e-06</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.120</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>13.248207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2009-06-27</td>\n",
       "      <td>4.814000e+10</td>\n",
       "      <td>3.517000e+10</td>\n",
       "      <td>2.225200e+10</td>\n",
       "      <td>1.666100e+10</td>\n",
       "      <td>2.588800e+10</td>\n",
       "      <td>8.337000e+09</td>\n",
       "      <td>1.229000e+09</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>17.79</td>\n",
       "      <td>20.91</td>\n",
       "      <td>14.66</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>4.57</td>\n",
       "      <td>22.440</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.511230e-05</td>\n",
       "      <td>2.098083e-06</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.310</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-10.690780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2009-09-26</td>\n",
       "      <td>4.750100e+10</td>\n",
       "      <td>3.155500e+10</td>\n",
       "      <td>1.586100e+10</td>\n",
       "      <td>1.150600e+10</td>\n",
       "      <td>3.164000e+10</td>\n",
       "      <td>1.623800e+10</td>\n",
       "      <td>4.196000e+09</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>23.09</td>\n",
       "      <td>26.99</td>\n",
       "      <td>19.20</td>\n",
       "      <td>0.3145</td>\n",
       "      <td>5.59</td>\n",
       "      <td>28.310</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.022461e-05</td>\n",
       "      <td>4.196167e-06</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.420</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-14.681126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2009-12-26</td>\n",
       "      <td>5.392600e+10</td>\n",
       "      <td>3.333200e+10</td>\n",
       "      <td>1.815800e+10</td>\n",
       "      <td>1.309700e+10</td>\n",
       "      <td>3.576800e+10</td>\n",
       "      <td>1.568300e+10</td>\n",
       "      <td>3.378000e+09</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>27.86</td>\n",
       "      <td>29.91</td>\n",
       "      <td>25.81</td>\n",
       "      <td>0.3404</td>\n",
       "      <td>5.55</td>\n",
       "      <td>21.530</td>\n",
       "      <td>0.03</td>\n",
       "      <td>6.044922e-05</td>\n",
       "      <td>8.392334e-06</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.850</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-7.163159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2010-03-27</td>\n",
       "      <td>5.705700e+10</td>\n",
       "      <td>3.233600e+10</td>\n",
       "      <td>1.770900e+10</td>\n",
       "      <td>1.222900e+10</td>\n",
       "      <td>3.934800e+10</td>\n",
       "      <td>1.349900e+10</td>\n",
       "      <td>3.074000e+09</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>30.16</td>\n",
       "      <td>33.14</td>\n",
       "      <td>27.18</td>\n",
       "      <td>0.3582</td>\n",
       "      <td>5.36</td>\n",
       "      <td>19.280</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.208984e-04</td>\n",
       "      <td>1.678467e-05</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.320</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.027321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2010-06-26</td>\n",
       "      <td>6.472500e+10</td>\n",
       "      <td>3.603300e+10</td>\n",
       "      <td>2.161400e+10</td>\n",
       "      <td>1.561200e+10</td>\n",
       "      <td>4.311100e+10</td>\n",
       "      <td>1.570000e+10</td>\n",
       "      <td>3.253000e+09</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>34.16</td>\n",
       "      <td>39.86</td>\n",
       "      <td>28.46</td>\n",
       "      <td>0.3710</td>\n",
       "      <td>5.53</td>\n",
       "      <td>18.460</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2.417969e-04</td>\n",
       "      <td>3.356934e-05</td>\n",
       "      <td>0.227400</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.650</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>10.104505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2010-09-25</td>\n",
       "      <td>7.518300e+10</td>\n",
       "      <td>4.167800e+10</td>\n",
       "      <td>2.739200e+10</td>\n",
       "      <td>2.072200e+10</td>\n",
       "      <td>4.779100e+10</td>\n",
       "      <td>2.034300e+10</td>\n",
       "      <td>4.308000e+09</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>37.79</td>\n",
       "      <td>41.93</td>\n",
       "      <td>33.65</td>\n",
       "      <td>0.3376</td>\n",
       "      <td>5.61</td>\n",
       "      <td>17.510</td>\n",
       "      <td>0.03</td>\n",
       "      <td>4.835937e-04</td>\n",
       "      <td>6.713867e-05</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.760</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-10.268484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2010-12-25</td>\n",
       "      <td>8.674200e+10</td>\n",
       "      <td>4.392700e+10</td>\n",
       "      <td>3.207600e+10</td>\n",
       "      <td>2.379500e+10</td>\n",
       "      <td>5.466600e+10</td>\n",
       "      <td>2.674100e+10</td>\n",
       "      <td>6.004000e+09</td>\n",
       "      <td>6.53</td>\n",
       "      <td>6.430000</td>\n",
       "      <td>42.91</td>\n",
       "      <td>46.53</td>\n",
       "      <td>39.29</td>\n",
       "      <td>0.3599</td>\n",
       "      <td>5.77</td>\n",
       "      <td>19.830</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.671875e-04</td>\n",
       "      <td>1.342773e-04</td>\n",
       "      <td>0.218100</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.320</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-7.182426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2011-03-26</td>\n",
       "      <td>9.490400e+10</td>\n",
       "      <td>4.699700e+10</td>\n",
       "      <td>3.342700e+10</td>\n",
       "      <td>2.432700e+10</td>\n",
       "      <td>6.147700e+10</td>\n",
       "      <td>2.466700e+10</td>\n",
       "      <td>5.987000e+09</td>\n",
       "      <td>6.49</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>49.02</td>\n",
       "      <td>52.13</td>\n",
       "      <td>45.90</td>\n",
       "      <td>0.3777</td>\n",
       "      <td>5.78</td>\n",
       "      <td>19.160</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.934375e-03</td>\n",
       "      <td>2.685547e-04</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.860</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-6.268487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2011-06-25</td>\n",
       "      <td>1.067580e+11</td>\n",
       "      <td>4.689800e+10</td>\n",
       "      <td>3.741500e+10</td>\n",
       "      <td>2.685900e+10</td>\n",
       "      <td>6.934300e+10</td>\n",
       "      <td>2.857100e+10</td>\n",
       "      <td>7.308000e+09</td>\n",
       "      <td>7.89</td>\n",
       "      <td>7.790000</td>\n",
       "      <td>47.55</td>\n",
       "      <td>50.73</td>\n",
       "      <td>44.36</td>\n",
       "      <td>0.4048</td>\n",
       "      <td>5.01</td>\n",
       "      <td>15.870</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.868750e-03</td>\n",
       "      <td>5.371094e-04</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.560</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.797943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2011-09-24</td>\n",
       "      <td>1.163710e+11</td>\n",
       "      <td>4.498800e+10</td>\n",
       "      <td>3.975600e+10</td>\n",
       "      <td>2.797000e+10</td>\n",
       "      <td>7.661500e+10</td>\n",
       "      <td>2.827000e+10</td>\n",
       "      <td>6.623000e+09</td>\n",
       "      <td>7.13</td>\n",
       "      <td>7.060000</td>\n",
       "      <td>53.58</td>\n",
       "      <td>60.41</td>\n",
       "      <td>46.75</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>5.01</td>\n",
       "      <td>14.850</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.737500e-03</td>\n",
       "      <td>1.074219e-03</td>\n",
       "      <td>0.239500</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.900</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>12.217348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>1.386810e+11</td>\n",
       "      <td>5.477100e+10</td>\n",
       "      <td>4.862700e+10</td>\n",
       "      <td>3.460700e+10</td>\n",
       "      <td>9.005400e+10</td>\n",
       "      <td>4.633300e+10</td>\n",
       "      <td>1.306400e+10</td>\n",
       "      <td>14.03</td>\n",
       "      <td>13.870000</td>\n",
       "      <td>55.78</td>\n",
       "      <td>60.96</td>\n",
       "      <td>50.61</td>\n",
       "      <td>0.4435</td>\n",
       "      <td>4.74</td>\n",
       "      <td>14.110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.547500e-02</td>\n",
       "      <td>2.148438e-03</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.470</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-11.909225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>1.509340e+11</td>\n",
       "      <td>5.071200e+10</td>\n",
       "      <td>4.843600e+10</td>\n",
       "      <td>3.203600e+10</td>\n",
       "      <td>1.024980e+11</td>\n",
       "      <td>3.918600e+10</td>\n",
       "      <td>1.162200e+10</td>\n",
       "      <td>12.45</td>\n",
       "      <td>12.300000</td>\n",
       "      <td>73.61</td>\n",
       "      <td>88.78</td>\n",
       "      <td>58.43</td>\n",
       "      <td>0.4563</td>\n",
       "      <td>5.33</td>\n",
       "      <td>14.670</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.095000e-02</td>\n",
       "      <td>4.296875e-03</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.900</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-7.820111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>1.628960e+11</td>\n",
       "      <td>5.194300e+10</td>\n",
       "      <td>5.115000e+10</td>\n",
       "      <td>3.306000e+10</td>\n",
       "      <td>1.117460e+11</td>\n",
       "      <td>3.502300e+10</td>\n",
       "      <td>8.824000e+09</td>\n",
       "      <td>9.42</td>\n",
       "      <td>9.320000</td>\n",
       "      <td>83.30</td>\n",
       "      <td>92.00</td>\n",
       "      <td>74.60</td>\n",
       "      <td>0.4214</td>\n",
       "      <td>5.32</td>\n",
       "      <td>14.220</td>\n",
       "      <td>0.41</td>\n",
       "      <td>6.190000e-02</td>\n",
       "      <td>8.593750e-03</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.100</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.644121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2012-09-29</td>\n",
       "      <td>1.760640e+11</td>\n",
       "      <td>5.765300e+10</td>\n",
       "      <td>5.785400e+10</td>\n",
       "      <td>3.854200e+10</td>\n",
       "      <td>1.182100e+11</td>\n",
       "      <td>3.596600e+10</td>\n",
       "      <td>8.223000e+09</td>\n",
       "      <td>8.76</td>\n",
       "      <td>8.680000</td>\n",
       "      <td>91.08</td>\n",
       "      <td>100.72</td>\n",
       "      <td>81.43</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>5.35</td>\n",
       "      <td>14.980</td>\n",
       "      <td>0.41</td>\n",
       "      <td>5.950000e-02</td>\n",
       "      <td>1.718750e-02</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.860</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.231417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2012-12-29</td>\n",
       "      <td>1.960880e+11</td>\n",
       "      <td>7.234800e+10</td>\n",
       "      <td>6.874200e+10</td>\n",
       "      <td>4.687900e+10</td>\n",
       "      <td>1.273460e+11</td>\n",
       "      <td>5.451200e+10</td>\n",
       "      <td>1.307800e+10</td>\n",
       "      <td>13.93</td>\n",
       "      <td>13.810000</td>\n",
       "      <td>84.14</td>\n",
       "      <td>96.68</td>\n",
       "      <td>71.60</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>4.69</td>\n",
       "      <td>13.330</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.191000e-01</td>\n",
       "      <td>3.437500e-02</td>\n",
       "      <td>0.253500</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.190</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.401937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2013-03-30</td>\n",
       "      <td>1.947430e+11</td>\n",
       "      <td>6.333700e+10</td>\n",
       "      <td>5.925300e+10</td>\n",
       "      <td>3.550800e+10</td>\n",
       "      <td>1.354900e+11</td>\n",
       "      <td>4.360300e+10</td>\n",
       "      <td>9.547000e+09</td>\n",
       "      <td>10.16</td>\n",
       "      <td>10.090000</td>\n",
       "      <td>69.58</td>\n",
       "      <td>79.29</td>\n",
       "      <td>59.86</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>3.59</td>\n",
       "      <td>11.040</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.880000e-01</td>\n",
       "      <td>6.875000e-02</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1.550</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-11.424451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2013-06-29</td>\n",
       "      <td>1.998560e+11</td>\n",
       "      <td>6.821900e+10</td>\n",
       "      <td>7.650200e+10</td>\n",
       "      <td>3.631900e+10</td>\n",
       "      <td>1.233540e+11</td>\n",
       "      <td>3.532300e+10</td>\n",
       "      <td>6.900000e+09</td>\n",
       "      <td>7.51</td>\n",
       "      <td>7.470000</td>\n",
       "      <td>60.78</td>\n",
       "      <td>66.54</td>\n",
       "      <td>55.01</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>2.95</td>\n",
       "      <td>10.150</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.052000e-01</td>\n",
       "      <td>1.375000e-01</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.910</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.397345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2013-09-28</td>\n",
       "      <td>2.070000e+11</td>\n",
       "      <td>7.328600e+10</td>\n",
       "      <td>8.345100e+10</td>\n",
       "      <td>4.365800e+10</td>\n",
       "      <td>1.235490e+11</td>\n",
       "      <td>3.747200e+10</td>\n",
       "      <td>7.512000e+09</td>\n",
       "      <td>8.31</td>\n",
       "      <td>8.260000</td>\n",
       "      <td>65.36</td>\n",
       "      <td>73.39</td>\n",
       "      <td>57.32</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>3.37</td>\n",
       "      <td>11.420</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.833000e-01</td>\n",
       "      <td>1.373000e-01</td>\n",
       "      <td>0.216700</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.210</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.400675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2013-12-28</td>\n",
       "      <td>2.251840e+11</td>\n",
       "      <td>8.034700e+10</td>\n",
       "      <td>9.550000e+10</td>\n",
       "      <td>5.376900e+10</td>\n",
       "      <td>1.296840e+11</td>\n",
       "      <td>5.759400e+10</td>\n",
       "      <td>1.307200e+10</td>\n",
       "      <td>14.59</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>74.97</td>\n",
       "      <td>82.16</td>\n",
       "      <td>67.77</td>\n",
       "      <td>0.2893</td>\n",
       "      <td>3.82</td>\n",
       "      <td>13.240</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.896000e-01</td>\n",
       "      <td>1.308000e-01</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3.300</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-9.416891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2014-03-29</td>\n",
       "      <td>2.059890e+11</td>\n",
       "      <td>7.054100e+10</td>\n",
       "      <td>8.581000e+10</td>\n",
       "      <td>4.320800e+10</td>\n",
       "      <td>1.201790e+11</td>\n",
       "      <td>4.564600e+10</td>\n",
       "      <td>1.022300e+10</td>\n",
       "      <td>11.69</td>\n",
       "      <td>11.620000</td>\n",
       "      <td>75.34</td>\n",
       "      <td>80.18</td>\n",
       "      <td>70.51</td>\n",
       "      <td>0.3036</td>\n",
       "      <td>3.63</td>\n",
       "      <td>13.080</td>\n",
       "      <td>2.91</td>\n",
       "      <td>2.881000e-01</td>\n",
       "      <td>1.411000e-01</td>\n",
       "      <td>0.214200</td>\n",
       "      <td>0.84</td>\n",
       "      <td>2.000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.722812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2014-06-28</td>\n",
       "      <td>2.225200e+11</td>\n",
       "      <td>6.794900e+10</td>\n",
       "      <td>1.015800e+11</td>\n",
       "      <td>4.620500e+10</td>\n",
       "      <td>1.209400e+11</td>\n",
       "      <td>3.743200e+10</td>\n",
       "      <td>7.748000e+09</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>84.05</td>\n",
       "      <td>95.05</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.3120</td>\n",
       "      <td>4.22</td>\n",
       "      <td>14.060</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.829000e-01</td>\n",
       "      <td>2.400000e-01</td>\n",
       "      <td>0.216400</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.310</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.126143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2014-09-27</td>\n",
       "      <td>2.318390e+11</td>\n",
       "      <td>6.853100e+10</td>\n",
       "      <td>1.202920e+11</td>\n",
       "      <td>6.344800e+10</td>\n",
       "      <td>1.115470e+11</td>\n",
       "      <td>4.212300e+10</td>\n",
       "      <td>8.467000e+09</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>97.91</td>\n",
       "      <td>103.74</td>\n",
       "      <td>92.09</td>\n",
       "      <td>0.3276</td>\n",
       "      <td>4.85</td>\n",
       "      <td>15.810</td>\n",
       "      <td>3.86</td>\n",
       "      <td>2.778000e-01</td>\n",
       "      <td>2.599000e-01</td>\n",
       "      <td>0.216100</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.120563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2014-12-27</td>\n",
       "      <td>2.618940e+11</td>\n",
       "      <td>8.340300e+10</td>\n",
       "      <td>1.385660e+11</td>\n",
       "      <td>7.361100e+10</td>\n",
       "      <td>1.233280e+11</td>\n",
       "      <td>7.459900e+10</td>\n",
       "      <td>1.802400e+10</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.060000</td>\n",
       "      <td>107.47</td>\n",
       "      <td>119.75</td>\n",
       "      <td>95.18</td>\n",
       "      <td>0.3736</td>\n",
       "      <td>5.65</td>\n",
       "      <td>16.710</td>\n",
       "      <td>4.33</td>\n",
       "      <td>2.473000e-01</td>\n",
       "      <td>2.636000e-01</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5.230</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.480043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2015-03-28</td>\n",
       "      <td>2.611940e+11</td>\n",
       "      <td>6.789100e+10</td>\n",
       "      <td>1.321880e+11</td>\n",
       "      <td>5.872900e+10</td>\n",
       "      <td>1.290060e+11</td>\n",
       "      <td>5.801000e+10</td>\n",
       "      <td>1.356900e+10</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>119.11</td>\n",
       "      <td>133.60</td>\n",
       "      <td>104.63</td>\n",
       "      <td>0.3944</td>\n",
       "      <td>5.63</td>\n",
       "      <td>16.050</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.317000e-01</td>\n",
       "      <td>3.106000e-01</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.880</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.371732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2015-06-27</td>\n",
       "      <td>2.731510e+11</td>\n",
       "      <td>7.095300e+10</td>\n",
       "      <td>1.474740e+11</td>\n",
       "      <td>6.528500e+10</td>\n",
       "      <td>1.256770e+11</td>\n",
       "      <td>4.960500e+10</td>\n",
       "      <td>1.067700e+10</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>128.82</td>\n",
       "      <td>134.54</td>\n",
       "      <td>123.10</td>\n",
       "      <td>0.4146</td>\n",
       "      <td>5.75</td>\n",
       "      <td>15.920</td>\n",
       "      <td>5.32</td>\n",
       "      <td>2.213000e-01</td>\n",
       "      <td>3.773000e-01</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.260</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.962535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2015-09-26</td>\n",
       "      <td>2.904790e+11</td>\n",
       "      <td>8.937800e+10</td>\n",
       "      <td>1.711240e+11</td>\n",
       "      <td>8.061000e+10</td>\n",
       "      <td>1.193550e+11</td>\n",
       "      <td>5.150100e+10</td>\n",
       "      <td>1.112400e+10</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>112.48</td>\n",
       "      <td>132.97</td>\n",
       "      <td>92.00</td>\n",
       "      <td>0.4294</td>\n",
       "      <td>5.10</td>\n",
       "      <td>12.990</td>\n",
       "      <td>5.84</td>\n",
       "      <td>2.118000e-01</td>\n",
       "      <td>4.479000e-01</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.760</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.448906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2015-12-26</td>\n",
       "      <td>2.932840e+11</td>\n",
       "      <td>7.621900e+10</td>\n",
       "      <td>1.650170e+11</td>\n",
       "      <td>7.609200e+10</td>\n",
       "      <td>1.282670e+11</td>\n",
       "      <td>7.587200e+10</td>\n",
       "      <td>1.836100e+10</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>114.69</td>\n",
       "      <td>123.82</td>\n",
       "      <td>105.57</td>\n",
       "      <td>0.4279</td>\n",
       "      <td>5.36</td>\n",
       "      <td>12.470</td>\n",
       "      <td>6.36</td>\n",
       "      <td>2.132000e-01</td>\n",
       "      <td>4.148000e-01</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4.230</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-6.982814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>3.052770e+11</td>\n",
       "      <td>8.759200e+10</td>\n",
       "      <td>1.748200e+11</td>\n",
       "      <td>6.826500e+10</td>\n",
       "      <td>1.304570e+11</td>\n",
       "      <td>5.055700e+10</td>\n",
       "      <td>1.051600e+10</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>100.91</td>\n",
       "      <td>109.43</td>\n",
       "      <td>92.39</td>\n",
       "      <td>0.4024</td>\n",
       "      <td>4.36</td>\n",
       "      <td>10.710</td>\n",
       "      <td>6.88</td>\n",
       "      <td>2.288000e-01</td>\n",
       "      <td>5.318000e-01</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.640</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.612601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2016-06-25</td>\n",
       "      <td>3.056020e+11</td>\n",
       "      <td>9.376100e+10</td>\n",
       "      <td>1.790610e+11</td>\n",
       "      <td>7.148600e+10</td>\n",
       "      <td>1.265410e+11</td>\n",
       "      <td>4.235800e+10</td>\n",
       "      <td>7.796000e+09</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>100.93</td>\n",
       "      <td>112.39</td>\n",
       "      <td>89.47</td>\n",
       "      <td>0.3789</td>\n",
       "      <td>4.24</td>\n",
       "      <td>11.230</td>\n",
       "      <td>7.45</td>\n",
       "      <td>2.448000e-01</td>\n",
       "      <td>5.448000e-01</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.430</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.384584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>3.216860e+11</td>\n",
       "      <td>1.068690e+11</td>\n",
       "      <td>1.934370e+11</td>\n",
       "      <td>7.900600e+10</td>\n",
       "      <td>1.282490e+11</td>\n",
       "      <td>4.685200e+10</td>\n",
       "      <td>9.014000e+09</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>103.84</td>\n",
       "      <td>116.18</td>\n",
       "      <td>91.50</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>4.42</td>\n",
       "      <td>12.130</td>\n",
       "      <td>8.02</td>\n",
       "      <td>2.592000e-01</td>\n",
       "      <td>5.881000e-01</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.270</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.080260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>3.311410e+11</td>\n",
       "      <td>1.033320e+11</td>\n",
       "      <td>1.987510e+11</td>\n",
       "      <td>8.413000e+10</td>\n",
       "      <td>1.323900e+11</td>\n",
       "      <td>7.835100e+10</td>\n",
       "      <td>1.789100e+10</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>111.38</td>\n",
       "      <td>118.69</td>\n",
       "      <td>104.08</td>\n",
       "      <td>0.3494</td>\n",
       "      <td>4.63</td>\n",
       "      <td>13.450</td>\n",
       "      <td>8.59</td>\n",
       "      <td>2.643000e-01</td>\n",
       "      <td>5.556000e-01</td>\n",
       "      <td>0.207300</td>\n",
       "      <td>0.69</td>\n",
       "      <td>4.510</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-7.871664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>3.345320e+11</td>\n",
       "      <td>1.019900e+11</td>\n",
       "      <td>2.004500e+11</td>\n",
       "      <td>7.334200e+10</td>\n",
       "      <td>1.340820e+11</td>\n",
       "      <td>5.289600e+10</td>\n",
       "      <td>1.102900e+10</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>129.63</td>\n",
       "      <td>144.50</td>\n",
       "      <td>114.76</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>5.14</td>\n",
       "      <td>15.510</td>\n",
       "      <td>9.16</td>\n",
       "      <td>2.640000e-01</td>\n",
       "      <td>6.304000e-01</td>\n",
       "      <td>0.207400</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.820</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.393340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>3.451730e+11</td>\n",
       "      <td>1.128750e+11</td>\n",
       "      <td>2.127480e+11</td>\n",
       "      <td>8.130200e+10</td>\n",
       "      <td>1.324250e+11</td>\n",
       "      <td>4.540800e+10</td>\n",
       "      <td>8.717000e+09</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.670000</td>\n",
       "      <td>148.36</td>\n",
       "      <td>156.65</td>\n",
       "      <td>140.06</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>5.77</td>\n",
       "      <td>17.330</td>\n",
       "      <td>9.79</td>\n",
       "      <td>2.627000e-01</td>\n",
       "      <td>6.786000e-01</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.160</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.111269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>3.753190e+11</td>\n",
       "      <td>1.286450e+11</td>\n",
       "      <td>2.412720e+11</td>\n",
       "      <td>1.008140e+11</td>\n",
       "      <td>1.340470e+11</td>\n",
       "      <td>5.257900e+10</td>\n",
       "      <td>1.071400e+10</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>153.68</td>\n",
       "      <td>164.94</td>\n",
       "      <td>142.41</td>\n",
       "      <td>0.3629</td>\n",
       "      <td>5.99</td>\n",
       "      <td>17.440</td>\n",
       "      <td>10.42</td>\n",
       "      <td>2.575000e-01</td>\n",
       "      <td>7.252000e-01</td>\n",
       "      <td>0.210900</td>\n",
       "      <td>0.66</td>\n",
       "      <td>2.270</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.013578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>4.067940e+11</td>\n",
       "      <td>1.438100e+11</td>\n",
       "      <td>2.665950e+11</td>\n",
       "      <td>1.157880e+11</td>\n",
       "      <td>1.401990e+11</td>\n",
       "      <td>8.829300e+10</td>\n",
       "      <td>2.006500e+10</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.890000</td>\n",
       "      <td>164.83</td>\n",
       "      <td>177.20</td>\n",
       "      <td>152.46</td>\n",
       "      <td>0.3737</td>\n",
       "      <td>6.31</td>\n",
       "      <td>17.940</td>\n",
       "      <td>11.05</td>\n",
       "      <td>2.505000e-01</td>\n",
       "      <td>7.412000e-01</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.990</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-10.256037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quarter end        Assets  Current Assets   Liabilities  \\\n",
       "0   1996-03-29  5.234000e+09    4.277000e+09  3.178000e+09   \n",
       "1   1996-06-28  5.345000e+09    4.454000e+09  3.325000e+09   \n",
       "2   1996-09-27  5.364000e+09    4.515000e+09  3.306000e+09   \n",
       "3   1996-12-27  5.272000e+09    4.419000e+09  3.330000e+09   \n",
       "4   1997-03-28  4.487000e+09    3.642000e+09  3.245000e+09   \n",
       "5   1997-06-27  4.341000e+09    3.493000e+09  3.145000e+09   \n",
       "6   1997-09-26  4.233000e+09    3.424000e+09  3.033000e+09   \n",
       "7   1997-12-26  4.126000e+09    3.373000e+09  2.882000e+09   \n",
       "8   1998-03-27  3.963000e+09    3.213000e+09  2.575000e+09   \n",
       "9   1998-06-26  4.041000e+09    3.375000e+09  2.555000e+09   \n",
       "10  1998-09-25  4.289000e+09    3.698000e+09  2.647000e+09   \n",
       "11  1998-12-26  4.592000e+09    3.867000e+09  2.669000e+09   \n",
       "12  1999-03-27  4.935000e+09    4.092000e+09  2.760000e+09   \n",
       "13  1999-06-26  5.019000e+09    4.298000e+09  2.049000e+09   \n",
       "14  1999-09-25  5.161000e+09    4.285000e+09  2.057000e+09   \n",
       "15  2000-01-01  7.586000e+09    4.909000e+09  3.172000e+09   \n",
       "16  2000-04-01  7.007000e+09    4.912000e+09  2.792000e+09   \n",
       "17  2000-07-01  6.932000e+09    5.157000e+09  2.756000e+09   \n",
       "18  2000-09-30  6.803000e+09    5.427000e+09  2.696000e+09   \n",
       "19  2000-12-30  5.986000e+09    4.926000e+09  2.274000e+09   \n",
       "20  2001-03-31  6.130000e+09    5.345000e+09  2.392000e+09   \n",
       "21  2001-06-30  6.071000e+09    5.248000e+09  2.213000e+09   \n",
       "22  2001-09-29  6.021000e+09    5.143000e+09  2.101000e+09   \n",
       "23  2001-12-29  6.122000e+09    5.272000e+09  2.156000e+09   \n",
       "24  2002-03-30  6.264000e+09    5.414000e+09  2.250000e+09   \n",
       "25  2002-06-29  6.293000e+09    5.443000e+09  2.227000e+09   \n",
       "26  2002-09-28  6.298000e+09    5.388000e+09  2.203000e+09   \n",
       "27  2002-12-28  6.269000e+09    5.384000e+09  2.153000e+09   \n",
       "28  2003-03-29  6.361000e+09    5.468000e+09  2.222000e+09   \n",
       "29  2003-06-28  6.441000e+09    5.546000e+09  2.243000e+09   \n",
       "30  2003-09-27  6.815000e+09    5.887000e+09  2.592000e+09   \n",
       "31  2003-12-27  6.971000e+09    6.029000e+09  2.641000e+09   \n",
       "32  2004-03-27  6.735000e+09    5.784000e+09  2.245000e+09   \n",
       "33  2004-06-26  7.223000e+09    6.286000e+09  2.411000e+09   \n",
       "34  2004-09-25  8.050000e+09    7.055000e+09  2.974000e+09   \n",
       "35  2004-12-25  9.362000e+09    8.322000e+09  3.572000e+09   \n",
       "36  2005-03-26  1.011100e+10    9.007000e+09  3.725000e+09   \n",
       "37  2005-06-25  1.048800e+10    9.376000e+09  3.667000e+09   \n",
       "38  2005-09-24  1.155100e+10    1.030000e+10  4.085000e+09   \n",
       "39  2005-12-31  1.418100e+10    1.216200e+10  5.801000e+09   \n",
       "40  2006-04-01  1.391100e+10    1.128600e+10  5.229000e+09   \n",
       "41  2006-07-01  1.511400e+10    1.249100e+10  5.784000e+09   \n",
       "42  2006-09-30  1.720500e+10    1.450900e+10  7.221000e+09   \n",
       "43  2006-12-30  1.946100e+10    1.666400e+10  8.233000e+09   \n",
       "44  2007-03-31  1.871100e+10    1.602900e+10  6.450000e+09   \n",
       "45  2007-06-30  2.164700e+10    1.874500e+10  8.243000e+09   \n",
       "46  2007-09-29  2.534700e+10    2.195600e+10  1.081500e+10   \n",
       "47  2007-12-29  3.003900e+10    2.618900e+10  1.323500e+10   \n",
       "48  2008-03-29  3.047100e+10    2.673600e+10  1.241800e+10   \n",
       "49  2008-06-28  3.170900e+10    2.799800e+10  1.208700e+10   \n",
       "50  2008-09-27  3.957200e+10    3.469000e+10  1.854200e+10   \n",
       "51  2008-12-27  4.278700e+10    3.516300e+10  1.987800e+10   \n",
       "52  2009-03-28  4.323700e+10    3.385300e+10  1.892600e+10   \n",
       "53  2009-06-27  4.814000e+10    3.517000e+10  2.225200e+10   \n",
       "54  2009-09-26  4.750100e+10    3.155500e+10  1.586100e+10   \n",
       "55  2009-12-26  5.392600e+10    3.333200e+10  1.815800e+10   \n",
       "56  2010-03-27  5.705700e+10    3.233600e+10  1.770900e+10   \n",
       "57  2010-06-26  6.472500e+10    3.603300e+10  2.161400e+10   \n",
       "58  2010-09-25  7.518300e+10    4.167800e+10  2.739200e+10   \n",
       "59  2010-12-25  8.674200e+10    4.392700e+10  3.207600e+10   \n",
       "60  2011-03-26  9.490400e+10    4.699700e+10  3.342700e+10   \n",
       "61  2011-06-25  1.067580e+11    4.689800e+10  3.741500e+10   \n",
       "62  2011-09-24  1.163710e+11    4.498800e+10  3.975600e+10   \n",
       "63  2011-12-31  1.386810e+11    5.477100e+10  4.862700e+10   \n",
       "64  2012-03-31  1.509340e+11    5.071200e+10  4.843600e+10   \n",
       "65  2012-06-30  1.628960e+11    5.194300e+10  5.115000e+10   \n",
       "66  2012-09-29  1.760640e+11    5.765300e+10  5.785400e+10   \n",
       "67  2012-12-29  1.960880e+11    7.234800e+10  6.874200e+10   \n",
       "68  2013-03-30  1.947430e+11    6.333700e+10  5.925300e+10   \n",
       "69  2013-06-29  1.998560e+11    6.821900e+10  7.650200e+10   \n",
       "70  2013-09-28  2.070000e+11    7.328600e+10  8.345100e+10   \n",
       "71  2013-12-28  2.251840e+11    8.034700e+10  9.550000e+10   \n",
       "72  2014-03-29  2.059890e+11    7.054100e+10  8.581000e+10   \n",
       "73  2014-06-28  2.225200e+11    6.794900e+10  1.015800e+11   \n",
       "74  2014-09-27  2.318390e+11    6.853100e+10  1.202920e+11   \n",
       "75  2014-12-27  2.618940e+11    8.340300e+10  1.385660e+11   \n",
       "76  2015-03-28  2.611940e+11    6.789100e+10  1.321880e+11   \n",
       "77  2015-06-27  2.731510e+11    7.095300e+10  1.474740e+11   \n",
       "78  2015-09-26  2.904790e+11    8.937800e+10  1.711240e+11   \n",
       "79  2015-12-26  2.932840e+11    7.621900e+10  1.650170e+11   \n",
       "80  2016-03-26  3.052770e+11    8.759200e+10  1.748200e+11   \n",
       "81  2016-06-25  3.056020e+11    9.376100e+10  1.790610e+11   \n",
       "82  2016-09-24  3.216860e+11    1.068690e+11  1.934370e+11   \n",
       "83  2016-12-31  3.311410e+11    1.033320e+11  1.987510e+11   \n",
       "84  2017-04-01  3.345320e+11    1.019900e+11  2.004500e+11   \n",
       "85  2017-07-01  3.451730e+11    1.128750e+11  2.127480e+11   \n",
       "86  2017-09-30  3.753190e+11    1.286450e+11  2.412720e+11   \n",
       "87  2017-12-30  4.067940e+11    1.438100e+11  2.665950e+11   \n",
       "\n",
       "    Current Liabilities  Shareholders equity       Revenue      Earnings  \\\n",
       "0          2.273000e+09         2.056000e+09  2.185000e+09 -7.400000e+08   \n",
       "1          1.926000e+09         2.020000e+09  2.179000e+09 -3.200000e+07   \n",
       "2          2.003000e+09         2.058000e+09  2.321000e+09  2.500000e+07   \n",
       "3          2.044000e+09         1.942000e+09  2.129000e+09 -1.200000e+08   \n",
       "4          2.011000e+09         1.242000e+09  1.601000e+09 -7.080000e+08   \n",
       "5          1.910000e+09         1.196000e+09  1.737000e+09 -5.600000e+07   \n",
       "6          1.818000e+09         1.200000e+09  1.614000e+09 -1.610000e+08   \n",
       "7          1.669000e+09         1.244000e+09  1.578000e+09  4.700000e+07   \n",
       "8          1.384000e+09         1.388000e+09  1.405000e+09  5.500000e+07   \n",
       "9          1.389000e+09         1.486000e+09  1.402000e+09  1.010000e+08   \n",
       "10         1.520000e+09         1.642000e+09  1.556000e+09  1.060000e+08   \n",
       "11         1.484000e+09         1.923000e+09  1.710000e+09  1.520000e+08   \n",
       "12         1.544000e+09         2.175000e+09  1.530000e+09  1.350000e+08   \n",
       "13         1.539000e+09         2.970000e+09  1.558000e+09  2.030000e+08   \n",
       "14         1.549000e+09         3.104000e+09  1.336000e+09  1.110000e+08   \n",
       "15         1.965000e+09         4.414000e+09  2.343000e+09  1.830000e+08   \n",
       "16         1.853000e+09         4.215000e+09  1.945000e+09  2.330000e+08   \n",
       "17         1.873000e+09         4.176000e+09  1.825000e+09  2.000000e+08   \n",
       "18         1.933000e+09         4.107000e+09  1.870000e+09  1.700000e+08   \n",
       "19         1.637000e+09         3.712000e+09  1.007000e+09 -1.950000e+08   \n",
       "20         1.795000e+09         3.738000e+09  1.431000e+09  4.300000e+07   \n",
       "21         1.614000e+09         3.858000e+09  1.475000e+09  6.100000e+07   \n",
       "22         1.518000e+09         3.920000e+09  1.450000e+09  6.600000e+07   \n",
       "23         1.579000e+09         3.966000e+09  1.375000e+09  3.800000e+07   \n",
       "24         1.709000e+09         4.014000e+09  1.495000e+09  4.000000e+07   \n",
       "25         1.718000e+09         4.066000e+09  1.429000e+09  3.200000e+07   \n",
       "26         1.658000e+09         4.095000e+09  1.443000e+09 -4.500000e+07   \n",
       "27         1.595000e+09         4.116000e+09  1.472000e+09 -8.000000e+06   \n",
       "28         2.007000e+09         4.139000e+09  1.475000e+09  1.400000e+07   \n",
       "29         2.025000e+09         4.198000e+09  1.545000e+09  1.900000e+07   \n",
       "30         2.357000e+09         4.223000e+09  1.715000e+09  4.400000e+07   \n",
       "31         2.382000e+09         4.330000e+09  2.006000e+09  6.300000e+07   \n",
       "32         1.980000e+09         4.490000e+09  1.909000e+09  4.600000e+07   \n",
       "33         2.155000e+09         4.812000e+09  2.014000e+09  6.100000e+07   \n",
       "34         2.680000e+09         5.076000e+09  2.350000e+09  1.060000e+08   \n",
       "35         3.224000e+09         5.790000e+09  3.490000e+09  2.950000e+08   \n",
       "36         3.352000e+09         6.386000e+09  3.243000e+09  2.900000e+08   \n",
       "37         3.123000e+09         6.821000e+09  3.520000e+09  3.200000e+08   \n",
       "38         3.484000e+09         7.466000e+09  3.678000e+09  4.300000e+08   \n",
       "39         5.060000e+09         8.380000e+09  5.749000e+09  5.650000e+08   \n",
       "40         4.456000e+09         8.682000e+09  4.359000e+09  4.100000e+08   \n",
       "41         5.023000e+09         9.330000e+09  4.370000e+09  4.720000e+08   \n",
       "42         6.471000e+09         9.984000e+09  4.837000e+09  5.420000e+08   \n",
       "43         7.337000e+09         1.122800e+10  7.115000e+09  1.004000e+09   \n",
       "44         5.485000e+09         1.226100e+10  5.264000e+09  7.700000e+08   \n",
       "45         6.992000e+09         1.340400e+10  5.410000e+09  8.180000e+08   \n",
       "46         9.299000e+09         1.453200e+10  6.217000e+09  9.040000e+08   \n",
       "47         1.053500e+10         1.680400e+10  9.608000e+09  1.581000e+09   \n",
       "48         9.634000e+09         1.805300e+10  7.512000e+09  1.045000e+09   \n",
       "49         9.218000e+09         1.962200e+10  7.464000e+09  1.072000e+09   \n",
       "50         1.409200e+10         2.103000e+10  7.895000e+09  1.136000e+09   \n",
       "51         1.475700e+10         2.290900e+10  1.016700e+10  1.605000e+09   \n",
       "52         1.375100e+10         2.431100e+10  8.163000e+09  1.205000e+09   \n",
       "53         1.666100e+10         2.588800e+10  8.337000e+09  1.229000e+09   \n",
       "54         1.150600e+10         3.164000e+10  1.623800e+10  4.196000e+09   \n",
       "55         1.309700e+10         3.576800e+10  1.568300e+10  3.378000e+09   \n",
       "56         1.222900e+10         3.934800e+10  1.349900e+10  3.074000e+09   \n",
       "57         1.561200e+10         4.311100e+10  1.570000e+10  3.253000e+09   \n",
       "58         2.072200e+10         4.779100e+10  2.034300e+10  4.308000e+09   \n",
       "59         2.379500e+10         5.466600e+10  2.674100e+10  6.004000e+09   \n",
       "60         2.432700e+10         6.147700e+10  2.466700e+10  5.987000e+09   \n",
       "61         2.685900e+10         6.934300e+10  2.857100e+10  7.308000e+09   \n",
       "62         2.797000e+10         7.661500e+10  2.827000e+10  6.623000e+09   \n",
       "63         3.460700e+10         9.005400e+10  4.633300e+10  1.306400e+10   \n",
       "64         3.203600e+10         1.024980e+11  3.918600e+10  1.162200e+10   \n",
       "65         3.306000e+10         1.117460e+11  3.502300e+10  8.824000e+09   \n",
       "66         3.854200e+10         1.182100e+11  3.596600e+10  8.223000e+09   \n",
       "67         4.687900e+10         1.273460e+11  5.451200e+10  1.307800e+10   \n",
       "68         3.550800e+10         1.354900e+11  4.360300e+10  9.547000e+09   \n",
       "69         3.631900e+10         1.233540e+11  3.532300e+10  6.900000e+09   \n",
       "70         4.365800e+10         1.235490e+11  3.747200e+10  7.512000e+09   \n",
       "71         5.376900e+10         1.296840e+11  5.759400e+10  1.307200e+10   \n",
       "72         4.320800e+10         1.201790e+11  4.564600e+10  1.022300e+10   \n",
       "73         4.620500e+10         1.209400e+11  3.743200e+10  7.748000e+09   \n",
       "74         6.344800e+10         1.115470e+11  4.212300e+10  8.467000e+09   \n",
       "75         7.361100e+10         1.233280e+11  7.459900e+10  1.802400e+10   \n",
       "76         5.872900e+10         1.290060e+11  5.801000e+10  1.356900e+10   \n",
       "77         6.528500e+10         1.256770e+11  4.960500e+10  1.067700e+10   \n",
       "78         8.061000e+10         1.193550e+11  5.150100e+10  1.112400e+10   \n",
       "79         7.609200e+10         1.282670e+11  7.587200e+10  1.836100e+10   \n",
       "80         6.826500e+10         1.304570e+11  5.055700e+10  1.051600e+10   \n",
       "81         7.148600e+10         1.265410e+11  4.235800e+10  7.796000e+09   \n",
       "82         7.900600e+10         1.282490e+11  4.685200e+10  9.014000e+09   \n",
       "83         8.413000e+10         1.323900e+11  7.835100e+10  1.789100e+10   \n",
       "84         7.334200e+10         1.340820e+11  5.289600e+10  1.102900e+10   \n",
       "85         8.130200e+10         1.324250e+11  4.540800e+10  8.717000e+09   \n",
       "86         1.008140e+11         1.340470e+11  5.257900e+10  1.071400e+10   \n",
       "87         1.157880e+11         1.401990e+11  8.829300e+10  2.006500e+10   \n",
       "\n",
       "    EPS basic  EPS diluted   Price  Price high  Price low     ROE  P/B ratio  \\\n",
       "0       -5.99     1.945000    1.04        1.27       0.82 -0.2443       1.28   \n",
       "1       -0.26     0.005156    0.86        1.03       0.70 -0.3197       1.46   \n",
       "2        0.21     0.010313    0.73        0.89       0.57 -0.3656       1.26   \n",
       "3       -0.96     0.020625    0.88        0.99       0.76 -0.4294       1.49   \n",
       "4       -5.64     0.041250    0.69        0.83       0.54 -0.4599       1.23   \n",
       "5       -0.44     0.082500    0.61        0.71       0.52 -0.5337       1.74   \n",
       "6       -1.26     0.165000    0.76        1.06       0.46 -0.7491       2.24   \n",
       "7        0.37     0.330000    0.67        0.88       0.46 -0.7194       2.03   \n",
       "8        0.42     0.380000    0.73        1.00       0.46 -0.0915       2.21   \n",
       "9        0.76     0.650000    1.00        1.13       0.88  0.0316       2.70   \n",
       "10       0.78     0.650000    1.28        1.56       1.00  0.2146       3.28   \n",
       "11       1.12     0.950000    1.25        1.48       1.02  0.2572       2.91   \n",
       "12       0.99     0.840000    1.42        1.69       1.14  0.2735       2.84   \n",
       "13       1.41     1.200000    1.49        1.79       1.19  0.2737       2.61   \n",
       "14       0.71     0.620000    2.19        2.86       1.51  0.2363       3.32   \n",
       "15       1.14     1.030000    3.13        4.21       2.05  0.1996       4.54   \n",
       "16       1.44     1.280000    4.23        5.37       3.09  0.1986       4.32   \n",
       "17       0.62     0.550000    3.93        4.98       2.87  0.1828       4.27   \n",
       "18       0.51     0.470000    3.20        4.58       1.81  0.1859       3.48   \n",
       "19      -0.58    -0.580000    1.44        1.91       0.97  0.1007       1.66   \n",
       "20       0.12     0.120000    1.36        1.70       1.03  0.0554       1.77   \n",
       "21       0.17     0.170000    1.64        1.94       1.34  0.0205       2.16   \n",
       "22       0.20     0.200000    1.43        1.80       1.05 -0.0066       1.81   \n",
       "23       0.11     0.110000    1.39        1.70       1.07  0.0537       1.74   \n",
       "24       0.11     0.110000    1.65        1.82       1.48  0.0520       2.06   \n",
       "25       0.09     0.090000    1.52        1.86       1.18  0.0441       1.88   \n",
       "26      -0.13    -0.130000    1.17        1.34       1.00  0.0161       1.44   \n",
       "27      -0.02    -0.020000    1.10        1.23       0.97  0.0047       1.36   \n",
       "28       0.04     0.040000    1.04        1.09       0.99 -0.0017       1.27   \n",
       "29       0.05     0.050000    1.16        1.38       0.94 -0.0048       1.43   \n",
       "30       0.12     0.120000    1.50        1.65       1.36  0.0166       1.81   \n",
       "31       0.17     0.170000    1.59        1.77       1.41  0.0332       1.94   \n",
       "32       0.13     0.120000    1.75        1.98       1.51  0.0399       2.08   \n",
       "33       0.16     0.160000    2.13        2.41       1.84  0.0479       2.54   \n",
       "34       0.28     0.250000    2.40        2.71       2.08  0.0590       2.70   \n",
       "35       0.75     0.700000    3.77        4.89       2.66  0.1008       4.19   \n",
       "36       0.36     0.340000    5.47        6.44       4.51  0.1363       5.42   \n",
       "37       0.39     0.370000    5.56        6.25       4.88  0.1680       5.01   \n",
       "38       0.52     0.500000    6.40        7.60       5.20  0.2018       5.47   \n",
       "39       0.68     0.650000    8.81       10.78       6.84  0.2210       6.94   \n",
       "40       0.49     0.470000   10.29       12.34       8.24  0.2201       7.30   \n",
       "41       0.55     0.540000    9.23       10.54       7.92  0.2217       6.32   \n",
       "42       0.64     0.610000    9.14       11.11       7.17  0.2187       5.90   \n",
       "43       1.17     1.140000   11.84       13.31      10.37  0.2476       7.13   \n",
       "44       0.89     0.870000   12.84       13.97      11.70  0.2605       6.90   \n",
       "45       0.94     0.920000   15.52       18.23      12.80  0.2674       7.65   \n",
       "46       1.04     1.000000   19.05       22.14      15.95  0.2719       8.66   \n",
       "47       1.81     1.760000   25.25       28.99      21.52  0.2858      10.65   \n",
       "48       1.19     1.160000   22.56       28.64      16.49  0.2770       8.26   \n",
       "49       1.21     1.190000   23.91       27.46      20.36  0.2667       8.16   \n",
       "50       1.27     1.250000   21.54       25.84      17.24  0.2561       6.82   \n",
       "51       1.81     1.780000   14.21       17.10      11.31  0.2381       4.20   \n",
       "52       1.35     1.330000   13.44       15.71      11.17  0.2284       3.66   \n",
       "53       1.38     1.350000   17.79       20.91      14.66  0.2199       4.57   \n",
       "54       4.66     4.600000   23.09       26.99      19.20  0.3145       5.59   \n",
       "55       3.74     3.670000   27.86       29.91      25.81  0.3404       5.55   \n",
       "56       3.39     3.330000   30.16       33.14      27.18  0.3582       5.36   \n",
       "57       3.57     3.510000   34.16       39.86      28.46  0.3710       5.53   \n",
       "58       4.70     4.640000   37.79       41.93      33.65  0.3376       5.61   \n",
       "59       6.53     6.430000   42.91       46.53      39.29  0.3599       5.77   \n",
       "60       6.49     6.400000   49.02       52.13      45.90  0.3777       5.78   \n",
       "61       7.89     7.790000   47.55       50.73      44.36  0.4048       5.01   \n",
       "62       7.13     7.060000   53.58       60.41      46.75  0.3956       5.01   \n",
       "63      14.03    13.870000   55.78       60.96      50.61  0.4435       4.74   \n",
       "64      12.45    12.300000   73.61       88.78      58.43  0.4563       5.33   \n",
       "65       9.42     9.320000   83.30       92.00      74.60  0.4214       5.32   \n",
       "66       8.76     8.680000   91.08      100.72      81.43  0.3951       5.35   \n",
       "67      13.93    13.810000   84.14       96.68      71.60  0.3632       4.69   \n",
       "68      10.16    10.090000   69.58       79.29      59.86  0.3220       3.59   \n",
       "69       7.51     7.470000   60.78       66.54      55.01  0.2993       2.95   \n",
       "70       8.31     8.260000   65.36       73.39      57.32  0.2906       3.37   \n",
       "71      14.59    14.500000   74.97       82.16      67.77  0.2893       3.82   \n",
       "72      11.69    11.620000   75.34       80.18      70.51  0.3036       3.63   \n",
       "73       1.29     1.280000   84.05       95.05      73.05  0.3120       4.22   \n",
       "74       1.42     1.420000   97.91      103.74      92.09  0.3276       4.85   \n",
       "75       3.08     3.060000  107.47      119.75      95.18  0.3736       5.65   \n",
       "76       2.34     2.330000  119.11      133.60     104.63  0.3944       5.63   \n",
       "77       1.86     1.850000  128.82      134.54     123.10  0.4146       5.75   \n",
       "78       1.98     1.960000  112.48      132.97      92.00  0.4294       5.10   \n",
       "79       3.30     3.280000  114.69      123.82     105.57  0.4279       5.36   \n",
       "80       1.91     1.900000  100.91      109.43      92.39  0.4024       4.36   \n",
       "81       1.43     1.420000  100.93      112.39      89.47  0.3789       4.24   \n",
       "82       1.68     1.680000  103.84      116.18      91.50  0.3559       4.42   \n",
       "83       3.38     3.360000  111.38      118.69     104.08  0.3494       4.63   \n",
       "84       2.11     2.100000  129.63      144.50     114.76  0.3509       5.14   \n",
       "85       1.68     1.670000  148.36      156.65     140.06  0.3540       5.77   \n",
       "86       2.08     2.060000  153.68      164.94     142.41  0.3629       5.99   \n",
       "87       3.92     3.890000  164.83      177.20     152.46  0.3737       6.31   \n",
       "\n",
       "    P/E ratio  Cumulative dividends per share  Dividend payout ratio  \\\n",
       "0      21.570                            0.03           1.252500e-01   \n",
       "1      11.485                            0.03           3.355606e-21   \n",
       "2       1.400                            0.03           6.711211e-21   \n",
       "3       2.800                            0.03           1.342242e-20   \n",
       "4       5.600                            0.03           2.684485e-20   \n",
       "5      11.200                            0.03           5.368969e-20   \n",
       "6      22.400                            0.03           1.073794e-19   \n",
       "7      44.800                            0.03           2.147588e-19   \n",
       "8      89.600                            0.03           4.295175e-19   \n",
       "9     179.200                            0.03           8.590351e-19   \n",
       "10    358.400                            0.03           1.718070e-18   \n",
       "11     17.410                            0.03           3.436140e-18   \n",
       "12     15.120                            0.03           6.872281e-18   \n",
       "13     13.500                            0.03           1.374456e-17   \n",
       "14     16.850                            0.03           2.748912e-17   \n",
       "15     24.280                            0.03           5.497824e-17   \n",
       "16     32.100                            0.03           1.099565e-16   \n",
       "17     26.640                            0.03           2.199130e-16   \n",
       "18     22.230                            0.03           4.398260e-16   \n",
       "19      9.270                            0.03           8.796519e-16   \n",
       "20     17.630                            0.03           1.759304e-15   \n",
       "21     41.000                            0.03           3.518608e-15   \n",
       "22    111.220                            0.03           7.037215e-15   \n",
       "23     74.860                            0.03           1.407443e-14   \n",
       "24     38.500                            0.03           2.814886e-14   \n",
       "25     36.070                            0.03           5.629772e-14   \n",
       "26     32.120                            0.03           1.125954e-13   \n",
       "27     85.560                            0.03           2.251909e-13   \n",
       "28    291.200                            0.03           4.503818e-13   \n",
       "29    174.890                            0.03           9.007636e-13   \n",
       "30     58.580                            0.03           1.801527e-12   \n",
       "31    117.160                            0.03           3.603054e-12   \n",
       "32     64.470                            0.03           7.206108e-12   \n",
       "33     64.830                            0.03           1.441222e-11   \n",
       "34     58.950                            0.03           2.882443e-11   \n",
       "35     75.400                            0.03           5.764887e-11   \n",
       "36     62.260                            0.03           1.152977e-10   \n",
       "37     43.490                            0.03           2.305955e-10   \n",
       "38     37.810                            0.03           4.611909e-10   \n",
       "39     39.530                            0.03           9.223819e-10   \n",
       "40     38.730                            0.03           1.844764e-09   \n",
       "41     32.470                            0.03           3.689528e-09   \n",
       "42     29.620                            0.03           7.379055e-09   \n",
       "43     36.510                            0.03           1.475811e-08   \n",
       "44     32.570                            0.03           2.951622e-08   \n",
       "45     34.380                            0.03           5.903244e-08   \n",
       "46     37.670                            0.03           1.180649e-07   \n",
       "47     44.970                            0.03           2.361298e-07   \n",
       "48     34.710                            0.03           4.722595e-07   \n",
       "49     34.580                            0.03           9.445190e-07   \n",
       "50     29.510                            0.03           1.889038e-06   \n",
       "51     18.560                            0.03           3.778076e-06   \n",
       "52     17.490                            0.03           7.556152e-06   \n",
       "53     22.440                            0.03           1.511230e-05   \n",
       "54     28.310                            0.03           3.022461e-05   \n",
       "55     21.530                            0.03           6.044922e-05   \n",
       "56     19.280                            0.03           1.208984e-04   \n",
       "57     18.460                            0.03           2.417969e-04   \n",
       "58     17.510                            0.03           4.835937e-04   \n",
       "59     19.830                            0.03           9.671875e-04   \n",
       "60     19.160                            0.03           1.934375e-03   \n",
       "61     15.870                            0.03           3.868750e-03   \n",
       "62     14.850                            0.03           7.737500e-03   \n",
       "63     14.110                            0.03           1.547500e-02   \n",
       "64     14.670                            0.03           3.095000e-02   \n",
       "65     14.220                            0.41           6.190000e-02   \n",
       "66     14.980                            0.41           5.950000e-02   \n",
       "67     13.330                            0.79           1.191000e-01   \n",
       "68     11.040                            1.17           1.880000e-01   \n",
       "69     10.150                            1.60           2.052000e-01   \n",
       "70     11.420                            2.04           2.833000e-01   \n",
       "71     13.240                            2.47           2.896000e-01   \n",
       "72     13.080                            2.91           2.881000e-01   \n",
       "73     14.060                            3.38           2.829000e-01   \n",
       "74     15.810                            3.86           2.778000e-01   \n",
       "75     16.710                            4.33           2.473000e-01   \n",
       "76     16.050                            4.80           2.317000e-01   \n",
       "77     15.920                            5.32           2.213000e-01   \n",
       "78     12.990                            5.84           2.118000e-01   \n",
       "79     12.470                            6.36           2.132000e-01   \n",
       "80     10.710                            6.88           2.288000e-01   \n",
       "81     11.230                            7.45           2.448000e-01   \n",
       "82     12.130                            8.02           2.592000e-01   \n",
       "83     13.450                            8.59           2.643000e-01   \n",
       "84     15.510                            9.16           2.640000e-01   \n",
       "85     17.330                            9.79           2.627000e-01   \n",
       "86     17.440                           10.42           2.575000e-01   \n",
       "87     17.940                           11.05           2.505000e-01   \n",
       "\n",
       "    Long-term debt to equity ratio  Net margin  Asset turnover  \\\n",
       "0                     1.474000e-01    0.105600            1.81   \n",
       "1                     4.698000e-01    0.000027            1.80   \n",
       "2                     4.611000e-01    0.000055            1.75   \n",
       "3                     4.892000e-01    0.000109            1.66   \n",
       "4                     7.665000e-01    0.000219            1.61   \n",
       "5                     7.952000e-01    0.000438            1.60   \n",
       "6                     7.925000e-01    0.000875            1.54   \n",
       "7                     7.653000e-01    0.001750            1.52   \n",
       "8                     6.866000e-01    0.003500            1.52   \n",
       "9                     6.413000e-01    0.007000            1.47   \n",
       "10                    5.810000e-01    0.052000            1.45   \n",
       "11                    4.961000e-01    0.068200            1.44   \n",
       "12                    4.391000e-01    0.079700            1.39   \n",
       "13                    1.010000e-01    0.093800            1.35   \n",
       "14                    9.660000e-02    0.098000            1.25   \n",
       "15                    6.800000e-02    0.093400            1.19   \n",
       "16                    7.120000e-02    0.101600            1.16   \n",
       "17                    7.180000e-02    0.097600            1.12   \n",
       "18                    7.300000e-02    0.098500            1.13   \n",
       "19                    8.380000e-02    0.061400            0.99   \n",
       "20                    8.480000e-02    0.035500            0.95   \n",
       "21                    8.220000e-02    0.013700            0.93   \n",
       "22                    8.090000e-02    0.025000            0.89   \n",
       "23                    7.940000e-02    0.036300            0.94   \n",
       "24                    7.750000e-02    0.035400            0.95   \n",
       "25                    7.770000e-02    0.030600            0.93   \n",
       "26                    7.720000e-02    0.011300            0.92   \n",
       "27                    7.770000e-02    0.003300            0.93   \n",
       "28                    3.885000e-02    0.004425            0.92   \n",
       "29                    1.250555e-13    0.005550            0.94   \n",
       "30                    2.501110e-13    0.011100            0.96   \n",
       "31                    5.002221e-13    0.020800            1.01   \n",
       "32                    1.000444e-12    0.024000            1.06   \n",
       "33                    2.000888e-12    0.028000            1.10   \n",
       "34                    4.001777e-12    0.033300            1.14   \n",
       "35                    8.003553e-12    0.052000            1.24   \n",
       "36                    1.600711e-11    0.067800            1.28   \n",
       "37                    3.201421e-11    0.080200            1.33   \n",
       "38                    6.402843e-11    0.095800            1.34   \n",
       "39                    1.280569e-10    0.099100            1.40   \n",
       "40                    2.561137e-10    0.099700            1.38   \n",
       "41                    5.122274e-10    0.103400            1.33   \n",
       "42                    1.024455e-09    0.103000            1.28   \n",
       "43                    2.048910e-09    0.117400            1.26   \n",
       "44                    4.097819e-09    0.129200            1.22   \n",
       "45                    8.195639e-09    0.138500            1.18   \n",
       "46                    1.639128e-08    0.145600            1.13   \n",
       "47                    3.278255e-08    0.153700            1.11   \n",
       "48                    6.556511e-08    0.151300            1.07   \n",
       "49                    1.311302e-07    0.149400            1.05   \n",
       "50                    2.622604e-07    0.148800            0.99   \n",
       "51                    5.245209e-07    0.147000            0.91   \n",
       "52                    1.049042e-06    0.149000            0.86   \n",
       "53                    2.098083e-06    0.149700            0.80   \n",
       "54                    4.196167e-06    0.191900            0.94   \n",
       "55                    8.392334e-06    0.206700            1.00   \n",
       "56                    1.678467e-05    0.220900            1.04   \n",
       "57                    3.356934e-05    0.227400            1.10   \n",
       "58                    6.713867e-05    0.214800            1.04   \n",
       "59                    1.342773e-04    0.218100            1.08   \n",
       "60                    2.685547e-04    0.223600            1.09   \n",
       "61                    5.371094e-04    0.235300            1.10   \n",
       "62                    1.074219e-03    0.239500            1.07   \n",
       "63                    2.148438e-03    0.258000            1.12   \n",
       "64                    4.296875e-03    0.271300            1.11   \n",
       "65                    8.593750e-03    0.269700            1.05   \n",
       "66                    1.718750e-02    0.266700            1.00   \n",
       "67                    3.437500e-02    0.253500            0.96   \n",
       "68                    6.875000e-02    0.234600            0.93   \n",
       "69                    1.375000e-01    0.222800            0.88   \n",
       "70                    1.373000e-01    0.216700            0.86   \n",
       "71                    1.308000e-01    0.212800            0.84   \n",
       "72                    1.411000e-01    0.214200            0.84   \n",
       "73                    2.400000e-01    0.216400            0.83   \n",
       "74                    2.599000e-01    0.216100            0.83   \n",
       "75                    2.636000e-01    0.222500            0.87   \n",
       "76                    3.106000e-01    0.225300            0.87   \n",
       "77                    3.773000e-01    0.226200            0.87   \n",
       "78                    4.479000e-01    0.228500            0.86   \n",
       "79                    4.148000e-01    0.228700            0.84   \n",
       "80                    5.318000e-01    0.222700            0.78   \n",
       "81                    5.448000e-01    0.217000            0.74   \n",
       "82                    5.881000e-01    0.211900            0.70   \n",
       "83                    5.556000e-01    0.207300            0.69   \n",
       "84                    6.304000e-01    0.207400            0.68   \n",
       "85                    6.786000e-01    0.208700            0.67   \n",
       "86                    7.252000e-01    0.210900            0.66   \n",
       "87                    7.412000e-01    0.211200            0.65   \n",
       "\n",
       "    Free cash flow per share ticker  Relative Return DJIA  \n",
       "0                     -0.110   AAPL             -1.828901  \n",
       "1                      0.080   AAPL             -1.381029  \n",
       "2                      0.110   AAPL             -4.175122  \n",
       "3                      0.020   AAPL             -9.418520  \n",
       "4                      0.020   AAPL             -2.312751  \n",
       "5                     -0.060   AAPL            -16.662054  \n",
       "6                      0.050   AAPL             -3.305219  \n",
       "7                      0.040   AAPL              0.347391  \n",
       "8                      0.040   AAPL            -11.184244  \n",
       "9                      0.040   AAPL             -1.359833  \n",
       "10                     0.080   AAPL             12.672734  \n",
       "11                     0.060   AAPL            -17.094391  \n",
       "12                     0.070   AAPL             -6.450447  \n",
       "13                     0.020   AAPL            -12.055963  \n",
       "14                     0.040   AAPL              6.247408  \n",
       "15                     0.070   AAPL            -10.794300  \n",
       "16                     0.010   AAPL              5.354430  \n",
       "17                     0.050   AAPL              4.269249  \n",
       "18                     0.030   AAPL             -2.129014  \n",
       "19                    -0.010   AAPL             -1.826228  \n",
       "20                    -0.010   AAPL              8.362750  \n",
       "21                    -0.010   AAPL             -6.106840  \n",
       "22                     0.020   AAPL             15.628731  \n",
       "23                     0.005   AAPL            -13.296489  \n",
       "24                    -0.010   AAPL             -3.629145  \n",
       "25                    -0.010   AAPL             11.077370  \n",
       "26                     0.005   AAPL             17.634971  \n",
       "27                     0.020   AAPL             -9.934788  \n",
       "28                     0.010   AAPL              4.135283  \n",
       "29                    -0.010   AAPL            -12.313217  \n",
       "30                     0.010   AAPL             -2.930111  \n",
       "31                     0.030   AAPL            -12.649999  \n",
       "32                     0.030   AAPL              1.021049  \n",
       "33                     0.030   AAPL             -0.533796  \n",
       "34                     0.070   AAPL              3.530629  \n",
       "35                     0.130   AAPL             -6.400607  \n",
       "36                     0.090   AAPL              3.040651  \n",
       "37                     0.070   AAPL              2.194626  \n",
       "38                     0.110   AAPL             -2.707615  \n",
       "39                     0.030   AAPL             -1.031368  \n",
       "40                    -0.050   AAPL             -3.487899  \n",
       "41                     0.130   AAPL             -0.471172  \n",
       "42                     0.150   AAPL             -4.752707  \n",
       "43                     0.280   AAPL             -6.418143  \n",
       "44                     0.100   AAPL              0.957433  \n",
       "45                     0.160   AAPL             -8.324871  \n",
       "46                     0.240   AAPL             -3.404618  \n",
       "47                     0.420   AAPL              4.865088  \n",
       "48                     0.170   AAPL              7.446753  \n",
       "49                     0.160   AAPL              7.504089  \n",
       "50                     0.630   AAPL              4.300434  \n",
       "51                     0.580   AAPL             18.776236  \n",
       "52                     0.120   AAPL             13.248207  \n",
       "53                     0.310   AAPL            -10.690780  \n",
       "54                     0.420   AAPL            -14.681126  \n",
       "55                     0.850   AAPL             -7.163159  \n",
       "56                     0.320   AAPL             -4.027321  \n",
       "57                     0.650   AAPL             10.104505  \n",
       "58                     0.760   AAPL            -10.268484  \n",
       "59                     1.320   AAPL             -7.182426  \n",
       "60                     0.860   AAPL             -6.268487  \n",
       "61                     1.560   AAPL             -0.797943  \n",
       "62                     0.900   AAPL             12.217348  \n",
       "63                     2.470   AAPL            -11.909225  \n",
       "64                     1.900   AAPL             -7.820111  \n",
       "65                     1.100   AAPL              2.644121  \n",
       "66                     0.860   AAPL             -4.231417  \n",
       "67                     3.190   AAPL              2.401937  \n",
       "68                     1.550   AAPL            -11.424451  \n",
       "69                     0.910   AAPL             -2.397345  \n",
       "70                     1.210   AAPL             -1.400675  \n",
       "71                     3.300   AAPL             -9.416891  \n",
       "72                     2.000   AAPL              0.722812  \n",
       "73                     1.310   AAPL             -2.126143  \n",
       "74                     1.600   AAPL             -1.120563  \n",
       "75                     5.230   AAPL             -4.480043  \n",
       "76                     2.880   AAPL              0.371732  \n",
       "77                     2.260   AAPL              0.962535  \n",
       "78                     1.760   AAPL              7.448906  \n",
       "79                     4.230   AAPL             -6.982814  \n",
       "80                     1.640   AAPL             -1.612601  \n",
       "81                     1.430   AAPL             -1.384584  \n",
       "82                     2.270   AAPL             -2.080260  \n",
       "83                     4.510   AAPL             -7.871664  \n",
       "84                     1.820   AAPL             -4.393340  \n",
       "85                     1.160   AAPL              0.111269  \n",
       "86                     2.270   AAPL             -0.013578  \n",
       "87                     4.990   AAPL            -10.256037  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_data_filled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for trend standardization\n",
    "def trend_standardization(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    1. conver each series to numpy array\n",
    "    2. get values of standardization, assign to new numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    df_standardize = deepcopy(df)\n",
    "    cols_standardize = list(df_standardize.drop(columns=['Quarter end', 'ticker']).columns)\n",
    "    for col in cols_standardize:\n",
    "        np_col = df_standardize[col].to_numpy()\n",
    "        np_delta = np.zeros(len(np_col))\n",
    "        for n, val in enumerate(np_col):\n",
    "            if n == 0:\n",
    "                np_delta[0] = np_col[0]\n",
    "            else:\n",
    "                delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
    "                np_delta[n] = delta\n",
    "                \n",
    "            # mean substitution for 1996 Q1\n",
    "            np_delta[0] = np_delta.mean()\n",
    "        df_standardize[col] = np_delta\n",
    "    \n",
    "    return df_standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n",
      "<ipython-input-187-e04f05a9b6a9>:21: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  delta = (np_col[n] - np_col[n-1]) / (np_col[n-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "financial_data_trend_standardized = []\n",
    "for n, df in enumerate(financial_data_filled):\n",
    "    print(n)\n",
    "    financial_data_trend_standardized.append(trend_standardization(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter end</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Current Assets</th>\n",
       "      <th>Liabilities</th>\n",
       "      <th>Current Liabilities</th>\n",
       "      <th>Shareholders equity</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>EPS basic</th>\n",
       "      <th>EPS diluted</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price high</th>\n",
       "      <th>Price low</th>\n",
       "      <th>ROE</th>\n",
       "      <th>P/B ratio</th>\n",
       "      <th>P/E ratio</th>\n",
       "      <th>Cumulative dividends per share</th>\n",
       "      <th>Dividend payout ratio</th>\n",
       "      <th>Long-term debt to equity ratio</th>\n",
       "      <th>Net margin</th>\n",
       "      <th>Asset turnover</th>\n",
       "      <th>Free cash flow per share</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Relative Return DJIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-03-29</td>\n",
       "      <td>0.054530</td>\n",
       "      <td>0.045478</td>\n",
       "      <td>0.061899</td>\n",
       "      <td>0.058088</td>\n",
       "      <td>0.053942</td>\n",
       "      <td>0.075105</td>\n",
       "      <td>0.022818</td>\n",
       "      <td>-0.001147</td>\n",
       "      <td>0.099883</td>\n",
       "      <td>0.077414</td>\n",
       "      <td>0.078368</td>\n",
       "      <td>0.084413</td>\n",
       "      <td>-0.102530</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.130674</td>\n",
       "      <td>0.189728</td>\n",
       "      <td>0.746084</td>\n",
       "      <td>0.480173</td>\n",
       "      <td>0.202231</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>-0.004580</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>7.213807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-06-28</td>\n",
       "      <td>0.021207</td>\n",
       "      <td>0.041384</td>\n",
       "      <td>0.046256</td>\n",
       "      <td>-0.152662</td>\n",
       "      <td>-0.017510</td>\n",
       "      <td>-0.002746</td>\n",
       "      <td>-0.956757</td>\n",
       "      <td>-0.956594</td>\n",
       "      <td>-0.997349</td>\n",
       "      <td>-0.173077</td>\n",
       "      <td>-0.188976</td>\n",
       "      <td>-0.146341</td>\n",
       "      <td>0.308637</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.467548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.187246</td>\n",
       "      <td>-0.999741</td>\n",
       "      <td>-0.005525</td>\n",
       "      <td>-1.727273</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.244886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-09-27</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.013696</td>\n",
       "      <td>-0.005714</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>0.018812</td>\n",
       "      <td>0.065168</td>\n",
       "      <td>-1.781250</td>\n",
       "      <td>-1.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.151163</td>\n",
       "      <td>-0.135922</td>\n",
       "      <td>-0.185714</td>\n",
       "      <td>0.143572</td>\n",
       "      <td>-0.136986</td>\n",
       "      <td>-0.878102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018519</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027778</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.023195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-12-27</td>\n",
       "      <td>-0.017151</td>\n",
       "      <td>-0.021262</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>-0.056365</td>\n",
       "      <td>-0.082723</td>\n",
       "      <td>-5.800000</td>\n",
       "      <td>-5.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.205479</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.174508</td>\n",
       "      <td>0.182540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.051429</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.255867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-03-28</td>\n",
       "      <td>-0.148900</td>\n",
       "      <td>-0.175832</td>\n",
       "      <td>-0.025526</td>\n",
       "      <td>-0.016145</td>\n",
       "      <td>-0.360453</td>\n",
       "      <td>-0.248004</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.215909</td>\n",
       "      <td>-0.161616</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>0.071029</td>\n",
       "      <td>-0.174497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.754446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997-06-27</td>\n",
       "      <td>-0.032538</td>\n",
       "      <td>-0.040912</td>\n",
       "      <td>-0.030817</td>\n",
       "      <td>-0.050224</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>0.084947</td>\n",
       "      <td>-0.920904</td>\n",
       "      <td>-0.921986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.144578</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>0.160470</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037443</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006211</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.204432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997-09-26</td>\n",
       "      <td>-0.024879</td>\n",
       "      <td>-0.019754</td>\n",
       "      <td>-0.035612</td>\n",
       "      <td>-0.048168</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>-0.070812</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.863636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>-0.115385</td>\n",
       "      <td>0.403598</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.037500</td>\n",
       "      <td>-1.833333</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.801632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1997-12-26</td>\n",
       "      <td>-0.025278</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>-0.049786</td>\n",
       "      <td>-0.081958</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>-0.022305</td>\n",
       "      <td>-1.291925</td>\n",
       "      <td>-1.293651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.118421</td>\n",
       "      <td>-0.169811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.039648</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.105104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1998-03-27</td>\n",
       "      <td>-0.039506</td>\n",
       "      <td>-0.047436</td>\n",
       "      <td>-0.106523</td>\n",
       "      <td>-0.170761</td>\n",
       "      <td>0.115756</td>\n",
       "      <td>-0.109632</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.872811</td>\n",
       "      <td>0.088670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.102835</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-33.194949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1998-06-26</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>-0.007767</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.070605</td>\n",
       "      <td>-0.002135</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>-1.345355</td>\n",
       "      <td>0.221719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.065977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.032895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.878415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1998-09-25</td>\n",
       "      <td>0.061371</td>\n",
       "      <td>0.095704</td>\n",
       "      <td>0.036008</td>\n",
       "      <td>0.094312</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.109843</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.380531</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>5.791139</td>\n",
       "      <td>0.214815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.094028</td>\n",
       "      <td>6.428571</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-10.319328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1998-12-26</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.008311</td>\n",
       "      <td>-0.023684</td>\n",
       "      <td>0.171133</td>\n",
       "      <td>0.098972</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.198509</td>\n",
       "      <td>-0.112805</td>\n",
       "      <td>-0.951423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.146127</td>\n",
       "      <td>0.311538</td>\n",
       "      <td>-0.006897</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.348911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1999-03-27</td>\n",
       "      <td>0.074695</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.034095</td>\n",
       "      <td>0.040431</td>\n",
       "      <td>0.131045</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>-0.111842</td>\n",
       "      <td>-0.116071</td>\n",
       "      <td>-0.115789</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.141892</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>-0.024055</td>\n",
       "      <td>-0.131534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.114896</td>\n",
       "      <td>0.168622</td>\n",
       "      <td>-0.034722</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.622657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999-06-26</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>0.050342</td>\n",
       "      <td>-0.257609</td>\n",
       "      <td>-0.003238</td>\n",
       "      <td>0.365517</td>\n",
       "      <td>0.018301</td>\n",
       "      <td>0.503704</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>-0.080986</td>\n",
       "      <td>-0.107143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.769984</td>\n",
       "      <td>0.176913</td>\n",
       "      <td>-0.028777</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.869012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1999-09-25</td>\n",
       "      <td>0.028292</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.045118</td>\n",
       "      <td>-0.142490</td>\n",
       "      <td>-0.453202</td>\n",
       "      <td>-0.496454</td>\n",
       "      <td>-0.483333</td>\n",
       "      <td>0.469799</td>\n",
       "      <td>0.597765</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>-0.136646</td>\n",
       "      <td>0.272031</td>\n",
       "      <td>0.248148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043564</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.518201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>0.469870</td>\n",
       "      <td>0.145624</td>\n",
       "      <td>0.542052</td>\n",
       "      <td>0.268560</td>\n",
       "      <td>0.422036</td>\n",
       "      <td>0.753743</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.429224</td>\n",
       "      <td>0.472028</td>\n",
       "      <td>0.357616</td>\n",
       "      <td>-0.155311</td>\n",
       "      <td>0.367470</td>\n",
       "      <td>0.440950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.296066</td>\n",
       "      <td>-0.046939</td>\n",
       "      <td>-0.048000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.727804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>-0.076325</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>-0.119798</td>\n",
       "      <td>-0.056997</td>\n",
       "      <td>-0.045084</td>\n",
       "      <td>-0.169868</td>\n",
       "      <td>0.273224</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>0.351438</td>\n",
       "      <td>0.275534</td>\n",
       "      <td>0.507317</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>-0.048458</td>\n",
       "      <td>0.322076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.087794</td>\n",
       "      <td>-0.025210</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.496042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>-0.010704</td>\n",
       "      <td>0.049878</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>-0.009253</td>\n",
       "      <td>-0.061697</td>\n",
       "      <td>-0.141631</td>\n",
       "      <td>-0.569444</td>\n",
       "      <td>-0.570312</td>\n",
       "      <td>-0.070922</td>\n",
       "      <td>-0.072626</td>\n",
       "      <td>-0.071197</td>\n",
       "      <td>-0.079557</td>\n",
       "      <td>-0.011574</td>\n",
       "      <td>-0.170093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>-0.039370</td>\n",
       "      <td>-0.034483</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.202670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-09-30</td>\n",
       "      <td>-0.018609</td>\n",
       "      <td>0.052356</td>\n",
       "      <td>-0.021771</td>\n",
       "      <td>0.032034</td>\n",
       "      <td>-0.016523</td>\n",
       "      <td>0.024658</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-0.177419</td>\n",
       "      <td>-0.145455</td>\n",
       "      <td>-0.185751</td>\n",
       "      <td>-0.080321</td>\n",
       "      <td>-0.369338</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>-0.185012</td>\n",
       "      <td>-0.165541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016713</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.498686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-12-30</td>\n",
       "      <td>-0.120094</td>\n",
       "      <td>-0.092316</td>\n",
       "      <td>-0.156528</td>\n",
       "      <td>-0.153130</td>\n",
       "      <td>-0.096177</td>\n",
       "      <td>-0.461497</td>\n",
       "      <td>-2.147059</td>\n",
       "      <td>-2.137255</td>\n",
       "      <td>-2.234043</td>\n",
       "      <td>-0.550000</td>\n",
       "      <td>-0.582969</td>\n",
       "      <td>-0.464088</td>\n",
       "      <td>-0.458311</td>\n",
       "      <td>-0.522989</td>\n",
       "      <td>-0.582996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147945</td>\n",
       "      <td>-0.376650</td>\n",
       "      <td>-0.123894</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.142219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>0.024056</td>\n",
       "      <td>0.085059</td>\n",
       "      <td>0.051891</td>\n",
       "      <td>0.096518</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>-1.220513</td>\n",
       "      <td>-1.206897</td>\n",
       "      <td>-1.206897</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>-0.109948</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>-0.449851</td>\n",
       "      <td>0.066265</td>\n",
       "      <td>0.901834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>-0.421824</td>\n",
       "      <td>-0.040404</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-5.579248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001-06-30</td>\n",
       "      <td>-0.009625</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>-0.074833</td>\n",
       "      <td>-0.100836</td>\n",
       "      <td>0.032103</td>\n",
       "      <td>0.030748</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>-0.629964</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>1.325581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030660</td>\n",
       "      <td>-0.614085</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.730243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2001-09-29</td>\n",
       "      <td>-0.008236</td>\n",
       "      <td>-0.020008</td>\n",
       "      <td>-0.050610</td>\n",
       "      <td>-0.059480</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.128049</td>\n",
       "      <td>-0.072165</td>\n",
       "      <td>-0.216418</td>\n",
       "      <td>-1.321951</td>\n",
       "      <td>-0.162037</td>\n",
       "      <td>1.712683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015815</td>\n",
       "      <td>0.824818</td>\n",
       "      <td>-0.043011</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.559217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2001-12-29</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>0.025083</td>\n",
       "      <td>0.026178</td>\n",
       "      <td>0.040184</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>-0.051724</td>\n",
       "      <td>-0.424242</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.027972</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>-9.136364</td>\n",
       "      <td>-0.038674</td>\n",
       "      <td>-0.326920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018541</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.850772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2002-03-30</td>\n",
       "      <td>0.023195</td>\n",
       "      <td>0.026935</td>\n",
       "      <td>0.043599</td>\n",
       "      <td>0.082331</td>\n",
       "      <td>0.012103</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187050</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>-0.031657</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>-0.485707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023929</td>\n",
       "      <td>-0.024793</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.727060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2002-06-29</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>-0.010222</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>-0.044147</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-0.078788</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>-0.202703</td>\n",
       "      <td>-0.151923</td>\n",
       "      <td>-0.087379</td>\n",
       "      <td>-0.063117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>-0.135593</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.052336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2002-09-28</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>-0.010105</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>-0.034924</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.009797</td>\n",
       "      <td>-2.406250</td>\n",
       "      <td>-2.444444</td>\n",
       "      <td>-2.444444</td>\n",
       "      <td>-0.230263</td>\n",
       "      <td>-0.279570</td>\n",
       "      <td>-0.152542</td>\n",
       "      <td>-0.634921</td>\n",
       "      <td>-0.234043</td>\n",
       "      <td>-0.109509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006435</td>\n",
       "      <td>-0.630719</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.591982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2002-12-28</td>\n",
       "      <td>-0.004605</td>\n",
       "      <td>-0.000742</td>\n",
       "      <td>-0.022696</td>\n",
       "      <td>-0.037998</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.020097</td>\n",
       "      <td>-0.822222</td>\n",
       "      <td>-0.846154</td>\n",
       "      <td>-0.846154</td>\n",
       "      <td>-0.059829</td>\n",
       "      <td>-0.082090</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>-0.708075</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>1.663761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>-0.707965</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.563357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2003-03-29</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>0.258307</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>-0.113821</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>-1.361702</td>\n",
       "      <td>-0.066176</td>\n",
       "      <td>2.403460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.416243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2003-06-28</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>0.014265</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>-0.050505</td>\n",
       "      <td>1.823529</td>\n",
       "      <td>0.125984</td>\n",
       "      <td>-0.399416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.977599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2003-09-27</td>\n",
       "      <td>0.058066</td>\n",
       "      <td>0.061486</td>\n",
       "      <td>0.155595</td>\n",
       "      <td>0.163951</td>\n",
       "      <td>0.005955</td>\n",
       "      <td>0.110032</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>-4.458333</td>\n",
       "      <td>0.265734</td>\n",
       "      <td>-0.665047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.762035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2003-12-27</td>\n",
       "      <td>0.022891</td>\n",
       "      <td>0.024121</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.025337</td>\n",
       "      <td>0.169679</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873874</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.317242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2004-03-27</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.040637</td>\n",
       "      <td>-0.149943</td>\n",
       "      <td>-0.168766</td>\n",
       "      <td>0.036952</td>\n",
       "      <td>-0.048355</td>\n",
       "      <td>-0.269841</td>\n",
       "      <td>-0.235294</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.100629</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.201807</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>-0.449727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.080715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2004-06-26</td>\n",
       "      <td>0.072457</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>0.073942</td>\n",
       "      <td>0.088384</td>\n",
       "      <td>0.071715</td>\n",
       "      <td>0.055003</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.217143</td>\n",
       "      <td>0.217172</td>\n",
       "      <td>0.218543</td>\n",
       "      <td>0.200501</td>\n",
       "      <td>0.221154</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.522792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2004-09-25</td>\n",
       "      <td>0.114495</td>\n",
       "      <td>0.122335</td>\n",
       "      <td>0.233513</td>\n",
       "      <td>0.243619</td>\n",
       "      <td>0.054863</td>\n",
       "      <td>0.166832</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.124481</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.231733</td>\n",
       "      <td>0.062992</td>\n",
       "      <td>-0.090699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.189286</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-7.614191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2004-12-25</td>\n",
       "      <td>0.162981</td>\n",
       "      <td>0.179589</td>\n",
       "      <td>0.201076</td>\n",
       "      <td>0.202985</td>\n",
       "      <td>0.140662</td>\n",
       "      <td>0.485106</td>\n",
       "      <td>1.783019</td>\n",
       "      <td>1.678571</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>0.804428</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.708475</td>\n",
       "      <td>0.551852</td>\n",
       "      <td>0.279050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.561562</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.812880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2005-03-26</td>\n",
       "      <td>0.080004</td>\n",
       "      <td>0.082312</td>\n",
       "      <td>0.042833</td>\n",
       "      <td>0.039702</td>\n",
       "      <td>0.102936</td>\n",
       "      <td>-0.070774</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>-0.520000</td>\n",
       "      <td>-0.514286</td>\n",
       "      <td>0.450928</td>\n",
       "      <td>0.316973</td>\n",
       "      <td>0.695489</td>\n",
       "      <td>0.352183</td>\n",
       "      <td>0.293556</td>\n",
       "      <td>-0.174271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.303846</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>-0.307692</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.475057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2005-06-25</td>\n",
       "      <td>0.037286</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>-0.015570</td>\n",
       "      <td>-0.068317</td>\n",
       "      <td>0.068118</td>\n",
       "      <td>0.085415</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.016453</td>\n",
       "      <td>-0.029503</td>\n",
       "      <td>0.082040</td>\n",
       "      <td>0.232575</td>\n",
       "      <td>-0.075646</td>\n",
       "      <td>-0.301478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182891</td>\n",
       "      <td>0.039063</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.278238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2005-09-24</td>\n",
       "      <td>0.101354</td>\n",
       "      <td>0.098549</td>\n",
       "      <td>0.113990</td>\n",
       "      <td>0.115594</td>\n",
       "      <td>0.094561</td>\n",
       "      <td>0.044886</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.151079</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.201190</td>\n",
       "      <td>0.091816</td>\n",
       "      <td>-0.130605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194514</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.233748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>0.227686</td>\n",
       "      <td>0.180777</td>\n",
       "      <td>0.420073</td>\n",
       "      <td>0.452354</td>\n",
       "      <td>0.122422</td>\n",
       "      <td>0.563078</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.376563</td>\n",
       "      <td>0.418421</td>\n",
       "      <td>0.315385</td>\n",
       "      <td>0.095144</td>\n",
       "      <td>0.268739</td>\n",
       "      <td>0.045491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034447</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>-0.727273</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.619086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2006-04-01</td>\n",
       "      <td>-0.019040</td>\n",
       "      <td>-0.072028</td>\n",
       "      <td>-0.098604</td>\n",
       "      <td>-0.119368</td>\n",
       "      <td>0.036038</td>\n",
       "      <td>-0.241781</td>\n",
       "      <td>-0.274336</td>\n",
       "      <td>-0.279412</td>\n",
       "      <td>-0.276923</td>\n",
       "      <td>0.167991</td>\n",
       "      <td>0.144712</td>\n",
       "      <td>0.204678</td>\n",
       "      <td>-0.004072</td>\n",
       "      <td>0.051873</td>\n",
       "      <td>-0.020238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>-2.666667</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.381817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>0.086478</td>\n",
       "      <td>0.106769</td>\n",
       "      <td>0.106139</td>\n",
       "      <td>0.127244</td>\n",
       "      <td>0.074637</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.151220</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>-0.103013</td>\n",
       "      <td>-0.145867</td>\n",
       "      <td>-0.038835</td>\n",
       "      <td>0.007269</td>\n",
       "      <td>-0.134247</td>\n",
       "      <td>-0.161632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037111</td>\n",
       "      <td>-0.036232</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.864912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2006-09-30</td>\n",
       "      <td>0.138349</td>\n",
       "      <td>0.161556</td>\n",
       "      <td>0.248444</td>\n",
       "      <td>0.288274</td>\n",
       "      <td>0.070096</td>\n",
       "      <td>0.106865</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>-0.009751</td>\n",
       "      <td>0.054080</td>\n",
       "      <td>-0.094697</td>\n",
       "      <td>-0.013532</td>\n",
       "      <td>-0.066456</td>\n",
       "      <td>-0.087773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003868</td>\n",
       "      <td>-0.037594</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>9.086989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2006-12-30</td>\n",
       "      <td>0.131125</td>\n",
       "      <td>0.148528</td>\n",
       "      <td>0.140147</td>\n",
       "      <td>0.133828</td>\n",
       "      <td>0.124599</td>\n",
       "      <td>0.470953</td>\n",
       "      <td>0.852399</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.295405</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.446304</td>\n",
       "      <td>0.132144</td>\n",
       "      <td>0.208475</td>\n",
       "      <td>0.232613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139806</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.350419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-03-31</td>\n",
       "      <td>-0.038539</td>\n",
       "      <td>-0.038106</td>\n",
       "      <td>-0.216567</td>\n",
       "      <td>-0.252419</td>\n",
       "      <td>0.092002</td>\n",
       "      <td>-0.260155</td>\n",
       "      <td>-0.233068</td>\n",
       "      <td>-0.239316</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>0.084459</td>\n",
       "      <td>0.049587</td>\n",
       "      <td>0.128255</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>-0.032258</td>\n",
       "      <td>-0.107916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100511</td>\n",
       "      <td>-0.031746</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.149176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2007-06-30</td>\n",
       "      <td>0.156913</td>\n",
       "      <td>0.169443</td>\n",
       "      <td>0.277984</td>\n",
       "      <td>0.274749</td>\n",
       "      <td>0.093222</td>\n",
       "      <td>0.027736</td>\n",
       "      <td>0.062338</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.208723</td>\n",
       "      <td>0.304939</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.026488</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.055573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071981</td>\n",
       "      <td>-0.032787</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-9.694990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2007-09-29</td>\n",
       "      <td>0.170924</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.312022</td>\n",
       "      <td>0.329949</td>\n",
       "      <td>0.084154</td>\n",
       "      <td>0.149168</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.227448</td>\n",
       "      <td>0.214482</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.016829</td>\n",
       "      <td>0.132026</td>\n",
       "      <td>0.095695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051264</td>\n",
       "      <td>-0.042373</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.591031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2007-12-29</td>\n",
       "      <td>0.185111</td>\n",
       "      <td>0.192795</td>\n",
       "      <td>0.223763</td>\n",
       "      <td>0.132918</td>\n",
       "      <td>0.156345</td>\n",
       "      <td>0.545440</td>\n",
       "      <td>0.748894</td>\n",
       "      <td>0.740385</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.325459</td>\n",
       "      <td>0.309395</td>\n",
       "      <td>0.349216</td>\n",
       "      <td>0.051122</td>\n",
       "      <td>0.229792</td>\n",
       "      <td>0.193788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055632</td>\n",
       "      <td>-0.017699</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.428967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008-03-29</td>\n",
       "      <td>0.014381</td>\n",
       "      <td>0.020887</td>\n",
       "      <td>-0.061730</td>\n",
       "      <td>-0.085524</td>\n",
       "      <td>0.074328</td>\n",
       "      <td>-0.218152</td>\n",
       "      <td>-0.339026</td>\n",
       "      <td>-0.342541</td>\n",
       "      <td>-0.340909</td>\n",
       "      <td>-0.106535</td>\n",
       "      <td>-0.012073</td>\n",
       "      <td>-0.233736</td>\n",
       "      <td>-0.030791</td>\n",
       "      <td>-0.224413</td>\n",
       "      <td>-0.228152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015615</td>\n",
       "      <td>-0.036036</td>\n",
       "      <td>-0.595238</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.530651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2008-06-28</td>\n",
       "      <td>0.040629</td>\n",
       "      <td>0.047202</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>-0.043180</td>\n",
       "      <td>0.086911</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>0.025837</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.059840</td>\n",
       "      <td>-0.041201</td>\n",
       "      <td>0.234688</td>\n",
       "      <td>-0.037184</td>\n",
       "      <td>-0.012107</td>\n",
       "      <td>-0.003745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012558</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.007699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2008-09-27</td>\n",
       "      <td>0.247974</td>\n",
       "      <td>0.239017</td>\n",
       "      <td>0.534045</td>\n",
       "      <td>0.528748</td>\n",
       "      <td>0.071756</td>\n",
       "      <td>0.057744</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.049587</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>-0.099122</td>\n",
       "      <td>-0.058995</td>\n",
       "      <td>-0.153242</td>\n",
       "      <td>-0.039745</td>\n",
       "      <td>-0.164216</td>\n",
       "      <td>-0.146617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004016</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.426921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2008-12-27</td>\n",
       "      <td>0.081244</td>\n",
       "      <td>0.013635</td>\n",
       "      <td>0.072053</td>\n",
       "      <td>0.047190</td>\n",
       "      <td>0.089349</td>\n",
       "      <td>0.287777</td>\n",
       "      <td>0.412852</td>\n",
       "      <td>0.425197</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>-0.340297</td>\n",
       "      <td>-0.338235</td>\n",
       "      <td>-0.343968</td>\n",
       "      <td>-0.070285</td>\n",
       "      <td>-0.384164</td>\n",
       "      <td>-0.371061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012097</td>\n",
       "      <td>-0.080808</td>\n",
       "      <td>-0.079365</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3.366126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2009-03-28</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>-0.037255</td>\n",
       "      <td>-0.047892</td>\n",
       "      <td>-0.068171</td>\n",
       "      <td>0.061199</td>\n",
       "      <td>-0.197108</td>\n",
       "      <td>-0.249221</td>\n",
       "      <td>-0.254144</td>\n",
       "      <td>-0.252809</td>\n",
       "      <td>-0.054187</td>\n",
       "      <td>-0.081287</td>\n",
       "      <td>-0.012378</td>\n",
       "      <td>-0.040739</td>\n",
       "      <td>-0.128571</td>\n",
       "      <td>-0.057651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>-0.054945</td>\n",
       "      <td>-0.793103</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.294416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2009-06-27</td>\n",
       "      <td>0.113398</td>\n",
       "      <td>0.038903</td>\n",
       "      <td>0.175737</td>\n",
       "      <td>0.211621</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>0.021316</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.323661</td>\n",
       "      <td>0.330999</td>\n",
       "      <td>0.312444</td>\n",
       "      <td>-0.037215</td>\n",
       "      <td>0.248634</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>-0.069767</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.806961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2009-09-26</td>\n",
       "      <td>-0.013274</td>\n",
       "      <td>-0.102786</td>\n",
       "      <td>-0.287210</td>\n",
       "      <td>-0.309405</td>\n",
       "      <td>0.222188</td>\n",
       "      <td>0.947703</td>\n",
       "      <td>2.414158</td>\n",
       "      <td>2.376812</td>\n",
       "      <td>2.407407</td>\n",
       "      <td>0.297920</td>\n",
       "      <td>0.290770</td>\n",
       "      <td>0.309686</td>\n",
       "      <td>0.430196</td>\n",
       "      <td>0.223195</td>\n",
       "      <td>0.261586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.281897</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.373251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2009-12-26</td>\n",
       "      <td>0.135260</td>\n",
       "      <td>0.056314</td>\n",
       "      <td>0.144821</td>\n",
       "      <td>0.138276</td>\n",
       "      <td>0.130468</td>\n",
       "      <td>-0.034179</td>\n",
       "      <td>-0.194948</td>\n",
       "      <td>-0.197425</td>\n",
       "      <td>-0.202174</td>\n",
       "      <td>0.206583</td>\n",
       "      <td>0.108188</td>\n",
       "      <td>0.344271</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>-0.007156</td>\n",
       "      <td>-0.239491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.512084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2010-03-27</td>\n",
       "      <td>0.058061</td>\n",
       "      <td>-0.029881</td>\n",
       "      <td>-0.024727</td>\n",
       "      <td>-0.066275</td>\n",
       "      <td>0.100089</td>\n",
       "      <td>-0.139259</td>\n",
       "      <td>-0.089994</td>\n",
       "      <td>-0.093583</td>\n",
       "      <td>-0.092643</td>\n",
       "      <td>0.082556</td>\n",
       "      <td>0.107991</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.052291</td>\n",
       "      <td>-0.034234</td>\n",
       "      <td>-0.104505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068699</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>-0.623529</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.437773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2010-06-26</td>\n",
       "      <td>0.134392</td>\n",
       "      <td>0.114331</td>\n",
       "      <td>0.220509</td>\n",
       "      <td>0.276638</td>\n",
       "      <td>0.095634</td>\n",
       "      <td>0.163049</td>\n",
       "      <td>0.058230</td>\n",
       "      <td>0.053097</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.132626</td>\n",
       "      <td>0.202776</td>\n",
       "      <td>0.047093</td>\n",
       "      <td>0.035734</td>\n",
       "      <td>0.031716</td>\n",
       "      <td>-0.042531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029425</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.508989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2010-09-25</td>\n",
       "      <td>0.161576</td>\n",
       "      <td>0.156662</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.327312</td>\n",
       "      <td>0.108557</td>\n",
       "      <td>0.295732</td>\n",
       "      <td>0.324316</td>\n",
       "      <td>0.316527</td>\n",
       "      <td>0.321937</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0.051932</td>\n",
       "      <td>0.182361</td>\n",
       "      <td>-0.090027</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>-0.051463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055409</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.016228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2010-12-25</td>\n",
       "      <td>0.153745</td>\n",
       "      <td>0.053961</td>\n",
       "      <td>0.170999</td>\n",
       "      <td>0.148296</td>\n",
       "      <td>0.143856</td>\n",
       "      <td>0.314506</td>\n",
       "      <td>0.393686</td>\n",
       "      <td>0.389362</td>\n",
       "      <td>0.385776</td>\n",
       "      <td>0.135486</td>\n",
       "      <td>0.109707</td>\n",
       "      <td>0.167608</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.028520</td>\n",
       "      <td>0.132496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015363</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.300537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2011-03-26</td>\n",
       "      <td>0.094095</td>\n",
       "      <td>0.069889</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>0.124593</td>\n",
       "      <td>-0.077559</td>\n",
       "      <td>-0.002831</td>\n",
       "      <td>-0.006126</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>0.142391</td>\n",
       "      <td>0.120352</td>\n",
       "      <td>0.168236</td>\n",
       "      <td>0.049458</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>-0.033787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>-0.348485</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.127247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2011-06-25</td>\n",
       "      <td>0.124905</td>\n",
       "      <td>-0.002107</td>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.104082</td>\n",
       "      <td>0.127950</td>\n",
       "      <td>0.158268</td>\n",
       "      <td>0.220645</td>\n",
       "      <td>0.215716</td>\n",
       "      <td>0.217187</td>\n",
       "      <td>-0.029988</td>\n",
       "      <td>-0.026856</td>\n",
       "      <td>-0.033551</td>\n",
       "      <td>0.071750</td>\n",
       "      <td>-0.133218</td>\n",
       "      <td>-0.171712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052326</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.872706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2011-09-24</td>\n",
       "      <td>0.090045</td>\n",
       "      <td>-0.040727</td>\n",
       "      <td>0.062568</td>\n",
       "      <td>0.041364</td>\n",
       "      <td>0.104870</td>\n",
       "      <td>-0.010535</td>\n",
       "      <td>-0.093733</td>\n",
       "      <td>-0.096324</td>\n",
       "      <td>-0.093710</td>\n",
       "      <td>0.126814</td>\n",
       "      <td>0.190814</td>\n",
       "      <td>0.053877</td>\n",
       "      <td>-0.022727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.064272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017850</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.423077</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-16.311055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>0.191714</td>\n",
       "      <td>0.217458</td>\n",
       "      <td>0.223136</td>\n",
       "      <td>0.237290</td>\n",
       "      <td>0.175410</td>\n",
       "      <td>0.638946</td>\n",
       "      <td>0.972520</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.964589</td>\n",
       "      <td>0.041060</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.082567</td>\n",
       "      <td>0.121082</td>\n",
       "      <td>-0.053892</td>\n",
       "      <td>-0.049832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077244</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>1.744444</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.974780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>0.088354</td>\n",
       "      <td>-0.074109</td>\n",
       "      <td>-0.003928</td>\n",
       "      <td>-0.074291</td>\n",
       "      <td>0.138184</td>\n",
       "      <td>-0.154253</td>\n",
       "      <td>-0.110380</td>\n",
       "      <td>-0.112616</td>\n",
       "      <td>-0.113194</td>\n",
       "      <td>0.319649</td>\n",
       "      <td>0.456365</td>\n",
       "      <td>0.154515</td>\n",
       "      <td>0.028861</td>\n",
       "      <td>0.124473</td>\n",
       "      <td>0.039688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051550</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>-0.230769</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.343357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>0.079253</td>\n",
       "      <td>0.024274</td>\n",
       "      <td>0.056033</td>\n",
       "      <td>0.031964</td>\n",
       "      <td>0.090226</td>\n",
       "      <td>-0.106237</td>\n",
       "      <td>-0.240750</td>\n",
       "      <td>-0.243373</td>\n",
       "      <td>-0.242276</td>\n",
       "      <td>0.131640</td>\n",
       "      <td>0.036269</td>\n",
       "      <td>0.276741</td>\n",
       "      <td>-0.076485</td>\n",
       "      <td>-0.001876</td>\n",
       "      <td>-0.030675</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005898</td>\n",
       "      <td>-0.054054</td>\n",
       "      <td>-0.421053</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.338118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2012-09-29</td>\n",
       "      <td>0.080837</td>\n",
       "      <td>0.109928</td>\n",
       "      <td>0.131065</td>\n",
       "      <td>0.165820</td>\n",
       "      <td>0.057845</td>\n",
       "      <td>0.026925</td>\n",
       "      <td>-0.068110</td>\n",
       "      <td>-0.070064</td>\n",
       "      <td>-0.068670</td>\n",
       "      <td>0.093397</td>\n",
       "      <td>0.094783</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>-0.062411</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.053446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.038772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011123</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>-0.218182</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.600312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2012-12-29</td>\n",
       "      <td>0.113731</td>\n",
       "      <td>0.254887</td>\n",
       "      <td>0.188198</td>\n",
       "      <td>0.216309</td>\n",
       "      <td>0.077286</td>\n",
       "      <td>0.515654</td>\n",
       "      <td>0.590417</td>\n",
       "      <td>0.590183</td>\n",
       "      <td>0.591014</td>\n",
       "      <td>-0.076197</td>\n",
       "      <td>-0.040111</td>\n",
       "      <td>-0.120717</td>\n",
       "      <td>-0.080739</td>\n",
       "      <td>-0.123364</td>\n",
       "      <td>-0.110147</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>1.001681</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.049494</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>2.709302</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.567644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2013-03-30</td>\n",
       "      <td>-0.006859</td>\n",
       "      <td>-0.124551</td>\n",
       "      <td>-0.138038</td>\n",
       "      <td>-0.242561</td>\n",
       "      <td>0.063952</td>\n",
       "      <td>-0.200121</td>\n",
       "      <td>-0.269995</td>\n",
       "      <td>-0.270639</td>\n",
       "      <td>-0.269370</td>\n",
       "      <td>-0.173045</td>\n",
       "      <td>-0.179872</td>\n",
       "      <td>-0.163966</td>\n",
       "      <td>-0.113436</td>\n",
       "      <td>-0.234542</td>\n",
       "      <td>-0.171793</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>0.578505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074556</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>-0.514107</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-5.756350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2013-06-29</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.077080</td>\n",
       "      <td>0.291108</td>\n",
       "      <td>0.022840</td>\n",
       "      <td>-0.089571</td>\n",
       "      <td>-0.189895</td>\n",
       "      <td>-0.277260</td>\n",
       "      <td>-0.260827</td>\n",
       "      <td>-0.259663</td>\n",
       "      <td>-0.126473</td>\n",
       "      <td>-0.160802</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-0.070497</td>\n",
       "      <td>-0.178273</td>\n",
       "      <td>-0.080616</td>\n",
       "      <td>0.367521</td>\n",
       "      <td>0.091489</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.050298</td>\n",
       "      <td>-0.053763</td>\n",
       "      <td>-0.412903</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.790157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2013-09-28</td>\n",
       "      <td>0.035746</td>\n",
       "      <td>0.074275</td>\n",
       "      <td>0.090834</td>\n",
       "      <td>0.202071</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.060839</td>\n",
       "      <td>0.088696</td>\n",
       "      <td>0.106525</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.075354</td>\n",
       "      <td>0.102946</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>-0.029068</td>\n",
       "      <td>0.142373</td>\n",
       "      <td>0.125123</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.380604</td>\n",
       "      <td>-0.001455</td>\n",
       "      <td>-0.027379</td>\n",
       "      <td>-0.022727</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.415739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2013-12-28</td>\n",
       "      <td>0.087845</td>\n",
       "      <td>0.096349</td>\n",
       "      <td>0.144384</td>\n",
       "      <td>0.231596</td>\n",
       "      <td>0.049656</td>\n",
       "      <td>0.536988</td>\n",
       "      <td>0.740149</td>\n",
       "      <td>0.755716</td>\n",
       "      <td>0.755448</td>\n",
       "      <td>0.147032</td>\n",
       "      <td>0.119499</td>\n",
       "      <td>0.182310</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>0.133531</td>\n",
       "      <td>0.159370</td>\n",
       "      <td>0.210784</td>\n",
       "      <td>0.022238</td>\n",
       "      <td>-0.047342</td>\n",
       "      <td>-0.017997</td>\n",
       "      <td>-0.023256</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>5.723109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2014-03-29</td>\n",
       "      <td>-0.085241</td>\n",
       "      <td>-0.122046</td>\n",
       "      <td>-0.101466</td>\n",
       "      <td>-0.196414</td>\n",
       "      <td>-0.073294</td>\n",
       "      <td>-0.207452</td>\n",
       "      <td>-0.217947</td>\n",
       "      <td>-0.198766</td>\n",
       "      <td>-0.198621</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>-0.024099</td>\n",
       "      <td>0.040431</td>\n",
       "      <td>0.049430</td>\n",
       "      <td>-0.049738</td>\n",
       "      <td>-0.012085</td>\n",
       "      <td>0.178138</td>\n",
       "      <td>-0.005180</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.393939</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.076757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2014-06-28</td>\n",
       "      <td>0.080252</td>\n",
       "      <td>-0.036745</td>\n",
       "      <td>0.183778</td>\n",
       "      <td>0.069362</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>-0.179950</td>\n",
       "      <td>-0.242101</td>\n",
       "      <td>-0.889649</td>\n",
       "      <td>-0.889845</td>\n",
       "      <td>0.115609</td>\n",
       "      <td>0.185458</td>\n",
       "      <td>0.036023</td>\n",
       "      <td>0.027668</td>\n",
       "      <td>0.162534</td>\n",
       "      <td>0.074924</td>\n",
       "      <td>0.161512</td>\n",
       "      <td>-0.018049</td>\n",
       "      <td>0.700921</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.345000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.941488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2014-09-27</td>\n",
       "      <td>0.041879</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.184209</td>\n",
       "      <td>0.373185</td>\n",
       "      <td>-0.077667</td>\n",
       "      <td>0.125321</td>\n",
       "      <td>0.092798</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.164902</td>\n",
       "      <td>0.091426</td>\n",
       "      <td>0.260643</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.149289</td>\n",
       "      <td>0.124467</td>\n",
       "      <td>0.142012</td>\n",
       "      <td>-0.018028</td>\n",
       "      <td>0.082917</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.472960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2014-12-27</td>\n",
       "      <td>0.129637</td>\n",
       "      <td>0.217011</td>\n",
       "      <td>0.151914</td>\n",
       "      <td>0.160178</td>\n",
       "      <td>0.105615</td>\n",
       "      <td>0.770980</td>\n",
       "      <td>1.128735</td>\n",
       "      <td>1.169014</td>\n",
       "      <td>1.154930</td>\n",
       "      <td>0.097641</td>\n",
       "      <td>0.154328</td>\n",
       "      <td>0.033554</td>\n",
       "      <td>0.140415</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>0.056926</td>\n",
       "      <td>0.121762</td>\n",
       "      <td>-0.109791</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.029616</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>2.268750</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.998029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2015-03-28</td>\n",
       "      <td>-0.002673</td>\n",
       "      <td>-0.185989</td>\n",
       "      <td>-0.046029</td>\n",
       "      <td>-0.202171</td>\n",
       "      <td>0.046040</td>\n",
       "      <td>-0.222376</td>\n",
       "      <td>-0.247170</td>\n",
       "      <td>-0.240260</td>\n",
       "      <td>-0.238562</td>\n",
       "      <td>0.108309</td>\n",
       "      <td>0.115658</td>\n",
       "      <td>0.099286</td>\n",
       "      <td>0.055675</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.039497</td>\n",
       "      <td>0.108545</td>\n",
       "      <td>-0.063081</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.449331</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.082975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2015-06-27</td>\n",
       "      <td>0.045778</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>0.115638</td>\n",
       "      <td>0.111631</td>\n",
       "      <td>-0.025805</td>\n",
       "      <td>-0.144889</td>\n",
       "      <td>-0.213133</td>\n",
       "      <td>-0.205128</td>\n",
       "      <td>-0.206009</td>\n",
       "      <td>0.081521</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.176527</td>\n",
       "      <td>0.051217</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>-0.008100</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>-0.044886</td>\n",
       "      <td>0.214746</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.215278</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.589325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2015-09-26</td>\n",
       "      <td>0.063437</td>\n",
       "      <td>0.259679</td>\n",
       "      <td>0.160367</td>\n",
       "      <td>0.234740</td>\n",
       "      <td>-0.050304</td>\n",
       "      <td>0.038222</td>\n",
       "      <td>0.041866</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>-0.126844</td>\n",
       "      <td>-0.011669</td>\n",
       "      <td>-0.252640</td>\n",
       "      <td>0.035697</td>\n",
       "      <td>-0.113043</td>\n",
       "      <td>-0.184045</td>\n",
       "      <td>0.097744</td>\n",
       "      <td>-0.042928</td>\n",
       "      <td>0.187119</td>\n",
       "      <td>0.010168</td>\n",
       "      <td>-0.011494</td>\n",
       "      <td>-0.221239</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.738844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2015-12-26</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>-0.147229</td>\n",
       "      <td>-0.035688</td>\n",
       "      <td>-0.056048</td>\n",
       "      <td>0.074668</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.650575</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>-0.068813</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>-0.040031</td>\n",
       "      <td>0.089041</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>-0.073900</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>-0.023256</td>\n",
       "      <td>1.403409</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.937428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>0.040892</td>\n",
       "      <td>0.149215</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>-0.102862</td>\n",
       "      <td>0.017074</td>\n",
       "      <td>-0.333654</td>\n",
       "      <td>-0.427264</td>\n",
       "      <td>-0.421212</td>\n",
       "      <td>-0.420732</td>\n",
       "      <td>-0.120150</td>\n",
       "      <td>-0.116217</td>\n",
       "      <td>-0.124846</td>\n",
       "      <td>-0.059593</td>\n",
       "      <td>-0.186567</td>\n",
       "      <td>-0.141139</td>\n",
       "      <td>0.081761</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.282064</td>\n",
       "      <td>-0.026235</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.612293</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.769061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2016-06-25</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.070429</td>\n",
       "      <td>0.024259</td>\n",
       "      <td>0.047184</td>\n",
       "      <td>-0.030018</td>\n",
       "      <td>-0.162173</td>\n",
       "      <td>-0.258653</td>\n",
       "      <td>-0.251309</td>\n",
       "      <td>-0.252632</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.027049</td>\n",
       "      <td>-0.031605</td>\n",
       "      <td>-0.058400</td>\n",
       "      <td>-0.027523</td>\n",
       "      <td>0.048553</td>\n",
       "      <td>0.082849</td>\n",
       "      <td>0.069930</td>\n",
       "      <td>0.024445</td>\n",
       "      <td>-0.025595</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>-0.128049</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.141397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>0.052631</td>\n",
       "      <td>0.139802</td>\n",
       "      <td>0.080285</td>\n",
       "      <td>0.105195</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>0.106096</td>\n",
       "      <td>0.156234</td>\n",
       "      <td>0.174825</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>0.028832</td>\n",
       "      <td>0.033722</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>-0.060702</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.080142</td>\n",
       "      <td>0.076510</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.079479</td>\n",
       "      <td>-0.023502</td>\n",
       "      <td>-0.054054</td>\n",
       "      <td>0.587413</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.502444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>0.029392</td>\n",
       "      <td>-0.033097</td>\n",
       "      <td>0.027471</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>0.032289</td>\n",
       "      <td>0.672309</td>\n",
       "      <td>0.984801</td>\n",
       "      <td>1.011905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072612</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>0.137486</td>\n",
       "      <td>-0.018264</td>\n",
       "      <td>0.047511</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.071072</td>\n",
       "      <td>0.019676</td>\n",
       "      <td>-0.055263</td>\n",
       "      <td>-0.021708</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>0.986784</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.783981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>0.008548</td>\n",
       "      <td>-0.128230</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>-0.324884</td>\n",
       "      <td>-0.383545</td>\n",
       "      <td>-0.375740</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.163853</td>\n",
       "      <td>0.217457</td>\n",
       "      <td>0.102613</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.110151</td>\n",
       "      <td>0.153160</td>\n",
       "      <td>0.066356</td>\n",
       "      <td>-0.001135</td>\n",
       "      <td>0.134629</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>-0.014493</td>\n",
       "      <td>-0.596452</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.441879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>0.031809</td>\n",
       "      <td>0.106726</td>\n",
       "      <td>0.061352</td>\n",
       "      <td>0.108533</td>\n",
       "      <td>-0.012358</td>\n",
       "      <td>-0.141561</td>\n",
       "      <td>-0.209629</td>\n",
       "      <td>-0.203791</td>\n",
       "      <td>-0.204762</td>\n",
       "      <td>0.144488</td>\n",
       "      <td>0.084083</td>\n",
       "      <td>0.220460</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.122568</td>\n",
       "      <td>0.117344</td>\n",
       "      <td>0.068777</td>\n",
       "      <td>-0.004924</td>\n",
       "      <td>0.076459</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>-0.014706</td>\n",
       "      <td>-0.362637</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.025327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>0.087336</td>\n",
       "      <td>0.139712</td>\n",
       "      <td>0.134074</td>\n",
       "      <td>0.239994</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.157924</td>\n",
       "      <td>0.229093</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>0.035859</td>\n",
       "      <td>0.052921</td>\n",
       "      <td>0.016779</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.064351</td>\n",
       "      <td>-0.019794</td>\n",
       "      <td>0.068671</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>-0.014925</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.122030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>0.083862</td>\n",
       "      <td>0.117883</td>\n",
       "      <td>0.104956</td>\n",
       "      <td>0.148531</td>\n",
       "      <td>0.045894</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.872783</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.888350</td>\n",
       "      <td>0.072553</td>\n",
       "      <td>0.074330</td>\n",
       "      <td>0.070571</td>\n",
       "      <td>0.029760</td>\n",
       "      <td>0.053422</td>\n",
       "      <td>0.028670</td>\n",
       "      <td>0.060461</td>\n",
       "      <td>-0.027184</td>\n",
       "      <td>0.022063</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.015152</td>\n",
       "      <td>1.198238</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>754.331132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quarter end    Assets  Current Assets  Liabilities  Current Liabilities  \\\n",
       "0   1996-03-29  0.054530        0.045478     0.061899             0.058088   \n",
       "1   1996-06-28  0.021207        0.041384     0.046256            -0.152662   \n",
       "2   1996-09-27  0.003555        0.013696    -0.005714             0.039979   \n",
       "3   1996-12-27 -0.017151       -0.021262     0.007260             0.020469   \n",
       "4   1997-03-28 -0.148900       -0.175832    -0.025526            -0.016145   \n",
       "5   1997-06-27 -0.032538       -0.040912    -0.030817            -0.050224   \n",
       "6   1997-09-26 -0.024879       -0.019754    -0.035612            -0.048168   \n",
       "7   1997-12-26 -0.025278       -0.014895    -0.049786            -0.081958   \n",
       "8   1998-03-27 -0.039506       -0.047436    -0.106523            -0.170761   \n",
       "9   1998-06-26  0.019682        0.050420    -0.007767             0.003613   \n",
       "10  1998-09-25  0.061371        0.095704     0.036008             0.094312   \n",
       "11  1998-12-26  0.070646        0.045700     0.008311            -0.023684   \n",
       "12  1999-03-27  0.074695        0.058185     0.034095             0.040431   \n",
       "13  1999-06-26  0.017021        0.050342    -0.257609            -0.003238   \n",
       "14  1999-09-25  0.028292       -0.003025     0.003904             0.006498   \n",
       "15  2000-01-01  0.469870        0.145624     0.542052             0.268560   \n",
       "16  2000-04-01 -0.076325        0.000611    -0.119798            -0.056997   \n",
       "17  2000-07-01 -0.010704        0.049878    -0.012894             0.010793   \n",
       "18  2000-09-30 -0.018609        0.052356    -0.021771             0.032034   \n",
       "19  2000-12-30 -0.120094       -0.092316    -0.156528            -0.153130   \n",
       "20  2001-03-31  0.024056        0.085059     0.051891             0.096518   \n",
       "21  2001-06-30 -0.009625       -0.018148    -0.074833            -0.100836   \n",
       "22  2001-09-29 -0.008236       -0.020008    -0.050610            -0.059480   \n",
       "23  2001-12-29  0.016775        0.025083     0.026178             0.040184   \n",
       "24  2002-03-30  0.023195        0.026935     0.043599             0.082331   \n",
       "25  2002-06-29  0.004630        0.005356    -0.010222             0.005266   \n",
       "26  2002-09-28  0.000795       -0.010105    -0.010777            -0.034924   \n",
       "27  2002-12-28 -0.004605       -0.000742    -0.022696            -0.037998   \n",
       "28  2003-03-29  0.014675        0.015602     0.032048             0.258307   \n",
       "29  2003-06-28  0.012577        0.014265     0.009451             0.008969   \n",
       "30  2003-09-27  0.058066        0.061486     0.155595             0.163951   \n",
       "31  2003-12-27  0.022891        0.024121     0.018904             0.010607   \n",
       "32  2004-03-27 -0.033855       -0.040637    -0.149943            -0.168766   \n",
       "33  2004-06-26  0.072457        0.086791     0.073942             0.088384   \n",
       "34  2004-09-25  0.114495        0.122335     0.233513             0.243619   \n",
       "35  2004-12-25  0.162981        0.179589     0.201076             0.202985   \n",
       "36  2005-03-26  0.080004        0.082312     0.042833             0.039702   \n",
       "37  2005-06-25  0.037286        0.040968    -0.015570            -0.068317   \n",
       "38  2005-09-24  0.101354        0.098549     0.113990             0.115594   \n",
       "39  2005-12-31  0.227686        0.180777     0.420073             0.452354   \n",
       "40  2006-04-01 -0.019040       -0.072028    -0.098604            -0.119368   \n",
       "41  2006-07-01  0.086478        0.106769     0.106139             0.127244   \n",
       "42  2006-09-30  0.138349        0.161556     0.248444             0.288274   \n",
       "43  2006-12-30  0.131125        0.148528     0.140147             0.133828   \n",
       "44  2007-03-31 -0.038539       -0.038106    -0.216567            -0.252419   \n",
       "45  2007-06-30  0.156913        0.169443     0.277984             0.274749   \n",
       "46  2007-09-29  0.170924        0.171299     0.312022             0.329949   \n",
       "47  2007-12-29  0.185111        0.192795     0.223763             0.132918   \n",
       "48  2008-03-29  0.014381        0.020887    -0.061730            -0.085524   \n",
       "49  2008-06-28  0.040629        0.047202    -0.026655            -0.043180   \n",
       "50  2008-09-27  0.247974        0.239017     0.534045             0.528748   \n",
       "51  2008-12-27  0.081244        0.013635     0.072053             0.047190   \n",
       "52  2009-03-28  0.010517       -0.037255    -0.047892            -0.068171   \n",
       "53  2009-06-27  0.113398        0.038903     0.175737             0.211621   \n",
       "54  2009-09-26 -0.013274       -0.102786    -0.287210            -0.309405   \n",
       "55  2009-12-26  0.135260        0.056314     0.144821             0.138276   \n",
       "56  2010-03-27  0.058061       -0.029881    -0.024727            -0.066275   \n",
       "57  2010-06-26  0.134392        0.114331     0.220509             0.276638   \n",
       "58  2010-09-25  0.161576        0.156662     0.267327             0.327312   \n",
       "59  2010-12-25  0.153745        0.053961     0.170999             0.148296   \n",
       "60  2011-03-26  0.094095        0.069889     0.042119             0.022358   \n",
       "61  2011-06-25  0.124905       -0.002107     0.119305             0.104082   \n",
       "62  2011-09-24  0.090045       -0.040727     0.062568             0.041364   \n",
       "63  2011-12-31  0.191714        0.217458     0.223136             0.237290   \n",
       "64  2012-03-31  0.088354       -0.074109    -0.003928            -0.074291   \n",
       "65  2012-06-30  0.079253        0.024274     0.056033             0.031964   \n",
       "66  2012-09-29  0.080837        0.109928     0.131065             0.165820   \n",
       "67  2012-12-29  0.113731        0.254887     0.188198             0.216309   \n",
       "68  2013-03-30 -0.006859       -0.124551    -0.138038            -0.242561   \n",
       "69  2013-06-29  0.026255        0.077080     0.291108             0.022840   \n",
       "70  2013-09-28  0.035746        0.074275     0.090834             0.202071   \n",
       "71  2013-12-28  0.087845        0.096349     0.144384             0.231596   \n",
       "72  2014-03-29 -0.085241       -0.122046    -0.101466            -0.196414   \n",
       "73  2014-06-28  0.080252       -0.036745     0.183778             0.069362   \n",
       "74  2014-09-27  0.041879        0.008565     0.184209             0.373185   \n",
       "75  2014-12-27  0.129637        0.217011     0.151914             0.160178   \n",
       "76  2015-03-28 -0.002673       -0.185989    -0.046029            -0.202171   \n",
       "77  2015-06-27  0.045778        0.045102     0.115638             0.111631   \n",
       "78  2015-09-26  0.063437        0.259679     0.160367             0.234740   \n",
       "79  2015-12-26  0.009656       -0.147229    -0.035688            -0.056048   \n",
       "80  2016-03-26  0.040892        0.149215     0.059406            -0.102862   \n",
       "81  2016-06-25  0.001065        0.070429     0.024259             0.047184   \n",
       "82  2016-09-24  0.052631        0.139802     0.080285             0.105195   \n",
       "83  2016-12-31  0.029392       -0.033097     0.027471             0.064856   \n",
       "84  2017-04-01  0.010240       -0.012987     0.008548            -0.128230   \n",
       "85  2017-07-01  0.031809        0.106726     0.061352             0.108533   \n",
       "86  2017-09-30  0.087336        0.139712     0.134074             0.239994   \n",
       "87  2017-12-30  0.083862        0.117883     0.104956             0.148531   \n",
       "\n",
       "    Shareholders equity   Revenue  Earnings  EPS basic  EPS diluted     Price  \\\n",
       "0              0.053942  0.075105  0.022818  -0.001147     0.099883  0.077414   \n",
       "1             -0.017510 -0.002746 -0.956757  -0.956594    -0.997349 -0.173077   \n",
       "2              0.018812  0.065168 -1.781250  -1.807692     1.000000 -0.151163   \n",
       "3             -0.056365 -0.082723 -5.800000  -5.571429     1.000000  0.205479   \n",
       "4             -0.360453 -0.248004  4.900000   4.875000     1.000000 -0.215909   \n",
       "5             -0.037037  0.084947 -0.920904  -0.921986     1.000000 -0.115942   \n",
       "6              0.003344 -0.070812  1.875000   1.863636     1.000000  0.245902   \n",
       "7              0.036667 -0.022305 -1.291925  -1.293651     1.000000 -0.118421   \n",
       "8              0.115756 -0.109632  0.170213   0.135135     0.151515  0.089552   \n",
       "9              0.070605 -0.002135  0.836364   0.809524     0.710526  0.369863   \n",
       "10             0.104980  0.109843  0.049505   0.026316     0.000000  0.280000   \n",
       "11             0.171133  0.098972  0.433962   0.435897     0.461538 -0.023438   \n",
       "12             0.131045 -0.105263 -0.111842  -0.116071    -0.115789  0.136000   \n",
       "13             0.365517  0.018301  0.503704   0.424242     0.428571  0.049296   \n",
       "14             0.045118 -0.142490 -0.453202  -0.496454    -0.483333  0.469799   \n",
       "15             0.422036  0.753743  0.648649   0.605634     0.661290  0.429224   \n",
       "16            -0.045084 -0.169868  0.273224   0.263158     0.242718  0.351438   \n",
       "17            -0.009253 -0.061697 -0.141631  -0.569444    -0.570312 -0.070922   \n",
       "18            -0.016523  0.024658 -0.150000  -0.177419    -0.145455 -0.185751   \n",
       "19            -0.096177 -0.461497 -2.147059  -2.137255    -2.234043 -0.550000   \n",
       "20             0.007004  0.421053 -1.220513  -1.206897    -1.206897 -0.055556   \n",
       "21             0.032103  0.030748  0.418605   0.416667     0.416667  0.205882   \n",
       "22             0.016071 -0.016949  0.081967   0.176471     0.176471 -0.128049   \n",
       "23             0.011735 -0.051724 -0.424242  -0.450000    -0.450000 -0.027972   \n",
       "24             0.012103  0.087273  0.052632   0.000000     0.000000  0.187050   \n",
       "25             0.012955 -0.044147 -0.200000  -0.181818    -0.181818 -0.078788   \n",
       "26             0.007132  0.009797 -2.406250  -2.444444    -2.444444 -0.230263   \n",
       "27             0.005128  0.020097 -0.822222  -0.846154    -0.846154 -0.059829   \n",
       "28             0.005588  0.002038 -2.750000  -3.000000    -3.000000 -0.054545   \n",
       "29             0.014255  0.047458  0.357143   0.250000     0.250000  0.115385   \n",
       "30             0.005955  0.110032  1.315789   1.400000     1.400000  0.293103   \n",
       "31             0.025337  0.169679  0.431818   0.416667     0.416667  0.060000   \n",
       "32             0.036952 -0.048355 -0.269841  -0.235294    -0.294118  0.100629   \n",
       "33             0.071715  0.055003  0.326087   0.230769     0.333333  0.217143   \n",
       "34             0.054863  0.166832  0.737705   0.750000     0.562500  0.126761   \n",
       "35             0.140662  0.485106  1.783019   1.678571     1.800000  0.570833   \n",
       "36             0.102936 -0.070774 -0.016949  -0.520000    -0.514286  0.450928   \n",
       "37             0.068118  0.085415  0.103448   0.083333     0.088235  0.016453   \n",
       "38             0.094561  0.044886  0.343750   0.333333     0.351351  0.151079   \n",
       "39             0.122422  0.563078  0.313953   0.307692     0.300000  0.376563   \n",
       "40             0.036038 -0.241781 -0.274336  -0.279412    -0.276923  0.167991   \n",
       "41             0.074637  0.002524  0.151220   0.122449     0.148936 -0.103013   \n",
       "42             0.070096  0.106865  0.148305   0.163636     0.129630 -0.009751   \n",
       "43             0.124599  0.470953  0.852399   0.828125     0.868852  0.295405   \n",
       "44             0.092002 -0.260155 -0.233068  -0.239316    -0.236842  0.084459   \n",
       "45             0.093222  0.027736  0.062338   0.056180     0.057471  0.208723   \n",
       "46             0.084154  0.149168  0.105134   0.106383     0.086957  0.227448   \n",
       "47             0.156345  0.545440  0.748894   0.740385     0.760000  0.325459   \n",
       "48             0.074328 -0.218152 -0.339026  -0.342541    -0.340909 -0.106535   \n",
       "49             0.086911 -0.006390  0.025837   0.016807     0.025862  0.059840   \n",
       "50             0.071756  0.057744  0.059701   0.049587     0.050420 -0.099122   \n",
       "51             0.089349  0.287777  0.412852   0.425197     0.424000 -0.340297   \n",
       "52             0.061199 -0.197108 -0.249221  -0.254144    -0.252809 -0.054187   \n",
       "53             0.064868  0.021316  0.019917   0.022222     0.015038  0.323661   \n",
       "54             0.222188  0.947703  2.414158   2.376812     2.407407  0.297920   \n",
       "55             0.130468 -0.034179 -0.194948  -0.197425    -0.202174  0.206583   \n",
       "56             0.100089 -0.139259 -0.089994  -0.093583    -0.092643  0.082556   \n",
       "57             0.095634  0.163049  0.058230   0.053097     0.054054  0.132626   \n",
       "58             0.108557  0.295732  0.324316   0.316527     0.321937  0.106265   \n",
       "59             0.143856  0.314506  0.393686   0.389362     0.385776  0.135486   \n",
       "60             0.124593 -0.077559 -0.002831  -0.006126    -0.004666  0.142391   \n",
       "61             0.127950  0.158268  0.220645   0.215716     0.217187 -0.029988   \n",
       "62             0.104870 -0.010535 -0.093733  -0.096324    -0.093710  0.126814   \n",
       "63             0.175410  0.638946  0.972520   0.967742     0.964589  0.041060   \n",
       "64             0.138184 -0.154253 -0.110380  -0.112616    -0.113194  0.319649   \n",
       "65             0.090226 -0.106237 -0.240750  -0.243373    -0.242276  0.131640   \n",
       "66             0.057845  0.026925 -0.068110  -0.070064    -0.068670  0.093397   \n",
       "67             0.077286  0.515654  0.590417   0.590183     0.591014 -0.076197   \n",
       "68             0.063952 -0.200121 -0.269995  -0.270639    -0.269370 -0.173045   \n",
       "69            -0.089571 -0.189895 -0.277260  -0.260827    -0.259663 -0.126473   \n",
       "70             0.001581  0.060839  0.088696   0.106525     0.105756  0.075354   \n",
       "71             0.049656  0.536988  0.740149   0.755716     0.755448  0.147032   \n",
       "72            -0.073294 -0.207452 -0.217947  -0.198766    -0.198621  0.004935   \n",
       "73             0.006332 -0.179950 -0.242101  -0.889649    -0.889845  0.115609   \n",
       "74            -0.077667  0.125321  0.092798   0.100775     0.109375  0.164902   \n",
       "75             0.105615  0.770980  1.128735   1.169014     1.154930  0.097641   \n",
       "76             0.046040 -0.222376 -0.247170  -0.240260    -0.238562  0.108309   \n",
       "77            -0.025805 -0.144889 -0.213133  -0.205128    -0.206009  0.081521   \n",
       "78            -0.050304  0.038222  0.041866   0.064516     0.059459 -0.126844   \n",
       "79             0.074668  0.473214  0.650575   0.666667     0.673469  0.019648   \n",
       "80             0.017074 -0.333654 -0.427264  -0.421212    -0.420732 -0.120150   \n",
       "81            -0.030018 -0.162173 -0.258653  -0.251309    -0.252632  0.000198   \n",
       "82             0.013498  0.106096  0.156234   0.174825     0.183099  0.028832   \n",
       "83             0.032289  0.672309  0.984801   1.011905     1.000000  0.072612   \n",
       "84             0.012780 -0.324884 -0.383545  -0.375740    -0.375000  0.163853   \n",
       "85            -0.012358 -0.141561 -0.209629  -0.203791    -0.204762  0.144488   \n",
       "86             0.012248  0.157924  0.229093   0.238095     0.233533  0.035859   \n",
       "87             0.045894  0.679245  0.872783   0.884615     0.888350  0.072553   \n",
       "\n",
       "    Price high  Price low       ROE  P/B ratio  P/E ratio  \\\n",
       "0     0.078368   0.084413 -0.102530   0.033678   0.130674   \n",
       "1    -0.188976  -0.146341  0.308637   0.140625  -0.467548   \n",
       "2    -0.135922  -0.185714  0.143572  -0.136986  -0.878102   \n",
       "3     0.112360   0.333333  0.174508   0.182540   1.000000   \n",
       "4    -0.161616  -0.289474  0.071029  -0.174497   1.000000   \n",
       "5    -0.144578  -0.037037  0.160470   0.414634   1.000000   \n",
       "6     0.492958  -0.115385  0.403598   0.287356   1.000000   \n",
       "7    -0.169811   0.000000 -0.039648  -0.093750   1.000000   \n",
       "8     0.136364   0.000000 -0.872811   0.088670   1.000000   \n",
       "9     0.130000   0.913043 -1.345355   0.221719   1.000000   \n",
       "10    0.380531   0.136364  5.791139   0.214815   1.000000   \n",
       "11   -0.051282   0.020000  0.198509  -0.112805  -0.951423   \n",
       "12    0.141892   0.117647  0.063375  -0.024055  -0.131534   \n",
       "13    0.059172   0.043860  0.000731  -0.080986  -0.107143   \n",
       "14    0.597765   0.268908 -0.136646   0.272031   0.248148   \n",
       "15    0.472028   0.357616 -0.155311   0.367470   0.440950   \n",
       "16    0.275534   0.507317 -0.005010  -0.048458   0.322076   \n",
       "17   -0.072626  -0.071197 -0.079557  -0.011574  -0.170093   \n",
       "18   -0.080321  -0.369338  0.016958  -0.185012  -0.165541   \n",
       "19   -0.582969  -0.464088 -0.458311  -0.522989  -0.582996   \n",
       "20   -0.109948   0.061856 -0.449851   0.066265   0.901834   \n",
       "21    0.141176   0.300971 -0.629964   0.220339   1.325581   \n",
       "22   -0.072165  -0.216418 -1.321951  -0.162037   1.712683   \n",
       "23   -0.055556   0.019048 -9.136364  -0.038674  -0.326920   \n",
       "24    0.070588   0.383178 -0.031657   0.183908  -0.485707   \n",
       "25    0.021978  -0.202703 -0.151923  -0.087379  -0.063117   \n",
       "26   -0.279570  -0.152542 -0.634921  -0.234043  -0.109509   \n",
       "27   -0.082090  -0.030000 -0.708075  -0.055556   1.663761   \n",
       "28   -0.113821   0.020619 -1.361702  -0.066176   2.403460   \n",
       "29    0.266055  -0.050505  1.823529   0.125984  -0.399416   \n",
       "30    0.195652   0.446809 -4.458333   0.265734  -0.665047   \n",
       "31    0.072727   0.036765  1.000000   0.071823   1.000000   \n",
       "32    0.118644   0.070922  0.201807   0.072165  -0.449727   \n",
       "33    0.217172   0.218543  0.200501   0.221154   0.005584   \n",
       "34    0.124481   0.130435  0.231733   0.062992  -0.090699   \n",
       "35    0.804428   0.278846  0.708475   0.551852   0.279050   \n",
       "36    0.316973   0.695489  0.352183   0.293556  -0.174271   \n",
       "37   -0.029503   0.082040  0.232575  -0.075646  -0.301478   \n",
       "38    0.216000   0.065574  0.201190   0.091816  -0.130605   \n",
       "39    0.418421   0.315385  0.095144   0.268739   0.045491   \n",
       "40    0.144712   0.204678 -0.004072   0.051873  -0.020238   \n",
       "41   -0.145867  -0.038835  0.007269  -0.134247  -0.161632   \n",
       "42    0.054080  -0.094697 -0.013532  -0.066456  -0.087773   \n",
       "43    0.198020   0.446304  0.132144   0.208475   0.232613   \n",
       "44    0.049587   0.128255  0.052100  -0.032258  -0.107916   \n",
       "45    0.304939   0.094017  0.026488   0.108696   0.055573   \n",
       "46    0.214482   0.246094  0.016829   0.132026   0.095695   \n",
       "47    0.309395   0.349216  0.051122   0.229792   0.193788   \n",
       "48   -0.012073  -0.233736 -0.030791  -0.224413  -0.228152   \n",
       "49   -0.041201   0.234688 -0.037184  -0.012107  -0.003745   \n",
       "50   -0.058995  -0.153242 -0.039745  -0.164216  -0.146617   \n",
       "51   -0.338235  -0.343968 -0.070285  -0.384164  -0.371061   \n",
       "52   -0.081287  -0.012378 -0.040739  -0.128571  -0.057651   \n",
       "53    0.330999   0.312444 -0.037215   0.248634   0.283019   \n",
       "54    0.290770   0.309686  0.430196   0.223195   0.261586   \n",
       "55    0.108188   0.344271  0.082353  -0.007156  -0.239491   \n",
       "56    0.107991   0.053080  0.052291  -0.034234  -0.104505   \n",
       "57    0.202776   0.047093  0.035734   0.031716  -0.042531   \n",
       "58    0.051932   0.182361 -0.090027   0.014467  -0.051463   \n",
       "59    0.109707   0.167608  0.066055   0.028520   0.132496   \n",
       "60    0.120352   0.168236  0.049458   0.001733  -0.033787   \n",
       "61   -0.026856  -0.033551  0.071750  -0.133218  -0.171712   \n",
       "62    0.190814   0.053877 -0.022727   0.000000  -0.064272   \n",
       "63    0.009104   0.082567  0.121082  -0.053892  -0.049832   \n",
       "64    0.456365   0.154515  0.028861   0.124473   0.039688   \n",
       "65    0.036269   0.276741 -0.076485  -0.001876  -0.030675   \n",
       "66    0.094783   0.091555 -0.062411   0.005639   0.053446   \n",
       "67   -0.040111  -0.120717 -0.080739  -0.123364  -0.110147   \n",
       "68   -0.179872  -0.163966 -0.113436  -0.234542  -0.171793   \n",
       "69   -0.160802  -0.081022 -0.070497  -0.178273  -0.080616   \n",
       "70    0.102946   0.041992 -0.029068   0.142373   0.125123   \n",
       "71    0.119499   0.182310 -0.004474   0.133531   0.159370   \n",
       "72   -0.024099   0.040431  0.049430  -0.049738  -0.012085   \n",
       "73    0.185458   0.036023  0.027668   0.162534   0.074924   \n",
       "74    0.091426   0.260643  0.050000   0.149289   0.124467   \n",
       "75    0.154328   0.033554  0.140415   0.164948   0.056926   \n",
       "76    0.115658   0.099286  0.055675  -0.003540  -0.039497   \n",
       "77    0.007036   0.176527  0.051217   0.021314  -0.008100   \n",
       "78   -0.011669  -0.252640  0.035697  -0.113043  -0.184045   \n",
       "79   -0.068813   0.147500 -0.003493   0.050980  -0.040031   \n",
       "80   -0.116217  -0.124846 -0.059593  -0.186567  -0.141139   \n",
       "81    0.027049  -0.031605 -0.058400  -0.027523   0.048553   \n",
       "82    0.033722   0.022689 -0.060702   0.042453   0.080142   \n",
       "83    0.021604   0.137486 -0.018264   0.047511   0.108821   \n",
       "84    0.217457   0.102613  0.004293   0.110151   0.153160   \n",
       "85    0.084083   0.220460  0.008834   0.122568   0.117344   \n",
       "86    0.052921   0.016779  0.025141   0.038128   0.006347   \n",
       "87    0.074330   0.070571  0.029760   0.053422   0.028670   \n",
       "\n",
       "    Cumulative dividends per share  Dividend payout ratio  \\\n",
       "0                         0.189728               0.746084   \n",
       "1                         0.000000              -1.000000   \n",
       "2                         0.000000               1.000000   \n",
       "3                         0.000000               1.000000   \n",
       "4                         0.000000               1.000000   \n",
       "5                         0.000000               1.000000   \n",
       "6                         0.000000               1.000000   \n",
       "7                         0.000000               1.000000   \n",
       "8                         0.000000               1.000000   \n",
       "9                         0.000000               1.000000   \n",
       "10                        0.000000               1.000000   \n",
       "11                        0.000000               1.000000   \n",
       "12                        0.000000               1.000000   \n",
       "13                        0.000000               1.000000   \n",
       "14                        0.000000               1.000000   \n",
       "15                        0.000000               1.000000   \n",
       "16                        0.000000               1.000000   \n",
       "17                        0.000000               1.000000   \n",
       "18                        0.000000               1.000000   \n",
       "19                        0.000000               1.000000   \n",
       "20                        0.000000               1.000000   \n",
       "21                        0.000000               1.000000   \n",
       "22                        0.000000               1.000000   \n",
       "23                        0.000000               1.000000   \n",
       "24                        0.000000               1.000000   \n",
       "25                        0.000000               1.000000   \n",
       "26                        0.000000               1.000000   \n",
       "27                        0.000000               1.000000   \n",
       "28                        0.000000               1.000000   \n",
       "29                        0.000000               1.000000   \n",
       "30                        0.000000               1.000000   \n",
       "31                        0.000000               1.000000   \n",
       "32                        0.000000               1.000000   \n",
       "33                        0.000000               1.000000   \n",
       "34                        0.000000               1.000000   \n",
       "35                        0.000000               1.000000   \n",
       "36                        0.000000               1.000000   \n",
       "37                        0.000000               1.000000   \n",
       "38                        0.000000               1.000000   \n",
       "39                        0.000000               1.000000   \n",
       "40                        0.000000               1.000000   \n",
       "41                        0.000000               1.000000   \n",
       "42                        0.000000               1.000000   \n",
       "43                        0.000000               1.000000   \n",
       "44                        0.000000               1.000000   \n",
       "45                        0.000000               1.000000   \n",
       "46                        0.000000               1.000000   \n",
       "47                        0.000000               1.000000   \n",
       "48                        0.000000               1.000000   \n",
       "49                        0.000000               1.000000   \n",
       "50                        0.000000               1.000000   \n",
       "51                        0.000000               1.000000   \n",
       "52                        0.000000               1.000000   \n",
       "53                        0.000000               1.000000   \n",
       "54                        0.000000               1.000000   \n",
       "55                        0.000000               1.000000   \n",
       "56                        0.000000               1.000000   \n",
       "57                        0.000000               1.000000   \n",
       "58                        0.000000               1.000000   \n",
       "59                        0.000000               1.000000   \n",
       "60                        0.000000               1.000000   \n",
       "61                        0.000000               1.000000   \n",
       "62                        0.000000               1.000000   \n",
       "63                        0.000000               1.000000   \n",
       "64                        0.000000               1.000000   \n",
       "65                       12.666667               1.000000   \n",
       "66                        0.000000              -0.038772   \n",
       "67                        0.926829               1.001681   \n",
       "68                        0.481013               0.578505   \n",
       "69                        0.367521               0.091489   \n",
       "70                        0.275000               0.380604   \n",
       "71                        0.210784               0.022238   \n",
       "72                        0.178138              -0.005180   \n",
       "73                        0.161512              -0.018049   \n",
       "74                        0.142012              -0.018028   \n",
       "75                        0.121762              -0.109791   \n",
       "76                        0.108545              -0.063081   \n",
       "77                        0.108333              -0.044886   \n",
       "78                        0.097744              -0.042928   \n",
       "79                        0.089041               0.006610   \n",
       "80                        0.081761               0.073171   \n",
       "81                        0.082849               0.069930   \n",
       "82                        0.076510               0.058824   \n",
       "83                        0.071072               0.019676   \n",
       "84                        0.066356              -0.001135   \n",
       "85                        0.068777              -0.004924   \n",
       "86                        0.064351              -0.019794   \n",
       "87                        0.060461              -0.027184   \n",
       "\n",
       "    Long-term debt to equity ratio  Net margin  Asset turnover  \\\n",
       "0                         0.480173    0.202231       -0.010829   \n",
       "1                         2.187246   -0.999741       -0.005525   \n",
       "2                        -0.018519    1.000000       -0.027778   \n",
       "3                         0.060941    1.000000       -0.051429   \n",
       "4                         0.566844    1.000000       -0.030120   \n",
       "5                         0.037443    1.000000       -0.006211   \n",
       "6                        -0.003395    1.000000       -0.037500   \n",
       "7                        -0.034322    1.000000       -0.012987   \n",
       "8                        -0.102835    1.000000        0.000000   \n",
       "9                        -0.065977    1.000000       -0.032895   \n",
       "10                       -0.094028    6.428571       -0.013605   \n",
       "11                       -0.146127    0.311538       -0.006897   \n",
       "12                       -0.114896    0.168622       -0.034722   \n",
       "13                       -0.769984    0.176913       -0.028777   \n",
       "14                       -0.043564    0.044776       -0.074074   \n",
       "15                       -0.296066   -0.046939       -0.048000   \n",
       "16                        0.047059    0.087794       -0.025210   \n",
       "17                        0.008427   -0.039370       -0.034483   \n",
       "18                        0.016713    0.009221        0.008929   \n",
       "19                        0.147945   -0.376650       -0.123894   \n",
       "20                        0.011933   -0.421824       -0.040404   \n",
       "21                       -0.030660   -0.614085       -0.021053   \n",
       "22                       -0.015815    0.824818       -0.043011   \n",
       "23                       -0.018541    0.452000        0.056180   \n",
       "24                       -0.023929   -0.024793        0.010638   \n",
       "25                        0.002581   -0.135593       -0.021053   \n",
       "26                       -0.006435   -0.630719       -0.010753   \n",
       "27                        0.006477   -0.707965        0.010870   \n",
       "28                       -0.500000    0.340909       -0.010753   \n",
       "29                       -1.000000    0.254237        0.021739   \n",
       "30                        1.000000    1.000000        0.021277   \n",
       "31                        1.000000    0.873874        0.052083   \n",
       "32                        1.000000    0.153846        0.049505   \n",
       "33                        1.000000    0.166667        0.037736   \n",
       "34                        1.000000    0.189286        0.036364   \n",
       "35                        1.000000    0.561562        0.087719   \n",
       "36                        1.000000    0.303846        0.032258   \n",
       "37                        1.000000    0.182891        0.039063   \n",
       "38                        1.000000    0.194514        0.007519   \n",
       "39                        1.000000    0.034447        0.044776   \n",
       "40                        1.000000    0.006054       -0.014286   \n",
       "41                        1.000000    0.037111       -0.036232   \n",
       "42                        1.000000   -0.003868       -0.037594   \n",
       "43                        1.000000    0.139806       -0.015625   \n",
       "44                        1.000000    0.100511       -0.031746   \n",
       "45                        1.000000    0.071981       -0.032787   \n",
       "46                        1.000000    0.051264       -0.042373   \n",
       "47                        1.000000    0.055632       -0.017699   \n",
       "48                        1.000000   -0.015615       -0.036036   \n",
       "49                        1.000000   -0.012558       -0.018692   \n",
       "50                        1.000000   -0.004016       -0.057143   \n",
       "51                        1.000000   -0.012097       -0.080808   \n",
       "52                        1.000000    0.013605       -0.054945   \n",
       "53                        1.000000    0.004698       -0.069767   \n",
       "54                        1.000000    0.281897        0.175000   \n",
       "55                        1.000000    0.077124        0.063830   \n",
       "56                        1.000000    0.068699        0.040000   \n",
       "57                        1.000000    0.029425        0.057692   \n",
       "58                        1.000000   -0.055409       -0.054545   \n",
       "59                        1.000000    0.015363        0.038462   \n",
       "60                        1.000000    0.025218        0.009259   \n",
       "61                        1.000000    0.052326        0.009174   \n",
       "62                        1.000000    0.017850       -0.027273   \n",
       "63                        1.000000    0.077244        0.046729   \n",
       "64                        1.000000    0.051550       -0.008929   \n",
       "65                        1.000000   -0.005898       -0.054054   \n",
       "66                        1.000000   -0.011123       -0.047619   \n",
       "67                        1.000000   -0.049494       -0.040000   \n",
       "68                        1.000000   -0.074556       -0.031250   \n",
       "69                        1.000000   -0.050298       -0.053763   \n",
       "70                       -0.001455   -0.027379       -0.022727   \n",
       "71                       -0.047342   -0.017997       -0.023256   \n",
       "72                        0.078746    0.006579        0.000000   \n",
       "73                        0.700921    0.010271       -0.011905   \n",
       "74                        0.082917   -0.001386        0.000000   \n",
       "75                        0.014236    0.029616        0.048193   \n",
       "76                        0.178300    0.012584        0.000000   \n",
       "77                        0.214746    0.003995        0.000000   \n",
       "78                        0.187119    0.010168       -0.011494   \n",
       "79                       -0.073900    0.000875       -0.023256   \n",
       "80                        0.282064   -0.026235       -0.071429   \n",
       "81                        0.024445   -0.025595       -0.051282   \n",
       "82                        0.079479   -0.023502       -0.054054   \n",
       "83                       -0.055263   -0.021708       -0.014286   \n",
       "84                        0.134629    0.000482       -0.014493   \n",
       "85                        0.076459    0.006268       -0.014706   \n",
       "86                        0.068671    0.010541       -0.014925   \n",
       "87                        0.022063    0.001422       -0.015152   \n",
       "\n",
       "    Free cash flow per share ticker  Relative Return DJIA  \n",
       "0                  -0.004580   AAPL              7.213807  \n",
       "1                  -1.727273   AAPL             -0.244886  \n",
       "2                   0.375000   AAPL              2.023195  \n",
       "3                  -0.818182   AAPL              1.255867  \n",
       "4                   0.000000   AAPL             -0.754446  \n",
       "5                  -4.000000   AAPL              6.204432  \n",
       "6                  -1.833333   AAPL             -0.801632  \n",
       "7                  -0.200000   AAPL             -1.105104  \n",
       "8                   0.000000   AAPL            -33.194949  \n",
       "9                   0.000000   AAPL             -0.878415  \n",
       "10                  1.000000   AAPL            -10.319328  \n",
       "11                 -0.250000   AAPL             -2.348911  \n",
       "12                  0.166667   AAPL             -0.622657  \n",
       "13                 -0.714286   AAPL              0.869012  \n",
       "14                  1.000000   AAPL             -1.518201  \n",
       "15                  0.750000   AAPL             -2.727804  \n",
       "16                 -0.857143   AAPL             -1.496042  \n",
       "17                  4.000000   AAPL             -0.202670  \n",
       "18                 -0.400000   AAPL             -1.498686  \n",
       "19                 -1.333333   AAPL             -0.142219  \n",
       "20                 -0.000000   AAPL             -5.579248  \n",
       "21                 -0.000000   AAPL             -1.730243  \n",
       "22                 -3.000000   AAPL             -3.559217  \n",
       "23                 -0.750000   AAPL             -1.850772  \n",
       "24                 -3.000000   AAPL             -0.727060  \n",
       "25                 -0.000000   AAPL             -4.052336  \n",
       "26                 -1.500000   AAPL              0.591982  \n",
       "27                  3.000000   AAPL             -1.563357  \n",
       "28                 -0.500000   AAPL             -1.416243  \n",
       "29                 -2.000000   AAPL             -3.977599  \n",
       "30                 -2.000000   AAPL             -0.762035  \n",
       "31                  2.000000   AAPL              3.317242  \n",
       "32                  0.000000   AAPL             -1.080715  \n",
       "33                  0.000000   AAPL             -1.522792  \n",
       "34                  1.333333   AAPL             -7.614191  \n",
       "35                  0.857143   AAPL             -2.812880  \n",
       "36                 -0.307692   AAPL             -1.475057  \n",
       "37                 -0.222222   AAPL             -0.278238  \n",
       "38                  0.571429   AAPL             -2.233748  \n",
       "39                 -0.727273   AAPL             -0.619086  \n",
       "40                 -2.666667   AAPL              2.381817  \n",
       "41                 -3.600000   AAPL             -0.864912  \n",
       "42                  0.153846   AAPL              9.086989  \n",
       "43                  0.866667   AAPL              0.350419  \n",
       "44                 -0.642857   AAPL             -1.149176  \n",
       "45                  0.600000   AAPL             -9.694990  \n",
       "46                  0.500000   AAPL             -0.591031  \n",
       "47                  0.750000   AAPL             -2.428967  \n",
       "48                 -0.595238   AAPL              0.530651  \n",
       "49                 -0.058824   AAPL              0.007699  \n",
       "50                  2.937500   AAPL             -0.426921  \n",
       "51                 -0.079365   AAPL              3.366126  \n",
       "52                 -0.793103   AAPL             -0.294416  \n",
       "53                  1.583333   AAPL             -1.806961  \n",
       "54                  0.354839   AAPL              0.373251  \n",
       "55                  1.023810   AAPL             -0.512084  \n",
       "56                 -0.623529   AAPL             -0.437773  \n",
       "57                  1.031250   AAPL             -3.508989  \n",
       "58                  0.169231   AAPL             -2.016228  \n",
       "59                  0.736842   AAPL             -0.300537  \n",
       "60                 -0.348485   AAPL             -0.127247  \n",
       "61                  0.813953   AAPL             -0.872706  \n",
       "62                 -0.423077   AAPL            -16.311055  \n",
       "63                  1.744444   AAPL             -1.974780  \n",
       "64                 -0.230769   AAPL             -0.343357  \n",
       "65                 -0.421053   AAPL             -1.338118  \n",
       "66                 -0.218182   AAPL             -2.600312  \n",
       "67                  2.709302   AAPL             -1.567644  \n",
       "68                 -0.514107   AAPL             -5.756350  \n",
       "69                 -0.412903   AAPL             -0.790157  \n",
       "70                  0.329670   AAPL             -0.415739  \n",
       "71                  1.727273   AAPL              5.723109  \n",
       "72                 -0.393939   AAPL             -1.076757  \n",
       "73                 -0.345000   AAPL             -3.941488  \n",
       "74                  0.221374   AAPL             -0.472960  \n",
       "75                  2.268750   AAPL              2.998029  \n",
       "76                 -0.449331   AAPL             -1.082975  \n",
       "77                 -0.215278   AAPL              1.589325  \n",
       "78                 -0.221239   AAPL              6.738844  \n",
       "79                  1.403409   AAPL             -1.937428  \n",
       "80                 -0.612293   AAPL             -0.769061  \n",
       "81                 -0.128049   AAPL             -0.141397  \n",
       "82                  0.587413   AAPL              0.502444  \n",
       "83                  0.986784   AAPL              2.783981  \n",
       "84                 -0.596452   AAPL             -0.441879  \n",
       "85                 -0.362637   AAPL             -1.025327  \n",
       "86                  0.956897   AAPL             -1.122030  \n",
       "87                  1.198238   AAPL            754.331132  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_data_trend_standardized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# looking at trend standardized columns\n",
    "fig = plt.figure(figsize=(40,5))\n",
    "sb.lineplot(data=financial_data_trend_standardized[0], x=\"Quarter end\", y='Relative Return DJIA')\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into 60-20-20\n",
    "\n",
    "- 60% training\n",
    "- 20% validation\n",
    "- 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data_trend_standardized = load_file(file_path=folder_path, file_name='financial_data_trend_standardized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter end</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Current Assets</th>\n",
       "      <th>Liabilities</th>\n",
       "      <th>Current Liabilities</th>\n",
       "      <th>Shareholders equity</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>EPS basic</th>\n",
       "      <th>EPS diluted</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price high</th>\n",
       "      <th>Price low</th>\n",
       "      <th>ROE</th>\n",
       "      <th>P/B ratio</th>\n",
       "      <th>P/E ratio</th>\n",
       "      <th>Cumulative dividends per share</th>\n",
       "      <th>Dividend payout ratio</th>\n",
       "      <th>Long-term debt to equity ratio</th>\n",
       "      <th>Net margin</th>\n",
       "      <th>Asset turnover</th>\n",
       "      <th>Free cash flow per share</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Relative Return DJIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-03-29</td>\n",
       "      <td>0.054530</td>\n",
       "      <td>0.045478</td>\n",
       "      <td>0.061899</td>\n",
       "      <td>0.058088</td>\n",
       "      <td>0.053942</td>\n",
       "      <td>0.075105</td>\n",
       "      <td>0.022818</td>\n",
       "      <td>-0.001147</td>\n",
       "      <td>0.099883</td>\n",
       "      <td>0.077414</td>\n",
       "      <td>0.078368</td>\n",
       "      <td>0.084413</td>\n",
       "      <td>-0.102530</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.130674</td>\n",
       "      <td>0.189728</td>\n",
       "      <td>0.746084</td>\n",
       "      <td>0.480173</td>\n",
       "      <td>0.202231</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>-0.004580</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.099229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-06-28</td>\n",
       "      <td>0.021207</td>\n",
       "      <td>0.041384</td>\n",
       "      <td>0.046256</td>\n",
       "      <td>-0.152662</td>\n",
       "      <td>-0.017510</td>\n",
       "      <td>-0.002746</td>\n",
       "      <td>-0.956757</td>\n",
       "      <td>-0.956594</td>\n",
       "      <td>-0.997349</td>\n",
       "      <td>-0.173077</td>\n",
       "      <td>-0.188976</td>\n",
       "      <td>-0.146341</td>\n",
       "      <td>0.308637</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.467548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.187246</td>\n",
       "      <td>-0.999741</td>\n",
       "      <td>-0.005525</td>\n",
       "      <td>-1.727273</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.299486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-09-27</td>\n",
       "      <td>0.003555</td>\n",
       "      <td>0.013696</td>\n",
       "      <td>-0.005714</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>0.018812</td>\n",
       "      <td>0.065168</td>\n",
       "      <td>-1.781250</td>\n",
       "      <td>-1.807692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.151163</td>\n",
       "      <td>-0.135922</td>\n",
       "      <td>-0.185714</td>\n",
       "      <td>0.143572</td>\n",
       "      <td>-0.136986</td>\n",
       "      <td>-0.878102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018519</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.027778</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.642616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-12-27</td>\n",
       "      <td>-0.017151</td>\n",
       "      <td>-0.021262</td>\n",
       "      <td>0.007260</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>-0.056365</td>\n",
       "      <td>-0.082723</td>\n",
       "      <td>-5.800000</td>\n",
       "      <td>-5.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.205479</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.174508</td>\n",
       "      <td>0.182540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.051429</td>\n",
       "      <td>-0.818182</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.417069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-03-28</td>\n",
       "      <td>-0.148900</td>\n",
       "      <td>-0.175832</td>\n",
       "      <td>-0.025526</td>\n",
       "      <td>-0.016145</td>\n",
       "      <td>-0.360453</td>\n",
       "      <td>-0.248004</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.215909</td>\n",
       "      <td>-0.161616</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>0.071029</td>\n",
       "      <td>-0.174497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.566844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.795442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997-06-27</td>\n",
       "      <td>-0.032538</td>\n",
       "      <td>-0.040912</td>\n",
       "      <td>-0.030817</td>\n",
       "      <td>-0.050224</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>0.084947</td>\n",
       "      <td>-0.920904</td>\n",
       "      <td>-0.921986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.115942</td>\n",
       "      <td>-0.144578</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>0.160470</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037443</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006211</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.661655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997-09-26</td>\n",
       "      <td>-0.024879</td>\n",
       "      <td>-0.019754</td>\n",
       "      <td>-0.035612</td>\n",
       "      <td>-0.048168</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>-0.070812</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>1.863636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>-0.115385</td>\n",
       "      <td>0.403598</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.037500</td>\n",
       "      <td>-1.833333</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.738995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1997-12-26</td>\n",
       "      <td>-0.025278</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>-0.049786</td>\n",
       "      <td>-0.081958</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>-0.022305</td>\n",
       "      <td>-1.291925</td>\n",
       "      <td>-1.293651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.118421</td>\n",
       "      <td>-0.169811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.039648</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.159558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1998-03-27</td>\n",
       "      <td>-0.039506</td>\n",
       "      <td>-0.047436</td>\n",
       "      <td>-0.106523</td>\n",
       "      <td>-0.170761</td>\n",
       "      <td>0.115756</td>\n",
       "      <td>-0.109632</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.872811</td>\n",
       "      <td>0.088670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.102835</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-18.429915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1998-06-26</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>-0.007767</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.070605</td>\n",
       "      <td>-0.002135</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>-1.345355</td>\n",
       "      <td>0.221719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.065977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.032895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.797464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1998-09-25</td>\n",
       "      <td>0.061371</td>\n",
       "      <td>0.095704</td>\n",
       "      <td>0.036008</td>\n",
       "      <td>0.094312</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>0.109843</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.380531</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>5.791139</td>\n",
       "      <td>0.214815</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.094028</td>\n",
       "      <td>6.428571</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-7.697962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1998-12-26</td>\n",
       "      <td>0.070646</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.008311</td>\n",
       "      <td>-0.023684</td>\n",
       "      <td>0.171133</td>\n",
       "      <td>0.098972</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.198509</td>\n",
       "      <td>-0.112805</td>\n",
       "      <td>-0.951423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.146127</td>\n",
       "      <td>0.311538</td>\n",
       "      <td>-0.006897</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.049943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1999-03-27</td>\n",
       "      <td>0.074695</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.034095</td>\n",
       "      <td>0.040431</td>\n",
       "      <td>0.131045</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>-0.111842</td>\n",
       "      <td>-0.116071</td>\n",
       "      <td>-0.115789</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.141892</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>-0.024055</td>\n",
       "      <td>-0.131534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.114896</td>\n",
       "      <td>0.168622</td>\n",
       "      <td>-0.034722</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.566196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999-06-26</td>\n",
       "      <td>0.017021</td>\n",
       "      <td>0.050342</td>\n",
       "      <td>-0.257609</td>\n",
       "      <td>-0.003238</td>\n",
       "      <td>0.365517</td>\n",
       "      <td>0.018301</td>\n",
       "      <td>0.503704</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.049296</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>-0.080986</td>\n",
       "      <td>-0.107143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.769984</td>\n",
       "      <td>0.176913</td>\n",
       "      <td>-0.028777</td>\n",
       "      <td>-0.714286</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.717602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1999-09-25</td>\n",
       "      <td>0.028292</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>0.045118</td>\n",
       "      <td>-0.142490</td>\n",
       "      <td>-0.453202</td>\n",
       "      <td>-0.496454</td>\n",
       "      <td>-0.483333</td>\n",
       "      <td>0.469799</td>\n",
       "      <td>0.597765</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>-0.136646</td>\n",
       "      <td>0.272031</td>\n",
       "      <td>0.248148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043564</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.521976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>0.469870</td>\n",
       "      <td>0.145624</td>\n",
       "      <td>0.542052</td>\n",
       "      <td>0.268560</td>\n",
       "      <td>0.422036</td>\n",
       "      <td>0.753743</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.605634</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.429224</td>\n",
       "      <td>0.472028</td>\n",
       "      <td>0.357616</td>\n",
       "      <td>-0.155311</td>\n",
       "      <td>0.367470</td>\n",
       "      <td>0.440950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.296066</td>\n",
       "      <td>-0.046939</td>\n",
       "      <td>-0.048000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.858005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-04-01</td>\n",
       "      <td>-0.076325</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>-0.119798</td>\n",
       "      <td>-0.056997</td>\n",
       "      <td>-0.045084</td>\n",
       "      <td>-0.169868</td>\n",
       "      <td>0.273224</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.242718</td>\n",
       "      <td>0.351438</td>\n",
       "      <td>0.275534</td>\n",
       "      <td>0.507317</td>\n",
       "      <td>-0.005010</td>\n",
       "      <td>-0.048458</td>\n",
       "      <td>0.322076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.087794</td>\n",
       "      <td>-0.025210</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.467200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-07-01</td>\n",
       "      <td>-0.010704</td>\n",
       "      <td>0.049878</td>\n",
       "      <td>-0.012894</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>-0.009253</td>\n",
       "      <td>-0.061697</td>\n",
       "      <td>-0.141631</td>\n",
       "      <td>-0.569444</td>\n",
       "      <td>-0.570312</td>\n",
       "      <td>-0.070922</td>\n",
       "      <td>-0.072626</td>\n",
       "      <td>-0.071197</td>\n",
       "      <td>-0.079557</td>\n",
       "      <td>-0.011574</td>\n",
       "      <td>-0.170093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>-0.039370</td>\n",
       "      <td>-0.034483</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.062467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-09-30</td>\n",
       "      <td>-0.018609</td>\n",
       "      <td>0.052356</td>\n",
       "      <td>-0.021771</td>\n",
       "      <td>0.032034</td>\n",
       "      <td>-0.016523</td>\n",
       "      <td>0.024658</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>-0.177419</td>\n",
       "      <td>-0.145455</td>\n",
       "      <td>-0.185751</td>\n",
       "      <td>-0.080321</td>\n",
       "      <td>-0.369338</td>\n",
       "      <td>0.016958</td>\n",
       "      <td>-0.185012</td>\n",
       "      <td>-0.165541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016713</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.373365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-12-30</td>\n",
       "      <td>-0.120094</td>\n",
       "      <td>-0.092316</td>\n",
       "      <td>-0.156528</td>\n",
       "      <td>-0.153130</td>\n",
       "      <td>-0.096177</td>\n",
       "      <td>-0.461497</td>\n",
       "      <td>-2.147059</td>\n",
       "      <td>-2.137255</td>\n",
       "      <td>-2.234043</td>\n",
       "      <td>-0.550000</td>\n",
       "      <td>-0.582969</td>\n",
       "      <td>-0.464088</td>\n",
       "      <td>-0.458311</td>\n",
       "      <td>-0.522989</td>\n",
       "      <td>-0.582996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147945</td>\n",
       "      <td>-0.376650</td>\n",
       "      <td>-0.123894</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.587237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>0.024056</td>\n",
       "      <td>0.085059</td>\n",
       "      <td>0.051891</td>\n",
       "      <td>0.096518</td>\n",
       "      <td>0.007004</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>-1.220513</td>\n",
       "      <td>-1.206897</td>\n",
       "      <td>-1.206897</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>-0.109948</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>-0.449851</td>\n",
       "      <td>0.066265</td>\n",
       "      <td>0.901834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>-0.421824</td>\n",
       "      <td>-0.040404</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-14.022237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001-06-30</td>\n",
       "      <td>-0.009625</td>\n",
       "      <td>-0.018148</td>\n",
       "      <td>-0.074833</td>\n",
       "      <td>-0.100836</td>\n",
       "      <td>0.032103</td>\n",
       "      <td>0.030748</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>-0.629964</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>1.325581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030660</td>\n",
       "      <td>-0.614085</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.664357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2001-09-29</td>\n",
       "      <td>-0.008236</td>\n",
       "      <td>-0.020008</td>\n",
       "      <td>-0.050610</td>\n",
       "      <td>-0.059480</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>0.081967</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>-0.128049</td>\n",
       "      <td>-0.072165</td>\n",
       "      <td>-0.216418</td>\n",
       "      <td>-1.321951</td>\n",
       "      <td>-0.162037</td>\n",
       "      <td>1.712683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015815</td>\n",
       "      <td>0.824818</td>\n",
       "      <td>-0.043011</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.065216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2001-12-29</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>0.025083</td>\n",
       "      <td>0.026178</td>\n",
       "      <td>0.040184</td>\n",
       "      <td>0.011735</td>\n",
       "      <td>-0.051724</td>\n",
       "      <td>-0.424242</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.450000</td>\n",
       "      <td>-0.027972</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>-9.136364</td>\n",
       "      <td>-0.038674</td>\n",
       "      <td>-0.326920</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018541</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.620554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2002-03-30</td>\n",
       "      <td>0.023195</td>\n",
       "      <td>0.026935</td>\n",
       "      <td>0.043599</td>\n",
       "      <td>0.082331</td>\n",
       "      <td>0.012103</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187050</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.383178</td>\n",
       "      <td>-0.031657</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>-0.485707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023929</td>\n",
       "      <td>-0.024793</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.669443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2002-06-29</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.005356</td>\n",
       "      <td>-0.010222</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>-0.044147</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>-0.078788</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>-0.202703</td>\n",
       "      <td>-0.151923</td>\n",
       "      <td>-0.087379</td>\n",
       "      <td>-0.063117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002581</td>\n",
       "      <td>-0.135593</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.271018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2002-09-28</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>-0.010105</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>-0.034924</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.009797</td>\n",
       "      <td>-2.406250</td>\n",
       "      <td>-2.444444</td>\n",
       "      <td>-2.444444</td>\n",
       "      <td>-0.230263</td>\n",
       "      <td>-0.279570</td>\n",
       "      <td>-0.152542</td>\n",
       "      <td>-0.634921</td>\n",
       "      <td>-0.234043</td>\n",
       "      <td>-0.109509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006435</td>\n",
       "      <td>-0.630719</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.739608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2002-12-28</td>\n",
       "      <td>-0.004605</td>\n",
       "      <td>-0.000742</td>\n",
       "      <td>-0.022696</td>\n",
       "      <td>-0.037998</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.020097</td>\n",
       "      <td>-0.822222</td>\n",
       "      <td>-0.846154</td>\n",
       "      <td>-0.846154</td>\n",
       "      <td>-0.059829</td>\n",
       "      <td>-0.082090</td>\n",
       "      <td>-0.030000</td>\n",
       "      <td>-0.708075</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>1.663761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>-0.707965</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.406145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2003-03-29</td>\n",
       "      <td>0.014675</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>0.258307</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>-0.113821</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>-1.361702</td>\n",
       "      <td>-0.066176</td>\n",
       "      <td>2.403460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.495944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2003-06-28</td>\n",
       "      <td>0.012577</td>\n",
       "      <td>0.014265</td>\n",
       "      <td>0.009451</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.266055</td>\n",
       "      <td>-0.050505</td>\n",
       "      <td>1.823529</td>\n",
       "      <td>0.125984</td>\n",
       "      <td>-0.399416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.522823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2003-09-27</td>\n",
       "      <td>0.058066</td>\n",
       "      <td>0.061486</td>\n",
       "      <td>0.155595</td>\n",
       "      <td>0.163951</td>\n",
       "      <td>0.005955</td>\n",
       "      <td>0.110032</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>-4.458333</td>\n",
       "      <td>0.265734</td>\n",
       "      <td>-0.665047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.694212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2003-12-27</td>\n",
       "      <td>0.022891</td>\n",
       "      <td>0.024121</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.025337</td>\n",
       "      <td>0.169679</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.873874</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.319034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2004-03-27</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.040637</td>\n",
       "      <td>-0.149943</td>\n",
       "      <td>-0.168766</td>\n",
       "      <td>0.036952</td>\n",
       "      <td>-0.048355</td>\n",
       "      <td>-0.269841</td>\n",
       "      <td>-0.235294</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td>0.100629</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.201807</td>\n",
       "      <td>0.072165</td>\n",
       "      <td>-0.449727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.073067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2004-06-26</td>\n",
       "      <td>0.072457</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>0.073942</td>\n",
       "      <td>0.088384</td>\n",
       "      <td>0.071715</td>\n",
       "      <td>0.055003</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.217143</td>\n",
       "      <td>0.217172</td>\n",
       "      <td>0.218543</td>\n",
       "      <td>0.200501</td>\n",
       "      <td>0.221154</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.161942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2004-09-25</td>\n",
       "      <td>0.114495</td>\n",
       "      <td>0.122335</td>\n",
       "      <td>0.233513</td>\n",
       "      <td>0.243619</td>\n",
       "      <td>0.054863</td>\n",
       "      <td>0.166832</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.126761</td>\n",
       "      <td>0.124481</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.231733</td>\n",
       "      <td>0.062992</td>\n",
       "      <td>-0.090699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.189286</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.529463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2004-12-25</td>\n",
       "      <td>0.162981</td>\n",
       "      <td>0.179589</td>\n",
       "      <td>0.201076</td>\n",
       "      <td>0.202985</td>\n",
       "      <td>0.140662</td>\n",
       "      <td>0.485106</td>\n",
       "      <td>1.783019</td>\n",
       "      <td>1.678571</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.570833</td>\n",
       "      <td>0.804428</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.708475</td>\n",
       "      <td>0.551852</td>\n",
       "      <td>0.279050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.561562</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.086496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2005-03-26</td>\n",
       "      <td>0.080004</td>\n",
       "      <td>0.082312</td>\n",
       "      <td>0.042833</td>\n",
       "      <td>0.039702</td>\n",
       "      <td>0.102936</td>\n",
       "      <td>-0.070774</td>\n",
       "      <td>-0.016949</td>\n",
       "      <td>-0.520000</td>\n",
       "      <td>-0.514286</td>\n",
       "      <td>0.450928</td>\n",
       "      <td>0.316973</td>\n",
       "      <td>0.695489</td>\n",
       "      <td>0.352183</td>\n",
       "      <td>0.293556</td>\n",
       "      <td>-0.174271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.303846</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>-0.307692</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.311465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2005-06-25</td>\n",
       "      <td>0.037286</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>-0.015570</td>\n",
       "      <td>-0.068317</td>\n",
       "      <td>0.068118</td>\n",
       "      <td>0.085415</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.016453</td>\n",
       "      <td>-0.029503</td>\n",
       "      <td>0.082040</td>\n",
       "      <td>0.232575</td>\n",
       "      <td>-0.075646</td>\n",
       "      <td>-0.301478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182891</td>\n",
       "      <td>0.039063</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2005-09-24</td>\n",
       "      <td>0.101354</td>\n",
       "      <td>0.098549</td>\n",
       "      <td>0.113990</td>\n",
       "      <td>0.115594</td>\n",
       "      <td>0.094561</td>\n",
       "      <td>0.044886</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.151079</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.201190</td>\n",
       "      <td>0.091816</td>\n",
       "      <td>-0.130605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194514</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.325806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>0.227686</td>\n",
       "      <td>0.180777</td>\n",
       "      <td>0.420073</td>\n",
       "      <td>0.452354</td>\n",
       "      <td>0.122422</td>\n",
       "      <td>0.563078</td>\n",
       "      <td>0.313953</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.376563</td>\n",
       "      <td>0.418421</td>\n",
       "      <td>0.315385</td>\n",
       "      <td>0.095144</td>\n",
       "      <td>0.268739</td>\n",
       "      <td>0.045491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034447</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>-0.727273</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.397696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2006-04-01</td>\n",
       "      <td>-0.019040</td>\n",
       "      <td>-0.072028</td>\n",
       "      <td>-0.098604</td>\n",
       "      <td>-0.119368</td>\n",
       "      <td>0.036038</td>\n",
       "      <td>-0.241781</td>\n",
       "      <td>-0.274336</td>\n",
       "      <td>-0.279412</td>\n",
       "      <td>-0.276923</td>\n",
       "      <td>0.167991</td>\n",
       "      <td>0.144712</td>\n",
       "      <td>0.204678</td>\n",
       "      <td>-0.004072</td>\n",
       "      <td>0.051873</td>\n",
       "      <td>-0.020238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>-2.666667</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.093514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2006-07-01</td>\n",
       "      <td>0.086478</td>\n",
       "      <td>0.106769</td>\n",
       "      <td>0.106139</td>\n",
       "      <td>0.127244</td>\n",
       "      <td>0.074637</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.151220</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>-0.103013</td>\n",
       "      <td>-0.145867</td>\n",
       "      <td>-0.038835</td>\n",
       "      <td>0.007269</td>\n",
       "      <td>-0.134247</td>\n",
       "      <td>-0.161632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037111</td>\n",
       "      <td>-0.036232</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.928606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2006-09-30</td>\n",
       "      <td>0.138349</td>\n",
       "      <td>0.161556</td>\n",
       "      <td>0.248444</td>\n",
       "      <td>0.288274</td>\n",
       "      <td>0.070096</td>\n",
       "      <td>0.106865</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>-0.009751</td>\n",
       "      <td>0.054080</td>\n",
       "      <td>-0.094697</td>\n",
       "      <td>-0.013532</td>\n",
       "      <td>-0.066456</td>\n",
       "      <td>-0.087773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003868</td>\n",
       "      <td>-0.037594</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>16.128502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2006-12-30</td>\n",
       "      <td>0.131125</td>\n",
       "      <td>0.148528</td>\n",
       "      <td>0.140147</td>\n",
       "      <td>0.133828</td>\n",
       "      <td>0.124599</td>\n",
       "      <td>0.470953</td>\n",
       "      <td>0.852399</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.295405</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.446304</td>\n",
       "      <td>0.132144</td>\n",
       "      <td>0.208475</td>\n",
       "      <td>0.232613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139806</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.457715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-03-31</td>\n",
       "      <td>-0.038539</td>\n",
       "      <td>-0.038106</td>\n",
       "      <td>-0.216567</td>\n",
       "      <td>-0.252419</td>\n",
       "      <td>0.092002</td>\n",
       "      <td>-0.260155</td>\n",
       "      <td>-0.233068</td>\n",
       "      <td>-0.239316</td>\n",
       "      <td>-0.236842</td>\n",
       "      <td>0.084459</td>\n",
       "      <td>0.049587</td>\n",
       "      <td>0.128255</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>-0.032258</td>\n",
       "      <td>-0.107916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100511</td>\n",
       "      <td>-0.031746</td>\n",
       "      <td>-0.642857</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.120882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2007-06-30</td>\n",
       "      <td>0.156913</td>\n",
       "      <td>0.169443</td>\n",
       "      <td>0.277984</td>\n",
       "      <td>0.274749</td>\n",
       "      <td>0.093222</td>\n",
       "      <td>0.027736</td>\n",
       "      <td>0.062338</td>\n",
       "      <td>0.056180</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.208723</td>\n",
       "      <td>0.304939</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.026488</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.055573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071981</td>\n",
       "      <td>-0.032787</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-11.137316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2007-09-29</td>\n",
       "      <td>0.170924</td>\n",
       "      <td>0.171299</td>\n",
       "      <td>0.312022</td>\n",
       "      <td>0.329949</td>\n",
       "      <td>0.084154</td>\n",
       "      <td>0.149168</td>\n",
       "      <td>0.105134</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.227448</td>\n",
       "      <td>0.214482</td>\n",
       "      <td>0.246094</td>\n",
       "      <td>0.016829</td>\n",
       "      <td>0.132026</td>\n",
       "      <td>0.095695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051264</td>\n",
       "      <td>-0.042373</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.537597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2007-12-29</td>\n",
       "      <td>0.185111</td>\n",
       "      <td>0.192795</td>\n",
       "      <td>0.223763</td>\n",
       "      <td>0.132918</td>\n",
       "      <td>0.156345</td>\n",
       "      <td>0.545440</td>\n",
       "      <td>0.748894</td>\n",
       "      <td>0.740385</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.325459</td>\n",
       "      <td>0.309395</td>\n",
       "      <td>0.349216</td>\n",
       "      <td>0.051122</td>\n",
       "      <td>0.229792</td>\n",
       "      <td>0.193788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055632</td>\n",
       "      <td>-0.017699</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.186975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008-03-29</td>\n",
       "      <td>0.014381</td>\n",
       "      <td>0.020887</td>\n",
       "      <td>-0.061730</td>\n",
       "      <td>-0.085524</td>\n",
       "      <td>0.074328</td>\n",
       "      <td>-0.218152</td>\n",
       "      <td>-0.339026</td>\n",
       "      <td>-0.342541</td>\n",
       "      <td>-0.340909</td>\n",
       "      <td>-0.106535</td>\n",
       "      <td>-0.012073</td>\n",
       "      <td>-0.233736</td>\n",
       "      <td>-0.030791</td>\n",
       "      <td>-0.224413</td>\n",
       "      <td>-0.228152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015615</td>\n",
       "      <td>-0.036036</td>\n",
       "      <td>-0.595238</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.868366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2008-06-28</td>\n",
       "      <td>0.040629</td>\n",
       "      <td>0.047202</td>\n",
       "      <td>-0.026655</td>\n",
       "      <td>-0.043180</td>\n",
       "      <td>0.086911</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>0.025837</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.059840</td>\n",
       "      <td>-0.041201</td>\n",
       "      <td>0.234688</td>\n",
       "      <td>-0.037184</td>\n",
       "      <td>-0.012107</td>\n",
       "      <td>-0.003745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012558</td>\n",
       "      <td>-0.018692</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.035497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2008-09-27</td>\n",
       "      <td>0.247974</td>\n",
       "      <td>0.239017</td>\n",
       "      <td>0.534045</td>\n",
       "      <td>0.528748</td>\n",
       "      <td>0.071756</td>\n",
       "      <td>0.057744</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.049587</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>-0.099122</td>\n",
       "      <td>-0.058995</td>\n",
       "      <td>-0.153242</td>\n",
       "      <td>-0.039745</td>\n",
       "      <td>-0.164216</td>\n",
       "      <td>-0.146617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004016</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.411116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2008-12-27</td>\n",
       "      <td>0.081244</td>\n",
       "      <td>0.013635</td>\n",
       "      <td>0.072053</td>\n",
       "      <td>0.047190</td>\n",
       "      <td>0.089349</td>\n",
       "      <td>0.287777</td>\n",
       "      <td>0.412852</td>\n",
       "      <td>0.425197</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>-0.340297</td>\n",
       "      <td>-0.338235</td>\n",
       "      <td>-0.343968</td>\n",
       "      <td>-0.070285</td>\n",
       "      <td>-0.384164</td>\n",
       "      <td>-0.371061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012097</td>\n",
       "      <td>-0.080808</td>\n",
       "      <td>-0.079365</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.099811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2009-03-28</td>\n",
       "      <td>0.010517</td>\n",
       "      <td>-0.037255</td>\n",
       "      <td>-0.047892</td>\n",
       "      <td>-0.068171</td>\n",
       "      <td>0.061199</td>\n",
       "      <td>-0.197108</td>\n",
       "      <td>-0.249221</td>\n",
       "      <td>-0.254144</td>\n",
       "      <td>-0.252809</td>\n",
       "      <td>-0.054187</td>\n",
       "      <td>-0.081287</td>\n",
       "      <td>-0.012378</td>\n",
       "      <td>-0.040739</td>\n",
       "      <td>-0.128571</td>\n",
       "      <td>-0.057651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>-0.054945</td>\n",
       "      <td>-0.793103</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.357762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2009-06-27</td>\n",
       "      <td>0.113398</td>\n",
       "      <td>0.038903</td>\n",
       "      <td>0.175737</td>\n",
       "      <td>0.211621</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>0.021316</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.323661</td>\n",
       "      <td>0.330999</td>\n",
       "      <td>0.312444</td>\n",
       "      <td>-0.037215</td>\n",
       "      <td>0.248634</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>-0.069767</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.665381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2009-09-26</td>\n",
       "      <td>-0.013274</td>\n",
       "      <td>-0.102786</td>\n",
       "      <td>-0.287210</td>\n",
       "      <td>-0.309405</td>\n",
       "      <td>0.222188</td>\n",
       "      <td>0.947703</td>\n",
       "      <td>2.414158</td>\n",
       "      <td>2.376812</td>\n",
       "      <td>2.407407</td>\n",
       "      <td>0.297920</td>\n",
       "      <td>0.290770</td>\n",
       "      <td>0.309686</td>\n",
       "      <td>0.430196</td>\n",
       "      <td>0.223195</td>\n",
       "      <td>0.261586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.281897</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.300651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2009-12-26</td>\n",
       "      <td>0.135260</td>\n",
       "      <td>0.056314</td>\n",
       "      <td>0.144821</td>\n",
       "      <td>0.138276</td>\n",
       "      <td>0.130468</td>\n",
       "      <td>-0.034179</td>\n",
       "      <td>-0.194948</td>\n",
       "      <td>-0.197425</td>\n",
       "      <td>-0.202174</td>\n",
       "      <td>0.206583</td>\n",
       "      <td>0.108188</td>\n",
       "      <td>0.344271</td>\n",
       "      <td>0.082353</td>\n",
       "      <td>-0.007156</td>\n",
       "      <td>-0.239491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.469405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2010-03-27</td>\n",
       "      <td>0.058061</td>\n",
       "      <td>-0.029881</td>\n",
       "      <td>-0.024727</td>\n",
       "      <td>-0.066275</td>\n",
       "      <td>0.100089</td>\n",
       "      <td>-0.139259</td>\n",
       "      <td>-0.089994</td>\n",
       "      <td>-0.093583</td>\n",
       "      <td>-0.092643</td>\n",
       "      <td>0.082556</td>\n",
       "      <td>0.107991</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.052291</td>\n",
       "      <td>-0.034234</td>\n",
       "      <td>-0.104505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068699</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>-0.623529</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.429997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2010-06-26</td>\n",
       "      <td>0.134392</td>\n",
       "      <td>0.114331</td>\n",
       "      <td>0.220509</td>\n",
       "      <td>0.276638</td>\n",
       "      <td>0.095634</td>\n",
       "      <td>0.163049</td>\n",
       "      <td>0.058230</td>\n",
       "      <td>0.053097</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.132626</td>\n",
       "      <td>0.202776</td>\n",
       "      <td>0.047093</td>\n",
       "      <td>0.035734</td>\n",
       "      <td>0.031716</td>\n",
       "      <td>-0.042531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029425</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.715450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2010-09-25</td>\n",
       "      <td>0.161576</td>\n",
       "      <td>0.156662</td>\n",
       "      <td>0.267327</td>\n",
       "      <td>0.327312</td>\n",
       "      <td>0.108557</td>\n",
       "      <td>0.295732</td>\n",
       "      <td>0.324316</td>\n",
       "      <td>0.316527</td>\n",
       "      <td>0.321937</td>\n",
       "      <td>0.106265</td>\n",
       "      <td>0.051932</td>\n",
       "      <td>0.182361</td>\n",
       "      <td>-0.090027</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>-0.051463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055409</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.868606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2010-12-25</td>\n",
       "      <td>0.153745</td>\n",
       "      <td>0.053961</td>\n",
       "      <td>0.170999</td>\n",
       "      <td>0.148296</td>\n",
       "      <td>0.143856</td>\n",
       "      <td>0.314506</td>\n",
       "      <td>0.393686</td>\n",
       "      <td>0.389362</td>\n",
       "      <td>0.385776</td>\n",
       "      <td>0.135486</td>\n",
       "      <td>0.109707</td>\n",
       "      <td>0.167608</td>\n",
       "      <td>0.066055</td>\n",
       "      <td>0.028520</td>\n",
       "      <td>0.132496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015363</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.268407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2011-03-26</td>\n",
       "      <td>0.094095</td>\n",
       "      <td>0.069889</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>0.124593</td>\n",
       "      <td>-0.077559</td>\n",
       "      <td>-0.002831</td>\n",
       "      <td>-0.006126</td>\n",
       "      <td>-0.004666</td>\n",
       "      <td>0.142391</td>\n",
       "      <td>0.120352</td>\n",
       "      <td>0.168236</td>\n",
       "      <td>0.049458</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>-0.033787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>-0.348485</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.113218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2011-06-25</td>\n",
       "      <td>0.124905</td>\n",
       "      <td>-0.002107</td>\n",
       "      <td>0.119305</td>\n",
       "      <td>0.104082</td>\n",
       "      <td>0.127950</td>\n",
       "      <td>0.158268</td>\n",
       "      <td>0.220645</td>\n",
       "      <td>0.215716</td>\n",
       "      <td>0.217187</td>\n",
       "      <td>-0.029988</td>\n",
       "      <td>-0.026856</td>\n",
       "      <td>-0.033551</td>\n",
       "      <td>0.071750</td>\n",
       "      <td>-0.133218</td>\n",
       "      <td>-0.171712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052326</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.881286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2011-09-24</td>\n",
       "      <td>0.090045</td>\n",
       "      <td>-0.040727</td>\n",
       "      <td>0.062568</td>\n",
       "      <td>0.041364</td>\n",
       "      <td>0.104870</td>\n",
       "      <td>-0.010535</td>\n",
       "      <td>-0.093733</td>\n",
       "      <td>-0.096324</td>\n",
       "      <td>-0.093710</td>\n",
       "      <td>0.126814</td>\n",
       "      <td>0.190814</td>\n",
       "      <td>0.053877</td>\n",
       "      <td>-0.022727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.064272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.017850</td>\n",
       "      <td>-0.027273</td>\n",
       "      <td>-0.423077</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-19.612626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>0.191714</td>\n",
       "      <td>0.217458</td>\n",
       "      <td>0.223136</td>\n",
       "      <td>0.237290</td>\n",
       "      <td>0.175410</td>\n",
       "      <td>0.638946</td>\n",
       "      <td>0.972520</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.964589</td>\n",
       "      <td>0.041060</td>\n",
       "      <td>0.009104</td>\n",
       "      <td>0.082567</td>\n",
       "      <td>0.121082</td>\n",
       "      <td>-0.053892</td>\n",
       "      <td>-0.049832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077244</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>1.744444</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.786382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>0.088354</td>\n",
       "      <td>-0.074109</td>\n",
       "      <td>-0.003928</td>\n",
       "      <td>-0.074291</td>\n",
       "      <td>0.138184</td>\n",
       "      <td>-0.154253</td>\n",
       "      <td>-0.110380</td>\n",
       "      <td>-0.112616</td>\n",
       "      <td>-0.113194</td>\n",
       "      <td>0.319649</td>\n",
       "      <td>0.456365</td>\n",
       "      <td>0.154515</td>\n",
       "      <td>0.028861</td>\n",
       "      <td>0.124473</td>\n",
       "      <td>0.039688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051550</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>-0.230769</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.267736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>0.079253</td>\n",
       "      <td>0.024274</td>\n",
       "      <td>0.056033</td>\n",
       "      <td>0.031964</td>\n",
       "      <td>0.090226</td>\n",
       "      <td>-0.106237</td>\n",
       "      <td>-0.240750</td>\n",
       "      <td>-0.243373</td>\n",
       "      <td>-0.242276</td>\n",
       "      <td>0.131640</td>\n",
       "      <td>0.036269</td>\n",
       "      <td>0.276741</td>\n",
       "      <td>-0.076485</td>\n",
       "      <td>-0.001876</td>\n",
       "      <td>-0.030675</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005898</td>\n",
       "      <td>-0.054054</td>\n",
       "      <td>-0.421053</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.311671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2012-09-29</td>\n",
       "      <td>0.080837</td>\n",
       "      <td>0.109928</td>\n",
       "      <td>0.131065</td>\n",
       "      <td>0.165820</td>\n",
       "      <td>0.057845</td>\n",
       "      <td>0.026925</td>\n",
       "      <td>-0.068110</td>\n",
       "      <td>-0.070064</td>\n",
       "      <td>-0.068670</td>\n",
       "      <td>0.093397</td>\n",
       "      <td>0.094783</td>\n",
       "      <td>0.091555</td>\n",
       "      <td>-0.062411</td>\n",
       "      <td>0.005639</td>\n",
       "      <td>0.053446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.038772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011123</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>-0.218182</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-2.733291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2012-12-29</td>\n",
       "      <td>0.113731</td>\n",
       "      <td>0.254887</td>\n",
       "      <td>0.188198</td>\n",
       "      <td>0.216309</td>\n",
       "      <td>0.077286</td>\n",
       "      <td>0.515654</td>\n",
       "      <td>0.590417</td>\n",
       "      <td>0.590183</td>\n",
       "      <td>0.591014</td>\n",
       "      <td>-0.076197</td>\n",
       "      <td>-0.040111</td>\n",
       "      <td>-0.120717</td>\n",
       "      <td>-0.080739</td>\n",
       "      <td>-0.123364</td>\n",
       "      <td>-0.110147</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>1.001681</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.049494</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>2.709302</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.617445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2013-03-30</td>\n",
       "      <td>-0.006859</td>\n",
       "      <td>-0.124551</td>\n",
       "      <td>-0.138038</td>\n",
       "      <td>-0.242561</td>\n",
       "      <td>0.063952</td>\n",
       "      <td>-0.200121</td>\n",
       "      <td>-0.269995</td>\n",
       "      <td>-0.270639</td>\n",
       "      <td>-0.269370</td>\n",
       "      <td>-0.173045</td>\n",
       "      <td>-0.179872</td>\n",
       "      <td>-0.163966</td>\n",
       "      <td>-0.113436</td>\n",
       "      <td>-0.234542</td>\n",
       "      <td>-0.171793</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>0.578505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074556</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>-0.514107</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.797976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2013-06-29</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.077080</td>\n",
       "      <td>0.291108</td>\n",
       "      <td>0.022840</td>\n",
       "      <td>-0.089571</td>\n",
       "      <td>-0.189895</td>\n",
       "      <td>-0.277260</td>\n",
       "      <td>-0.260827</td>\n",
       "      <td>-0.259663</td>\n",
       "      <td>-0.126473</td>\n",
       "      <td>-0.160802</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-0.070497</td>\n",
       "      <td>-0.178273</td>\n",
       "      <td>-0.080616</td>\n",
       "      <td>0.367521</td>\n",
       "      <td>0.091489</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.050298</td>\n",
       "      <td>-0.053763</td>\n",
       "      <td>-0.412903</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.789348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2013-09-28</td>\n",
       "      <td>0.035746</td>\n",
       "      <td>0.074275</td>\n",
       "      <td>0.090834</td>\n",
       "      <td>0.202071</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.060839</td>\n",
       "      <td>0.088696</td>\n",
       "      <td>0.106525</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.075354</td>\n",
       "      <td>0.102946</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>-0.029068</td>\n",
       "      <td>0.142373</td>\n",
       "      <td>0.125123</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.380604</td>\n",
       "      <td>-0.001455</td>\n",
       "      <td>-0.027379</td>\n",
       "      <td>-0.022727</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.269374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2013-12-28</td>\n",
       "      <td>0.087845</td>\n",
       "      <td>0.096349</td>\n",
       "      <td>0.144384</td>\n",
       "      <td>0.231596</td>\n",
       "      <td>0.049656</td>\n",
       "      <td>0.536988</td>\n",
       "      <td>0.740149</td>\n",
       "      <td>0.755716</td>\n",
       "      <td>0.755448</td>\n",
       "      <td>0.147032</td>\n",
       "      <td>0.119499</td>\n",
       "      <td>0.182310</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>0.133531</td>\n",
       "      <td>0.159370</td>\n",
       "      <td>0.210784</td>\n",
       "      <td>0.022238</td>\n",
       "      <td>-0.047342</td>\n",
       "      <td>-0.017997</td>\n",
       "      <td>-0.023256</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.801711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2014-03-29</td>\n",
       "      <td>-0.085241</td>\n",
       "      <td>-0.122046</td>\n",
       "      <td>-0.101466</td>\n",
       "      <td>-0.196414</td>\n",
       "      <td>-0.073294</td>\n",
       "      <td>-0.207452</td>\n",
       "      <td>-0.217947</td>\n",
       "      <td>-0.198766</td>\n",
       "      <td>-0.198621</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>-0.024099</td>\n",
       "      <td>0.040431</td>\n",
       "      <td>0.049430</td>\n",
       "      <td>-0.049738</td>\n",
       "      <td>-0.012085</td>\n",
       "      <td>0.178138</td>\n",
       "      <td>-0.005180</td>\n",
       "      <td>0.078746</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.393939</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.080906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2014-06-28</td>\n",
       "      <td>0.080252</td>\n",
       "      <td>-0.036745</td>\n",
       "      <td>0.183778</td>\n",
       "      <td>0.069362</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>-0.179950</td>\n",
       "      <td>-0.242101</td>\n",
       "      <td>-0.889649</td>\n",
       "      <td>-0.889845</td>\n",
       "      <td>0.115609</td>\n",
       "      <td>0.185458</td>\n",
       "      <td>0.036023</td>\n",
       "      <td>0.027668</td>\n",
       "      <td>0.162534</td>\n",
       "      <td>0.074924</td>\n",
       "      <td>0.161512</td>\n",
       "      <td>-0.018049</td>\n",
       "      <td>0.700921</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.345000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-4.214184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2014-09-27</td>\n",
       "      <td>0.041879</td>\n",
       "      <td>0.008565</td>\n",
       "      <td>0.184209</td>\n",
       "      <td>0.373185</td>\n",
       "      <td>-0.077667</td>\n",
       "      <td>0.125321</td>\n",
       "      <td>0.092798</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.164902</td>\n",
       "      <td>0.091426</td>\n",
       "      <td>0.260643</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.149289</td>\n",
       "      <td>0.124467</td>\n",
       "      <td>0.142012</td>\n",
       "      <td>-0.018028</td>\n",
       "      <td>0.082917</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221374</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.378717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2014-12-27</td>\n",
       "      <td>0.129637</td>\n",
       "      <td>0.217011</td>\n",
       "      <td>0.151914</td>\n",
       "      <td>0.160178</td>\n",
       "      <td>0.105615</td>\n",
       "      <td>0.770980</td>\n",
       "      <td>1.128735</td>\n",
       "      <td>1.169014</td>\n",
       "      <td>1.154930</td>\n",
       "      <td>0.097641</td>\n",
       "      <td>0.154328</td>\n",
       "      <td>0.033554</td>\n",
       "      <td>0.140415</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>0.056926</td>\n",
       "      <td>0.121762</td>\n",
       "      <td>-0.109791</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.029616</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>2.268750</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.120490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2015-03-28</td>\n",
       "      <td>-0.002673</td>\n",
       "      <td>-0.185989</td>\n",
       "      <td>-0.046029</td>\n",
       "      <td>-0.202171</td>\n",
       "      <td>0.046040</td>\n",
       "      <td>-0.222376</td>\n",
       "      <td>-0.247170</td>\n",
       "      <td>-0.240260</td>\n",
       "      <td>-0.238562</td>\n",
       "      <td>0.108309</td>\n",
       "      <td>0.115658</td>\n",
       "      <td>0.099286</td>\n",
       "      <td>0.055675</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.039497</td>\n",
       "      <td>0.108545</td>\n",
       "      <td>-0.063081</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.449331</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.034818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2015-06-27</td>\n",
       "      <td>0.045778</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>0.115638</td>\n",
       "      <td>0.111631</td>\n",
       "      <td>-0.025805</td>\n",
       "      <td>-0.144889</td>\n",
       "      <td>-0.213133</td>\n",
       "      <td>-0.205128</td>\n",
       "      <td>-0.206009</td>\n",
       "      <td>0.081521</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.176527</td>\n",
       "      <td>0.051217</td>\n",
       "      <td>0.021314</td>\n",
       "      <td>-0.008100</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>-0.044886</td>\n",
       "      <td>0.214746</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.215278</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4.181488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2015-09-26</td>\n",
       "      <td>0.063437</td>\n",
       "      <td>0.259679</td>\n",
       "      <td>0.160367</td>\n",
       "      <td>0.234740</td>\n",
       "      <td>-0.050304</td>\n",
       "      <td>0.038222</td>\n",
       "      <td>0.041866</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>-0.126844</td>\n",
       "      <td>-0.011669</td>\n",
       "      <td>-0.252640</td>\n",
       "      <td>0.035697</td>\n",
       "      <td>-0.113043</td>\n",
       "      <td>-0.184045</td>\n",
       "      <td>0.097744</td>\n",
       "      <td>-0.042928</td>\n",
       "      <td>0.187119</td>\n",
       "      <td>0.010168</td>\n",
       "      <td>-0.011494</td>\n",
       "      <td>-0.221239</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>9.310070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2015-12-26</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>-0.147229</td>\n",
       "      <td>-0.035688</td>\n",
       "      <td>-0.056048</td>\n",
       "      <td>0.074668</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.650575</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.673469</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>-0.068813</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>-0.003493</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>-0.040031</td>\n",
       "      <td>0.089041</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>-0.073900</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>-0.023256</td>\n",
       "      <td>1.403409</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.788588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2016-03-26</td>\n",
       "      <td>0.040892</td>\n",
       "      <td>0.149215</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>-0.102862</td>\n",
       "      <td>0.017074</td>\n",
       "      <td>-0.333654</td>\n",
       "      <td>-0.427264</td>\n",
       "      <td>-0.421212</td>\n",
       "      <td>-0.420732</td>\n",
       "      <td>-0.120150</td>\n",
       "      <td>-0.116217</td>\n",
       "      <td>-0.124846</td>\n",
       "      <td>-0.059593</td>\n",
       "      <td>-0.186567</td>\n",
       "      <td>-0.141139</td>\n",
       "      <td>0.081761</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.282064</td>\n",
       "      <td>-0.026235</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.612293</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.794274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2016-06-25</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.070429</td>\n",
       "      <td>0.024259</td>\n",
       "      <td>0.047184</td>\n",
       "      <td>-0.030018</td>\n",
       "      <td>-0.162173</td>\n",
       "      <td>-0.258653</td>\n",
       "      <td>-0.251309</td>\n",
       "      <td>-0.252632</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.027049</td>\n",
       "      <td>-0.031605</td>\n",
       "      <td>-0.058400</td>\n",
       "      <td>-0.027523</td>\n",
       "      <td>0.048553</td>\n",
       "      <td>0.082849</td>\n",
       "      <td>0.069930</td>\n",
       "      <td>0.024445</td>\n",
       "      <td>-0.025595</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>-0.128049</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.011635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2016-09-24</td>\n",
       "      <td>0.052631</td>\n",
       "      <td>0.139802</td>\n",
       "      <td>0.080285</td>\n",
       "      <td>0.105195</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>0.106096</td>\n",
       "      <td>0.156234</td>\n",
       "      <td>0.174825</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>0.028832</td>\n",
       "      <td>0.033722</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>-0.060702</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.080142</td>\n",
       "      <td>0.076510</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.079479</td>\n",
       "      <td>-0.023502</td>\n",
       "      <td>-0.054054</td>\n",
       "      <td>0.587413</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.533132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>0.029392</td>\n",
       "      <td>-0.033097</td>\n",
       "      <td>0.027471</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>0.032289</td>\n",
       "      <td>0.672309</td>\n",
       "      <td>0.984801</td>\n",
       "      <td>1.011905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.072612</td>\n",
       "      <td>0.021604</td>\n",
       "      <td>0.137486</td>\n",
       "      <td>-0.018264</td>\n",
       "      <td>0.047511</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.071072</td>\n",
       "      <td>0.019676</td>\n",
       "      <td>-0.055263</td>\n",
       "      <td>-0.021708</td>\n",
       "      <td>-0.014286</td>\n",
       "      <td>0.986784</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.548683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>-0.012987</td>\n",
       "      <td>0.008548</td>\n",
       "      <td>-0.128230</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>-0.324884</td>\n",
       "      <td>-0.383545</td>\n",
       "      <td>-0.375740</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.163853</td>\n",
       "      <td>0.217457</td>\n",
       "      <td>0.102613</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>0.110151</td>\n",
       "      <td>0.153160</td>\n",
       "      <td>0.066356</td>\n",
       "      <td>-0.001135</td>\n",
       "      <td>0.134629</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>-0.014493</td>\n",
       "      <td>-0.596452</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.391512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>0.031809</td>\n",
       "      <td>0.106726</td>\n",
       "      <td>0.061352</td>\n",
       "      <td>0.108533</td>\n",
       "      <td>-0.012358</td>\n",
       "      <td>-0.141561</td>\n",
       "      <td>-0.209629</td>\n",
       "      <td>-0.203791</td>\n",
       "      <td>-0.204762</td>\n",
       "      <td>0.144488</td>\n",
       "      <td>0.084083</td>\n",
       "      <td>0.220460</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.122568</td>\n",
       "      <td>0.117344</td>\n",
       "      <td>0.068777</td>\n",
       "      <td>-0.004924</td>\n",
       "      <td>0.076459</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>-0.014706</td>\n",
       "      <td>-0.362637</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.257128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>0.087336</td>\n",
       "      <td>0.139712</td>\n",
       "      <td>0.134074</td>\n",
       "      <td>0.239994</td>\n",
       "      <td>0.012248</td>\n",
       "      <td>0.157924</td>\n",
       "      <td>0.229093</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>0.035859</td>\n",
       "      <td>0.052921</td>\n",
       "      <td>0.016779</td>\n",
       "      <td>0.025141</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>0.006347</td>\n",
       "      <td>0.064351</td>\n",
       "      <td>-0.019794</td>\n",
       "      <td>0.068671</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>-0.014925</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.412874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>0.083862</td>\n",
       "      <td>0.117883</td>\n",
       "      <td>0.104956</td>\n",
       "      <td>0.148531</td>\n",
       "      <td>0.045894</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.872783</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.888350</td>\n",
       "      <td>0.072553</td>\n",
       "      <td>0.074330</td>\n",
       "      <td>0.070571</td>\n",
       "      <td>0.029760</td>\n",
       "      <td>0.053422</td>\n",
       "      <td>0.028670</td>\n",
       "      <td>0.060461</td>\n",
       "      <td>-0.027184</td>\n",
       "      <td>0.022063</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>-0.015152</td>\n",
       "      <td>1.198238</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.987547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quarter end    Assets  Current Assets  Liabilities  Current Liabilities  \\\n",
       "0   1996-03-29  0.054530        0.045478     0.061899             0.058088   \n",
       "1   1996-06-28  0.021207        0.041384     0.046256            -0.152662   \n",
       "2   1996-09-27  0.003555        0.013696    -0.005714             0.039979   \n",
       "3   1996-12-27 -0.017151       -0.021262     0.007260             0.020469   \n",
       "4   1997-03-28 -0.148900       -0.175832    -0.025526            -0.016145   \n",
       "5   1997-06-27 -0.032538       -0.040912    -0.030817            -0.050224   \n",
       "6   1997-09-26 -0.024879       -0.019754    -0.035612            -0.048168   \n",
       "7   1997-12-26 -0.025278       -0.014895    -0.049786            -0.081958   \n",
       "8   1998-03-27 -0.039506       -0.047436    -0.106523            -0.170761   \n",
       "9   1998-06-26  0.019682        0.050420    -0.007767             0.003613   \n",
       "10  1998-09-25  0.061371        0.095704     0.036008             0.094312   \n",
       "11  1998-12-26  0.070646        0.045700     0.008311            -0.023684   \n",
       "12  1999-03-27  0.074695        0.058185     0.034095             0.040431   \n",
       "13  1999-06-26  0.017021        0.050342    -0.257609            -0.003238   \n",
       "14  1999-09-25  0.028292       -0.003025     0.003904             0.006498   \n",
       "15  2000-01-01  0.469870        0.145624     0.542052             0.268560   \n",
       "16  2000-04-01 -0.076325        0.000611    -0.119798            -0.056997   \n",
       "17  2000-07-01 -0.010704        0.049878    -0.012894             0.010793   \n",
       "18  2000-09-30 -0.018609        0.052356    -0.021771             0.032034   \n",
       "19  2000-12-30 -0.120094       -0.092316    -0.156528            -0.153130   \n",
       "20  2001-03-31  0.024056        0.085059     0.051891             0.096518   \n",
       "21  2001-06-30 -0.009625       -0.018148    -0.074833            -0.100836   \n",
       "22  2001-09-29 -0.008236       -0.020008    -0.050610            -0.059480   \n",
       "23  2001-12-29  0.016775        0.025083     0.026178             0.040184   \n",
       "24  2002-03-30  0.023195        0.026935     0.043599             0.082331   \n",
       "25  2002-06-29  0.004630        0.005356    -0.010222             0.005266   \n",
       "26  2002-09-28  0.000795       -0.010105    -0.010777            -0.034924   \n",
       "27  2002-12-28 -0.004605       -0.000742    -0.022696            -0.037998   \n",
       "28  2003-03-29  0.014675        0.015602     0.032048             0.258307   \n",
       "29  2003-06-28  0.012577        0.014265     0.009451             0.008969   \n",
       "30  2003-09-27  0.058066        0.061486     0.155595             0.163951   \n",
       "31  2003-12-27  0.022891        0.024121     0.018904             0.010607   \n",
       "32  2004-03-27 -0.033855       -0.040637    -0.149943            -0.168766   \n",
       "33  2004-06-26  0.072457        0.086791     0.073942             0.088384   \n",
       "34  2004-09-25  0.114495        0.122335     0.233513             0.243619   \n",
       "35  2004-12-25  0.162981        0.179589     0.201076             0.202985   \n",
       "36  2005-03-26  0.080004        0.082312     0.042833             0.039702   \n",
       "37  2005-06-25  0.037286        0.040968    -0.015570            -0.068317   \n",
       "38  2005-09-24  0.101354        0.098549     0.113990             0.115594   \n",
       "39  2005-12-31  0.227686        0.180777     0.420073             0.452354   \n",
       "40  2006-04-01 -0.019040       -0.072028    -0.098604            -0.119368   \n",
       "41  2006-07-01  0.086478        0.106769     0.106139             0.127244   \n",
       "42  2006-09-30  0.138349        0.161556     0.248444             0.288274   \n",
       "43  2006-12-30  0.131125        0.148528     0.140147             0.133828   \n",
       "44  2007-03-31 -0.038539       -0.038106    -0.216567            -0.252419   \n",
       "45  2007-06-30  0.156913        0.169443     0.277984             0.274749   \n",
       "46  2007-09-29  0.170924        0.171299     0.312022             0.329949   \n",
       "47  2007-12-29  0.185111        0.192795     0.223763             0.132918   \n",
       "48  2008-03-29  0.014381        0.020887    -0.061730            -0.085524   \n",
       "49  2008-06-28  0.040629        0.047202    -0.026655            -0.043180   \n",
       "50  2008-09-27  0.247974        0.239017     0.534045             0.528748   \n",
       "51  2008-12-27  0.081244        0.013635     0.072053             0.047190   \n",
       "52  2009-03-28  0.010517       -0.037255    -0.047892            -0.068171   \n",
       "53  2009-06-27  0.113398        0.038903     0.175737             0.211621   \n",
       "54  2009-09-26 -0.013274       -0.102786    -0.287210            -0.309405   \n",
       "55  2009-12-26  0.135260        0.056314     0.144821             0.138276   \n",
       "56  2010-03-27  0.058061       -0.029881    -0.024727            -0.066275   \n",
       "57  2010-06-26  0.134392        0.114331     0.220509             0.276638   \n",
       "58  2010-09-25  0.161576        0.156662     0.267327             0.327312   \n",
       "59  2010-12-25  0.153745        0.053961     0.170999             0.148296   \n",
       "60  2011-03-26  0.094095        0.069889     0.042119             0.022358   \n",
       "61  2011-06-25  0.124905       -0.002107     0.119305             0.104082   \n",
       "62  2011-09-24  0.090045       -0.040727     0.062568             0.041364   \n",
       "63  2011-12-31  0.191714        0.217458     0.223136             0.237290   \n",
       "64  2012-03-31  0.088354       -0.074109    -0.003928            -0.074291   \n",
       "65  2012-06-30  0.079253        0.024274     0.056033             0.031964   \n",
       "66  2012-09-29  0.080837        0.109928     0.131065             0.165820   \n",
       "67  2012-12-29  0.113731        0.254887     0.188198             0.216309   \n",
       "68  2013-03-30 -0.006859       -0.124551    -0.138038            -0.242561   \n",
       "69  2013-06-29  0.026255        0.077080     0.291108             0.022840   \n",
       "70  2013-09-28  0.035746        0.074275     0.090834             0.202071   \n",
       "71  2013-12-28  0.087845        0.096349     0.144384             0.231596   \n",
       "72  2014-03-29 -0.085241       -0.122046    -0.101466            -0.196414   \n",
       "73  2014-06-28  0.080252       -0.036745     0.183778             0.069362   \n",
       "74  2014-09-27  0.041879        0.008565     0.184209             0.373185   \n",
       "75  2014-12-27  0.129637        0.217011     0.151914             0.160178   \n",
       "76  2015-03-28 -0.002673       -0.185989    -0.046029            -0.202171   \n",
       "77  2015-06-27  0.045778        0.045102     0.115638             0.111631   \n",
       "78  2015-09-26  0.063437        0.259679     0.160367             0.234740   \n",
       "79  2015-12-26  0.009656       -0.147229    -0.035688            -0.056048   \n",
       "80  2016-03-26  0.040892        0.149215     0.059406            -0.102862   \n",
       "81  2016-06-25  0.001065        0.070429     0.024259             0.047184   \n",
       "82  2016-09-24  0.052631        0.139802     0.080285             0.105195   \n",
       "83  2016-12-31  0.029392       -0.033097     0.027471             0.064856   \n",
       "84  2017-04-01  0.010240       -0.012987     0.008548            -0.128230   \n",
       "85  2017-07-01  0.031809        0.106726     0.061352             0.108533   \n",
       "86  2017-09-30  0.087336        0.139712     0.134074             0.239994   \n",
       "87  2017-12-30  0.083862        0.117883     0.104956             0.148531   \n",
       "\n",
       "    Shareholders equity   Revenue  Earnings  EPS basic  EPS diluted     Price  \\\n",
       "0              0.053942  0.075105  0.022818  -0.001147     0.099883  0.077414   \n",
       "1             -0.017510 -0.002746 -0.956757  -0.956594    -0.997349 -0.173077   \n",
       "2              0.018812  0.065168 -1.781250  -1.807692     1.000000 -0.151163   \n",
       "3             -0.056365 -0.082723 -5.800000  -5.571429     1.000000  0.205479   \n",
       "4             -0.360453 -0.248004  4.900000   4.875000     1.000000 -0.215909   \n",
       "5             -0.037037  0.084947 -0.920904  -0.921986     1.000000 -0.115942   \n",
       "6              0.003344 -0.070812  1.875000   1.863636     1.000000  0.245902   \n",
       "7              0.036667 -0.022305 -1.291925  -1.293651     1.000000 -0.118421   \n",
       "8              0.115756 -0.109632  0.170213   0.135135     0.151515  0.089552   \n",
       "9              0.070605 -0.002135  0.836364   0.809524     0.710526  0.369863   \n",
       "10             0.104980  0.109843  0.049505   0.026316     0.000000  0.280000   \n",
       "11             0.171133  0.098972  0.433962   0.435897     0.461538 -0.023438   \n",
       "12             0.131045 -0.105263 -0.111842  -0.116071    -0.115789  0.136000   \n",
       "13             0.365517  0.018301  0.503704   0.424242     0.428571  0.049296   \n",
       "14             0.045118 -0.142490 -0.453202  -0.496454    -0.483333  0.469799   \n",
       "15             0.422036  0.753743  0.648649   0.605634     0.661290  0.429224   \n",
       "16            -0.045084 -0.169868  0.273224   0.263158     0.242718  0.351438   \n",
       "17            -0.009253 -0.061697 -0.141631  -0.569444    -0.570312 -0.070922   \n",
       "18            -0.016523  0.024658 -0.150000  -0.177419    -0.145455 -0.185751   \n",
       "19            -0.096177 -0.461497 -2.147059  -2.137255    -2.234043 -0.550000   \n",
       "20             0.007004  0.421053 -1.220513  -1.206897    -1.206897 -0.055556   \n",
       "21             0.032103  0.030748  0.418605   0.416667     0.416667  0.205882   \n",
       "22             0.016071 -0.016949  0.081967   0.176471     0.176471 -0.128049   \n",
       "23             0.011735 -0.051724 -0.424242  -0.450000    -0.450000 -0.027972   \n",
       "24             0.012103  0.087273  0.052632   0.000000     0.000000  0.187050   \n",
       "25             0.012955 -0.044147 -0.200000  -0.181818    -0.181818 -0.078788   \n",
       "26             0.007132  0.009797 -2.406250  -2.444444    -2.444444 -0.230263   \n",
       "27             0.005128  0.020097 -0.822222  -0.846154    -0.846154 -0.059829   \n",
       "28             0.005588  0.002038 -2.750000  -3.000000    -3.000000 -0.054545   \n",
       "29             0.014255  0.047458  0.357143   0.250000     0.250000  0.115385   \n",
       "30             0.005955  0.110032  1.315789   1.400000     1.400000  0.293103   \n",
       "31             0.025337  0.169679  0.431818   0.416667     0.416667  0.060000   \n",
       "32             0.036952 -0.048355 -0.269841  -0.235294    -0.294118  0.100629   \n",
       "33             0.071715  0.055003  0.326087   0.230769     0.333333  0.217143   \n",
       "34             0.054863  0.166832  0.737705   0.750000     0.562500  0.126761   \n",
       "35             0.140662  0.485106  1.783019   1.678571     1.800000  0.570833   \n",
       "36             0.102936 -0.070774 -0.016949  -0.520000    -0.514286  0.450928   \n",
       "37             0.068118  0.085415  0.103448   0.083333     0.088235  0.016453   \n",
       "38             0.094561  0.044886  0.343750   0.333333     0.351351  0.151079   \n",
       "39             0.122422  0.563078  0.313953   0.307692     0.300000  0.376563   \n",
       "40             0.036038 -0.241781 -0.274336  -0.279412    -0.276923  0.167991   \n",
       "41             0.074637  0.002524  0.151220   0.122449     0.148936 -0.103013   \n",
       "42             0.070096  0.106865  0.148305   0.163636     0.129630 -0.009751   \n",
       "43             0.124599  0.470953  0.852399   0.828125     0.868852  0.295405   \n",
       "44             0.092002 -0.260155 -0.233068  -0.239316    -0.236842  0.084459   \n",
       "45             0.093222  0.027736  0.062338   0.056180     0.057471  0.208723   \n",
       "46             0.084154  0.149168  0.105134   0.106383     0.086957  0.227448   \n",
       "47             0.156345  0.545440  0.748894   0.740385     0.760000  0.325459   \n",
       "48             0.074328 -0.218152 -0.339026  -0.342541    -0.340909 -0.106535   \n",
       "49             0.086911 -0.006390  0.025837   0.016807     0.025862  0.059840   \n",
       "50             0.071756  0.057744  0.059701   0.049587     0.050420 -0.099122   \n",
       "51             0.089349  0.287777  0.412852   0.425197     0.424000 -0.340297   \n",
       "52             0.061199 -0.197108 -0.249221  -0.254144    -0.252809 -0.054187   \n",
       "53             0.064868  0.021316  0.019917   0.022222     0.015038  0.323661   \n",
       "54             0.222188  0.947703  2.414158   2.376812     2.407407  0.297920   \n",
       "55             0.130468 -0.034179 -0.194948  -0.197425    -0.202174  0.206583   \n",
       "56             0.100089 -0.139259 -0.089994  -0.093583    -0.092643  0.082556   \n",
       "57             0.095634  0.163049  0.058230   0.053097     0.054054  0.132626   \n",
       "58             0.108557  0.295732  0.324316   0.316527     0.321937  0.106265   \n",
       "59             0.143856  0.314506  0.393686   0.389362     0.385776  0.135486   \n",
       "60             0.124593 -0.077559 -0.002831  -0.006126    -0.004666  0.142391   \n",
       "61             0.127950  0.158268  0.220645   0.215716     0.217187 -0.029988   \n",
       "62             0.104870 -0.010535 -0.093733  -0.096324    -0.093710  0.126814   \n",
       "63             0.175410  0.638946  0.972520   0.967742     0.964589  0.041060   \n",
       "64             0.138184 -0.154253 -0.110380  -0.112616    -0.113194  0.319649   \n",
       "65             0.090226 -0.106237 -0.240750  -0.243373    -0.242276  0.131640   \n",
       "66             0.057845  0.026925 -0.068110  -0.070064    -0.068670  0.093397   \n",
       "67             0.077286  0.515654  0.590417   0.590183     0.591014 -0.076197   \n",
       "68             0.063952 -0.200121 -0.269995  -0.270639    -0.269370 -0.173045   \n",
       "69            -0.089571 -0.189895 -0.277260  -0.260827    -0.259663 -0.126473   \n",
       "70             0.001581  0.060839  0.088696   0.106525     0.105756  0.075354   \n",
       "71             0.049656  0.536988  0.740149   0.755716     0.755448  0.147032   \n",
       "72            -0.073294 -0.207452 -0.217947  -0.198766    -0.198621  0.004935   \n",
       "73             0.006332 -0.179950 -0.242101  -0.889649    -0.889845  0.115609   \n",
       "74            -0.077667  0.125321  0.092798   0.100775     0.109375  0.164902   \n",
       "75             0.105615  0.770980  1.128735   1.169014     1.154930  0.097641   \n",
       "76             0.046040 -0.222376 -0.247170  -0.240260    -0.238562  0.108309   \n",
       "77            -0.025805 -0.144889 -0.213133  -0.205128    -0.206009  0.081521   \n",
       "78            -0.050304  0.038222  0.041866   0.064516     0.059459 -0.126844   \n",
       "79             0.074668  0.473214  0.650575   0.666667     0.673469  0.019648   \n",
       "80             0.017074 -0.333654 -0.427264  -0.421212    -0.420732 -0.120150   \n",
       "81            -0.030018 -0.162173 -0.258653  -0.251309    -0.252632  0.000198   \n",
       "82             0.013498  0.106096  0.156234   0.174825     0.183099  0.028832   \n",
       "83             0.032289  0.672309  0.984801   1.011905     1.000000  0.072612   \n",
       "84             0.012780 -0.324884 -0.383545  -0.375740    -0.375000  0.163853   \n",
       "85            -0.012358 -0.141561 -0.209629  -0.203791    -0.204762  0.144488   \n",
       "86             0.012248  0.157924  0.229093   0.238095     0.233533  0.035859   \n",
       "87             0.045894  0.679245  0.872783   0.884615     0.888350  0.072553   \n",
       "\n",
       "    Price high  Price low       ROE  P/B ratio  P/E ratio  \\\n",
       "0     0.078368   0.084413 -0.102530   0.033678   0.130674   \n",
       "1    -0.188976  -0.146341  0.308637   0.140625  -0.467548   \n",
       "2    -0.135922  -0.185714  0.143572  -0.136986  -0.878102   \n",
       "3     0.112360   0.333333  0.174508   0.182540   1.000000   \n",
       "4    -0.161616  -0.289474  0.071029  -0.174497   1.000000   \n",
       "5    -0.144578  -0.037037  0.160470   0.414634   1.000000   \n",
       "6     0.492958  -0.115385  0.403598   0.287356   1.000000   \n",
       "7    -0.169811   0.000000 -0.039648  -0.093750   1.000000   \n",
       "8     0.136364   0.000000 -0.872811   0.088670   1.000000   \n",
       "9     0.130000   0.913043 -1.345355   0.221719   1.000000   \n",
       "10    0.380531   0.136364  5.791139   0.214815   1.000000   \n",
       "11   -0.051282   0.020000  0.198509  -0.112805  -0.951423   \n",
       "12    0.141892   0.117647  0.063375  -0.024055  -0.131534   \n",
       "13    0.059172   0.043860  0.000731  -0.080986  -0.107143   \n",
       "14    0.597765   0.268908 -0.136646   0.272031   0.248148   \n",
       "15    0.472028   0.357616 -0.155311   0.367470   0.440950   \n",
       "16    0.275534   0.507317 -0.005010  -0.048458   0.322076   \n",
       "17   -0.072626  -0.071197 -0.079557  -0.011574  -0.170093   \n",
       "18   -0.080321  -0.369338  0.016958  -0.185012  -0.165541   \n",
       "19   -0.582969  -0.464088 -0.458311  -0.522989  -0.582996   \n",
       "20   -0.109948   0.061856 -0.449851   0.066265   0.901834   \n",
       "21    0.141176   0.300971 -0.629964   0.220339   1.325581   \n",
       "22   -0.072165  -0.216418 -1.321951  -0.162037   1.712683   \n",
       "23   -0.055556   0.019048 -9.136364  -0.038674  -0.326920   \n",
       "24    0.070588   0.383178 -0.031657   0.183908  -0.485707   \n",
       "25    0.021978  -0.202703 -0.151923  -0.087379  -0.063117   \n",
       "26   -0.279570  -0.152542 -0.634921  -0.234043  -0.109509   \n",
       "27   -0.082090  -0.030000 -0.708075  -0.055556   1.663761   \n",
       "28   -0.113821   0.020619 -1.361702  -0.066176   2.403460   \n",
       "29    0.266055  -0.050505  1.823529   0.125984  -0.399416   \n",
       "30    0.195652   0.446809 -4.458333   0.265734  -0.665047   \n",
       "31    0.072727   0.036765  1.000000   0.071823   1.000000   \n",
       "32    0.118644   0.070922  0.201807   0.072165  -0.449727   \n",
       "33    0.217172   0.218543  0.200501   0.221154   0.005584   \n",
       "34    0.124481   0.130435  0.231733   0.062992  -0.090699   \n",
       "35    0.804428   0.278846  0.708475   0.551852   0.279050   \n",
       "36    0.316973   0.695489  0.352183   0.293556  -0.174271   \n",
       "37   -0.029503   0.082040  0.232575  -0.075646  -0.301478   \n",
       "38    0.216000   0.065574  0.201190   0.091816  -0.130605   \n",
       "39    0.418421   0.315385  0.095144   0.268739   0.045491   \n",
       "40    0.144712   0.204678 -0.004072   0.051873  -0.020238   \n",
       "41   -0.145867  -0.038835  0.007269  -0.134247  -0.161632   \n",
       "42    0.054080  -0.094697 -0.013532  -0.066456  -0.087773   \n",
       "43    0.198020   0.446304  0.132144   0.208475   0.232613   \n",
       "44    0.049587   0.128255  0.052100  -0.032258  -0.107916   \n",
       "45    0.304939   0.094017  0.026488   0.108696   0.055573   \n",
       "46    0.214482   0.246094  0.016829   0.132026   0.095695   \n",
       "47    0.309395   0.349216  0.051122   0.229792   0.193788   \n",
       "48   -0.012073  -0.233736 -0.030791  -0.224413  -0.228152   \n",
       "49   -0.041201   0.234688 -0.037184  -0.012107  -0.003745   \n",
       "50   -0.058995  -0.153242 -0.039745  -0.164216  -0.146617   \n",
       "51   -0.338235  -0.343968 -0.070285  -0.384164  -0.371061   \n",
       "52   -0.081287  -0.012378 -0.040739  -0.128571  -0.057651   \n",
       "53    0.330999   0.312444 -0.037215   0.248634   0.283019   \n",
       "54    0.290770   0.309686  0.430196   0.223195   0.261586   \n",
       "55    0.108188   0.344271  0.082353  -0.007156  -0.239491   \n",
       "56    0.107991   0.053080  0.052291  -0.034234  -0.104505   \n",
       "57    0.202776   0.047093  0.035734   0.031716  -0.042531   \n",
       "58    0.051932   0.182361 -0.090027   0.014467  -0.051463   \n",
       "59    0.109707   0.167608  0.066055   0.028520   0.132496   \n",
       "60    0.120352   0.168236  0.049458   0.001733  -0.033787   \n",
       "61   -0.026856  -0.033551  0.071750  -0.133218  -0.171712   \n",
       "62    0.190814   0.053877 -0.022727   0.000000  -0.064272   \n",
       "63    0.009104   0.082567  0.121082  -0.053892  -0.049832   \n",
       "64    0.456365   0.154515  0.028861   0.124473   0.039688   \n",
       "65    0.036269   0.276741 -0.076485  -0.001876  -0.030675   \n",
       "66    0.094783   0.091555 -0.062411   0.005639   0.053446   \n",
       "67   -0.040111  -0.120717 -0.080739  -0.123364  -0.110147   \n",
       "68   -0.179872  -0.163966 -0.113436  -0.234542  -0.171793   \n",
       "69   -0.160802  -0.081022 -0.070497  -0.178273  -0.080616   \n",
       "70    0.102946   0.041992 -0.029068   0.142373   0.125123   \n",
       "71    0.119499   0.182310 -0.004474   0.133531   0.159370   \n",
       "72   -0.024099   0.040431  0.049430  -0.049738  -0.012085   \n",
       "73    0.185458   0.036023  0.027668   0.162534   0.074924   \n",
       "74    0.091426   0.260643  0.050000   0.149289   0.124467   \n",
       "75    0.154328   0.033554  0.140415   0.164948   0.056926   \n",
       "76    0.115658   0.099286  0.055675  -0.003540  -0.039497   \n",
       "77    0.007036   0.176527  0.051217   0.021314  -0.008100   \n",
       "78   -0.011669  -0.252640  0.035697  -0.113043  -0.184045   \n",
       "79   -0.068813   0.147500 -0.003493   0.050980  -0.040031   \n",
       "80   -0.116217  -0.124846 -0.059593  -0.186567  -0.141139   \n",
       "81    0.027049  -0.031605 -0.058400  -0.027523   0.048553   \n",
       "82    0.033722   0.022689 -0.060702   0.042453   0.080142   \n",
       "83    0.021604   0.137486 -0.018264   0.047511   0.108821   \n",
       "84    0.217457   0.102613  0.004293   0.110151   0.153160   \n",
       "85    0.084083   0.220460  0.008834   0.122568   0.117344   \n",
       "86    0.052921   0.016779  0.025141   0.038128   0.006347   \n",
       "87    0.074330   0.070571  0.029760   0.053422   0.028670   \n",
       "\n",
       "    Cumulative dividends per share  Dividend payout ratio  \\\n",
       "0                         0.189728               0.746084   \n",
       "1                         0.000000              -1.000000   \n",
       "2                         0.000000               1.000000   \n",
       "3                         0.000000               1.000000   \n",
       "4                         0.000000               1.000000   \n",
       "5                         0.000000               1.000000   \n",
       "6                         0.000000               1.000000   \n",
       "7                         0.000000               1.000000   \n",
       "8                         0.000000               1.000000   \n",
       "9                         0.000000               1.000000   \n",
       "10                        0.000000               1.000000   \n",
       "11                        0.000000               1.000000   \n",
       "12                        0.000000               1.000000   \n",
       "13                        0.000000               1.000000   \n",
       "14                        0.000000               1.000000   \n",
       "15                        0.000000               1.000000   \n",
       "16                        0.000000               1.000000   \n",
       "17                        0.000000               1.000000   \n",
       "18                        0.000000               1.000000   \n",
       "19                        0.000000               1.000000   \n",
       "20                        0.000000               1.000000   \n",
       "21                        0.000000               1.000000   \n",
       "22                        0.000000               1.000000   \n",
       "23                        0.000000               1.000000   \n",
       "24                        0.000000               1.000000   \n",
       "25                        0.000000               1.000000   \n",
       "26                        0.000000               1.000000   \n",
       "27                        0.000000               1.000000   \n",
       "28                        0.000000               1.000000   \n",
       "29                        0.000000               1.000000   \n",
       "30                        0.000000               1.000000   \n",
       "31                        0.000000               1.000000   \n",
       "32                        0.000000               1.000000   \n",
       "33                        0.000000               1.000000   \n",
       "34                        0.000000               1.000000   \n",
       "35                        0.000000               1.000000   \n",
       "36                        0.000000               1.000000   \n",
       "37                        0.000000               1.000000   \n",
       "38                        0.000000               1.000000   \n",
       "39                        0.000000               1.000000   \n",
       "40                        0.000000               1.000000   \n",
       "41                        0.000000               1.000000   \n",
       "42                        0.000000               1.000000   \n",
       "43                        0.000000               1.000000   \n",
       "44                        0.000000               1.000000   \n",
       "45                        0.000000               1.000000   \n",
       "46                        0.000000               1.000000   \n",
       "47                        0.000000               1.000000   \n",
       "48                        0.000000               1.000000   \n",
       "49                        0.000000               1.000000   \n",
       "50                        0.000000               1.000000   \n",
       "51                        0.000000               1.000000   \n",
       "52                        0.000000               1.000000   \n",
       "53                        0.000000               1.000000   \n",
       "54                        0.000000               1.000000   \n",
       "55                        0.000000               1.000000   \n",
       "56                        0.000000               1.000000   \n",
       "57                        0.000000               1.000000   \n",
       "58                        0.000000               1.000000   \n",
       "59                        0.000000               1.000000   \n",
       "60                        0.000000               1.000000   \n",
       "61                        0.000000               1.000000   \n",
       "62                        0.000000               1.000000   \n",
       "63                        0.000000               1.000000   \n",
       "64                        0.000000               1.000000   \n",
       "65                       12.666667               1.000000   \n",
       "66                        0.000000              -0.038772   \n",
       "67                        0.926829               1.001681   \n",
       "68                        0.481013               0.578505   \n",
       "69                        0.367521               0.091489   \n",
       "70                        0.275000               0.380604   \n",
       "71                        0.210784               0.022238   \n",
       "72                        0.178138              -0.005180   \n",
       "73                        0.161512              -0.018049   \n",
       "74                        0.142012              -0.018028   \n",
       "75                        0.121762              -0.109791   \n",
       "76                        0.108545              -0.063081   \n",
       "77                        0.108333              -0.044886   \n",
       "78                        0.097744              -0.042928   \n",
       "79                        0.089041               0.006610   \n",
       "80                        0.081761               0.073171   \n",
       "81                        0.082849               0.069930   \n",
       "82                        0.076510               0.058824   \n",
       "83                        0.071072               0.019676   \n",
       "84                        0.066356              -0.001135   \n",
       "85                        0.068777              -0.004924   \n",
       "86                        0.064351              -0.019794   \n",
       "87                        0.060461              -0.027184   \n",
       "\n",
       "    Long-term debt to equity ratio  Net margin  Asset turnover  \\\n",
       "0                         0.480173    0.202231       -0.010829   \n",
       "1                         2.187246   -0.999741       -0.005525   \n",
       "2                        -0.018519    1.000000       -0.027778   \n",
       "3                         0.060941    1.000000       -0.051429   \n",
       "4                         0.566844    1.000000       -0.030120   \n",
       "5                         0.037443    1.000000       -0.006211   \n",
       "6                        -0.003395    1.000000       -0.037500   \n",
       "7                        -0.034322    1.000000       -0.012987   \n",
       "8                        -0.102835    1.000000        0.000000   \n",
       "9                        -0.065977    1.000000       -0.032895   \n",
       "10                       -0.094028    6.428571       -0.013605   \n",
       "11                       -0.146127    0.311538       -0.006897   \n",
       "12                       -0.114896    0.168622       -0.034722   \n",
       "13                       -0.769984    0.176913       -0.028777   \n",
       "14                       -0.043564    0.044776       -0.074074   \n",
       "15                       -0.296066   -0.046939       -0.048000   \n",
       "16                        0.047059    0.087794       -0.025210   \n",
       "17                        0.008427   -0.039370       -0.034483   \n",
       "18                        0.016713    0.009221        0.008929   \n",
       "19                        0.147945   -0.376650       -0.123894   \n",
       "20                        0.011933   -0.421824       -0.040404   \n",
       "21                       -0.030660   -0.614085       -0.021053   \n",
       "22                       -0.015815    0.824818       -0.043011   \n",
       "23                       -0.018541    0.452000        0.056180   \n",
       "24                       -0.023929   -0.024793        0.010638   \n",
       "25                        0.002581   -0.135593       -0.021053   \n",
       "26                       -0.006435   -0.630719       -0.010753   \n",
       "27                        0.006477   -0.707965        0.010870   \n",
       "28                       -0.500000    0.340909       -0.010753   \n",
       "29                       -1.000000    0.254237        0.021739   \n",
       "30                        1.000000    1.000000        0.021277   \n",
       "31                        1.000000    0.873874        0.052083   \n",
       "32                        1.000000    0.153846        0.049505   \n",
       "33                        1.000000    0.166667        0.037736   \n",
       "34                        1.000000    0.189286        0.036364   \n",
       "35                        1.000000    0.561562        0.087719   \n",
       "36                        1.000000    0.303846        0.032258   \n",
       "37                        1.000000    0.182891        0.039063   \n",
       "38                        1.000000    0.194514        0.007519   \n",
       "39                        1.000000    0.034447        0.044776   \n",
       "40                        1.000000    0.006054       -0.014286   \n",
       "41                        1.000000    0.037111       -0.036232   \n",
       "42                        1.000000   -0.003868       -0.037594   \n",
       "43                        1.000000    0.139806       -0.015625   \n",
       "44                        1.000000    0.100511       -0.031746   \n",
       "45                        1.000000    0.071981       -0.032787   \n",
       "46                        1.000000    0.051264       -0.042373   \n",
       "47                        1.000000    0.055632       -0.017699   \n",
       "48                        1.000000   -0.015615       -0.036036   \n",
       "49                        1.000000   -0.012558       -0.018692   \n",
       "50                        1.000000   -0.004016       -0.057143   \n",
       "51                        1.000000   -0.012097       -0.080808   \n",
       "52                        1.000000    0.013605       -0.054945   \n",
       "53                        1.000000    0.004698       -0.069767   \n",
       "54                        1.000000    0.281897        0.175000   \n",
       "55                        1.000000    0.077124        0.063830   \n",
       "56                        1.000000    0.068699        0.040000   \n",
       "57                        1.000000    0.029425        0.057692   \n",
       "58                        1.000000   -0.055409       -0.054545   \n",
       "59                        1.000000    0.015363        0.038462   \n",
       "60                        1.000000    0.025218        0.009259   \n",
       "61                        1.000000    0.052326        0.009174   \n",
       "62                        1.000000    0.017850       -0.027273   \n",
       "63                        1.000000    0.077244        0.046729   \n",
       "64                        1.000000    0.051550       -0.008929   \n",
       "65                        1.000000   -0.005898       -0.054054   \n",
       "66                        1.000000   -0.011123       -0.047619   \n",
       "67                        1.000000   -0.049494       -0.040000   \n",
       "68                        1.000000   -0.074556       -0.031250   \n",
       "69                        1.000000   -0.050298       -0.053763   \n",
       "70                       -0.001455   -0.027379       -0.022727   \n",
       "71                       -0.047342   -0.017997       -0.023256   \n",
       "72                        0.078746    0.006579        0.000000   \n",
       "73                        0.700921    0.010271       -0.011905   \n",
       "74                        0.082917   -0.001386        0.000000   \n",
       "75                        0.014236    0.029616        0.048193   \n",
       "76                        0.178300    0.012584        0.000000   \n",
       "77                        0.214746    0.003995        0.000000   \n",
       "78                        0.187119    0.010168       -0.011494   \n",
       "79                       -0.073900    0.000875       -0.023256   \n",
       "80                        0.282064   -0.026235       -0.071429   \n",
       "81                        0.024445   -0.025595       -0.051282   \n",
       "82                        0.079479   -0.023502       -0.054054   \n",
       "83                       -0.055263   -0.021708       -0.014286   \n",
       "84                        0.134629    0.000482       -0.014493   \n",
       "85                        0.076459    0.006268       -0.014706   \n",
       "86                        0.068671    0.010541       -0.014925   \n",
       "87                        0.022063    0.001422       -0.015152   \n",
       "\n",
       "    Free cash flow per share ticker  Relative Return DJIA  \n",
       "0                  -0.004580   AAPL             -1.099229  \n",
       "1                  -1.727273   AAPL             -0.299486  \n",
       "2                   0.375000   AAPL              2.642616  \n",
       "3                  -0.818182   AAPL              1.417069  \n",
       "4                   0.000000   AAPL             -0.795442  \n",
       "5                  -4.000000   AAPL              6.661655  \n",
       "6                  -1.833333   AAPL             -0.738995  \n",
       "7                  -0.200000   AAPL             -1.159558  \n",
       "8                   0.000000   AAPL            -18.429915  \n",
       "9                   0.000000   AAPL             -0.797464  \n",
       "10                  1.000000   AAPL             -7.697962  \n",
       "11                 -0.250000   AAPL             -2.049943  \n",
       "12                  0.166667   AAPL             -0.566196  \n",
       "13                 -0.714286   AAPL              0.717602  \n",
       "14                  1.000000   AAPL             -1.521976  \n",
       "15                  0.750000   AAPL             -2.858005  \n",
       "16                 -0.857143   AAPL             -1.467200  \n",
       "17                  4.000000   AAPL             -0.062467  \n",
       "18                 -0.400000   AAPL             -1.373365  \n",
       "19                 -1.333333   AAPL             -0.587237  \n",
       "20                 -0.000000   AAPL            -14.022237  \n",
       "21                 -0.000000   AAPL             -1.664357  \n",
       "22                 -3.000000   AAPL             -4.065216  \n",
       "23                 -0.750000   AAPL             -1.620554  \n",
       "24                 -3.000000   AAPL             -0.669443  \n",
       "25                 -0.000000   AAPL             -4.271018  \n",
       "26                 -1.500000   AAPL              0.739608  \n",
       "27                  3.000000   AAPL             -1.406145  \n",
       "28                 -0.500000   AAPL             -1.495944  \n",
       "29                 -2.000000   AAPL             -3.522823  \n",
       "30                 -2.000000   AAPL             -0.694212  \n",
       "31                  2.000000   AAPL              2.319034  \n",
       "32                  0.000000   AAPL             -1.073067  \n",
       "33                  0.000000   AAPL             -2.161942  \n",
       "34                  1.333333   AAPL             -4.529463  \n",
       "35                  0.857143   AAPL             -3.086496  \n",
       "36                 -0.307692   AAPL             -1.311465  \n",
       "37                 -0.222222   AAPL              0.001167  \n",
       "38                  0.571429   AAPL             -2.325806  \n",
       "39                 -0.727273   AAPL             -0.397696  \n",
       "40                 -2.666667   AAPL              1.093514  \n",
       "41                 -3.600000   AAPL             -0.928606  \n",
       "42                  0.153846   AAPL             16.128502  \n",
       "43                  0.866667   AAPL              0.457715  \n",
       "44                 -0.642857   AAPL             -1.120882  \n",
       "45                  0.600000   AAPL            -11.137316  \n",
       "46                  0.500000   AAPL             -0.537597  \n",
       "47                  0.750000   AAPL             -2.186975  \n",
       "48                 -0.595238   AAPL              0.868366  \n",
       "49                 -0.058824   AAPL             -0.035497  \n",
       "50                  2.937500   AAPL             -0.411116  \n",
       "51                 -0.079365   AAPL              4.099811  \n",
       "52                 -0.793103   AAPL             -0.357762  \n",
       "53                  1.583333   AAPL             -1.665381  \n",
       "54                  0.354839   AAPL              0.300651  \n",
       "55                  1.023810   AAPL             -0.469405  \n",
       "56                 -0.623529   AAPL             -0.429997  \n",
       "57                  1.031250   AAPL             -3.715450  \n",
       "58                  0.169231   AAPL             -1.868606  \n",
       "59                  0.736842   AAPL             -0.268407  \n",
       "60                 -0.348485   AAPL             -0.113218  \n",
       "61                  0.813953   AAPL             -0.881286  \n",
       "62                 -0.423077   AAPL            -19.612626  \n",
       "63                  1.744444   AAPL             -1.786382  \n",
       "64                 -0.230769   AAPL             -0.267736  \n",
       "65                 -0.421053   AAPL             -1.311671  \n",
       "66                 -0.218182   AAPL             -2.733291  \n",
       "67                  2.709302   AAPL             -1.617445  \n",
       "68                 -0.514107   AAPL             -4.797976  \n",
       "69                 -0.412903   AAPL             -0.789348  \n",
       "70                  0.329670   AAPL             -0.269374  \n",
       "71                  1.727273   AAPL              4.801711  \n",
       "72                 -0.393939   AAPL             -1.080906  \n",
       "73                 -0.345000   AAPL             -4.214184  \n",
       "74                  0.221374   AAPL             -0.378717  \n",
       "75                  2.268750   AAPL              2.120490  \n",
       "76                 -0.449331   AAPL             -1.034818  \n",
       "77                 -0.215278   AAPL              4.181488  \n",
       "78                 -0.221239   AAPL              9.310070  \n",
       "79                  1.403409   AAPL             -1.788588  \n",
       "80                 -0.612293   AAPL             -0.794274  \n",
       "81                 -0.128049   AAPL              0.011635  \n",
       "82                  0.587413   AAPL              0.533132  \n",
       "83                  0.986784   AAPL              2.548683  \n",
       "84                 -0.596452   AAPL             -0.391512  \n",
       "85                 -0.362637   AAPL             -0.257128  \n",
       "86                  0.956897   AAPL              0.412874  \n",
       "87                  1.198238   AAPL              0.987547  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_data_trend_standardized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization For all Features\n",
    "\n",
    "This is done after partitioning dataset into `Train-Validation-Test of 60%-20%-20%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.206517548075255"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_data_trend_standardized[51]['Relative Return DJIA'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization using StandardScalar sklearn library\n",
    "def standardize(df_train, df_test):\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    cols_x = list(df_train.drop(columns=['Quarter end', 'ticker']))\n",
    "    col_y = 'Relative Return DJIA'\n",
    "    \n",
    "    df_train = df_train[cols_x]\n",
    "    df_test = df_test[cols_x]\n",
    "    \n",
    "    df_train_stand = deepcopy(df_train)\n",
    "    df_test_stand = deepcopy(df_test)\n",
    "    \n",
    "    for c in cols_x:\n",
    "        # fit on training data column\n",
    "        scale = StandardScaler().fit(df_train_stand[[c]])\n",
    "\n",
    "        # transform the training data column\n",
    "        df_train_stand[c] = scale.transform(df_train_stand[[c]])\n",
    "\n",
    "        # transform the testing data column\n",
    "        df_test_stand[c] = scale.transform(df_test_stand[[c]])\n",
    "    \n",
    "    return df_train_stand, df_test_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the FNN model\n",
    "\n",
    "1. predict the relative return for next quarter for each stock using 60% data to train model\n",
    "2. predict relative return for validation set\n",
    "3. predict relative return for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Training/Validation/Test Set\n",
    "\n",
    "- Here we do not need the validation set, instead we validate when building the improved model\n",
    "- we replicate the model in the paper, then adjust parameters to find ways to improve the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index for training dataset\n",
    "idx_80percent = int(0.8*88)\n",
    "idx_20percent = int(0.2*88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = financial_data_trend_standardized[0][:idx_80percent]\n",
    "# df_validation = financial_data_trend_standardized[0][idx_60percent: idx_60percent+idx_20percent+1]\n",
    "df_test = financial_data_trend_standardized[0][idx_80percent:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 24), (18, 24))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_stand, df_test_stand = standardize(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stand, X_test_stand = df_train_stand.drop(columns='Relative Return DJIA').to_numpy(), df_test_stand.drop(columns='Relative Return DJIA').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_stand, Y_test_stand = df_train_stand['Relative Return DJIA'].to_numpy(), df_test_stand['Relative Return DJIA'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 21)                462       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(21, input_dim = len(X_train_stand[0, :]), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "7/7 [==============================] - 1s 51ms/step - loss: 1.1848 - accuracy: 0.0000e+00 - val_loss: 0.5192 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1651 - accuracy: 0.0000e+00 - val_loss: 0.5218 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1500 - accuracy: 0.0000e+00 - val_loss: 0.5260 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1354 - accuracy: 0.0000e+00 - val_loss: 0.5290 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1219 - accuracy: 0.0000e+00 - val_loss: 0.5312 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1090 - accuracy: 0.0000e+00 - val_loss: 0.5336 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0990 - accuracy: 0.0000e+00 - val_loss: 0.5368 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0861 - accuracy: 0.0000e+00 - val_loss: 0.5388 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0773 - accuracy: 0.0000e+00 - val_loss: 0.5408 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0687 - accuracy: 0.0000e+00 - val_loss: 0.5431 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0593 - accuracy: 0.0000e+00 - val_loss: 0.5444 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0507 - accuracy: 0.0000e+00 - val_loss: 0.5471 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0439 - accuracy: 0.0000e+00 - val_loss: 0.5471 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.0374 - accuracy: 0.0000e+00 - val_loss: 0.5483 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0314 - accuracy: 0.0000e+00 - val_loss: 0.5497 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0247 - accuracy: 0.0000e+00 - val_loss: 0.5494 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0182 - accuracy: 0.0000e+00 - val_loss: 0.5489 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0121 - accuracy: 0.0000e+00 - val_loss: 0.5487 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0070 - accuracy: 0.0000e+00 - val_loss: 0.5482 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0015 - accuracy: 0.0000e+00 - val_loss: 0.5495 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9967 - accuracy: 0.0000e+00 - val_loss: 0.5492 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9915 - accuracy: 0.0000e+00 - val_loss: 0.5499 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9866 - accuracy: 0.0000e+00 - val_loss: 0.5494 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9819 - accuracy: 0.0000e+00 - val_loss: 0.5496 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9772 - accuracy: 0.0000e+00 - val_loss: 0.5495 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9729 - accuracy: 0.0000e+00 - val_loss: 0.5488 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9693 - accuracy: 0.0000e+00 - val_loss: 0.5477 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9651 - accuracy: 0.0000e+00 - val_loss: 0.5487 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9611 - accuracy: 0.0000e+00 - val_loss: 0.5492 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9580 - accuracy: 0.0000e+00 - val_loss: 0.5503 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9535 - accuracy: 0.0000e+00 - val_loss: 0.5493 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9513 - accuracy: 0.0000e+00 - val_loss: 0.5475 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9479 - accuracy: 0.0000e+00 - val_loss: 0.5464 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9437 - accuracy: 0.0000e+00 - val_loss: 0.5476 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9408 - accuracy: 0.0000e+00 - val_loss: 0.5483 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9378 - accuracy: 0.0000e+00 - val_loss: 0.5481 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9351 - accuracy: 0.0000e+00 - val_loss: 0.5476 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9326 - accuracy: 0.0000e+00 - val_loss: 0.5482 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.5504 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9273 - accuracy: 0.0000e+00 - val_loss: 0.5519 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9245 - accuracy: 0.0000e+00 - val_loss: 0.5517 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9220 - accuracy: 0.0000e+00 - val_loss: 0.5537 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9204 - accuracy: 0.0000e+00 - val_loss: 0.5532 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9179 - accuracy: 0.0000e+00 - val_loss: 0.5546 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9156 - accuracy: 0.0000e+00 - val_loss: 0.5555 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9135 - accuracy: 0.0000e+00 - val_loss: 0.5560 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9118 - accuracy: 0.0000e+00 - val_loss: 0.5562 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9093 - accuracy: 0.0000e+00 - val_loss: 0.5581 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9079 - accuracy: 0.0000e+00 - val_loss: 0.5606 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9065 - accuracy: 0.0000e+00 - val_loss: 0.5590 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9038 - accuracy: 0.0000e+00 - val_loss: 0.5606 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9025 - accuracy: 0.0000e+00 - val_loss: 0.5616 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9007 - accuracy: 0.0000e+00 - val_loss: 0.5632 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8992 - accuracy: 0.0000e+00 - val_loss: 0.5646 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8977 - accuracy: 0.0000e+00 - val_loss: 0.5668 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8966 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8949 - accuracy: 0.0000e+00 - val_loss: 0.5690 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8935 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8918 - accuracy: 0.0000e+00 - val_loss: 0.5719 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.5743 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8894 - accuracy: 0.0000e+00 - val_loss: 0.5741 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8880 - accuracy: 0.0000e+00 - val_loss: 0.5751 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8869 - accuracy: 0.0000e+00 - val_loss: 0.5766 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8853 - accuracy: 0.0000e+00 - val_loss: 0.5780 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8845 - accuracy: 0.0000e+00 - val_loss: 0.5789 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8831 - accuracy: 0.0000e+00 - val_loss: 0.5806 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8821 - accuracy: 0.0000e+00 - val_loss: 0.5810 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8812 - accuracy: 0.0000e+00 - val_loss: 0.5822 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8798 - accuracy: 0.0000e+00 - val_loss: 0.5840 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.5860 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8780 - accuracy: 0.0000e+00 - val_loss: 0.5859 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.5871 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8758 - accuracy: 0.0000e+00 - val_loss: 0.5885 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.5900 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8740 - accuracy: 0.0000e+00 - val_loss: 0.5906 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.5916 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8720 - accuracy: 0.0000e+00 - val_loss: 0.5932 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.5948 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8703 - accuracy: 0.0000e+00 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8693 - accuracy: 0.0000e+00 - val_loss: 0.5959 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8686 - accuracy: 0.0000e+00 - val_loss: 0.5979 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8676 - accuracy: 0.0000e+00 - val_loss: 0.5988 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8667 - accuracy: 0.0000e+00 - val_loss: 0.5990 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8665 - accuracy: 0.0000e+00 - val_loss: 0.5984 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8653 - accuracy: 0.0000e+00 - val_loss: 0.5996 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8649 - accuracy: 0.0000e+00 - val_loss: 0.6019 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8639 - accuracy: 0.0000e+00 - val_loss: 0.6025 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8636 - accuracy: 0.0000e+00 - val_loss: 0.6013 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8622 - accuracy: 0.0000e+00 - val_loss: 0.6030 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8616 - accuracy: 0.0000e+00 - val_loss: 0.6030 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8608 - accuracy: 0.0000e+00 - val_loss: 0.6033 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8599 - accuracy: 0.0000e+00 - val_loss: 0.6043 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8594 - accuracy: 0.0000e+00 - val_loss: 0.6047 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8586 - accuracy: 0.0000e+00 - val_loss: 0.6052 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8579 - accuracy: 0.0000e+00 - val_loss: 0.6055 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8575 - accuracy: 0.0000e+00 - val_loss: 0.6075 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8568 - accuracy: 0.0000e+00 - val_loss: 0.6077 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8558 - accuracy: 0.0000e+00 - val_loss: 0.6080 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.6084 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8549 - accuracy: 0.0000e+00 - val_loss: 0.6093 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8546 - accuracy: 0.0000e+00 - val_loss: 0.6084 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.6089 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8530 - accuracy: 0.0000e+00 - val_loss: 0.6095 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8525 - accuracy: 0.0000e+00 - val_loss: 0.6098 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8519 - accuracy: 0.0000e+00 - val_loss: 0.6099 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8513 - accuracy: 0.0000e+00 - val_loss: 0.6107 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - val_loss: 0.6121 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8502 - accuracy: 0.0000e+00 - val_loss: 0.6133 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8495 - accuracy: 0.0000e+00 - val_loss: 0.6135 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.6145 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8486 - accuracy: 0.0000e+00 - val_loss: 0.6147 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8480 - accuracy: 0.0000e+00 - val_loss: 0.6154 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8475 - accuracy: 0.0000e+00 - val_loss: 0.6161 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8473 - accuracy: 0.0000e+00 - val_loss: 0.6158 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8466 - accuracy: 0.0000e+00 - val_loss: 0.6165 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8460 - accuracy: 0.0000e+00 - val_loss: 0.6175 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8458 - accuracy: 0.0000e+00 - val_loss: 0.6174 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8457 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8450 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8442 - accuracy: 0.0000e+00 - val_loss: 0.6194 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8439 - accuracy: 0.0000e+00 - val_loss: 0.6204 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8435 - accuracy: 0.0000e+00 - val_loss: 0.6208 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8428 - accuracy: 0.0000e+00 - val_loss: 0.6206 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.6211 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8422 - accuracy: 0.0000e+00 - val_loss: 0.6210 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8417 - accuracy: 0.0000e+00 - val_loss: 0.6216 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8414 - accuracy: 0.0000e+00 - val_loss: 0.6220 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8408 - accuracy: 0.0000e+00 - val_loss: 0.6228 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8404 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8402 - accuracy: 0.0000e+00 - val_loss: 0.6230 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8399 - accuracy: 0.0000e+00 - val_loss: 0.6240 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8394 - accuracy: 0.0000e+00 - val_loss: 0.6247 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8390 - accuracy: 0.0000e+00 - val_loss: 0.6253 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8386 - accuracy: 0.0000e+00 - val_loss: 0.6248 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8382 - accuracy: 0.0000e+00 - val_loss: 0.6255 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8377 - accuracy: 0.0000e+00 - val_loss: 0.6256 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.6262 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.6262 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8366 - accuracy: 0.0000e+00 - val_loss: 0.6268 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8365 - accuracy: 0.0000e+00 - val_loss: 0.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8362 - accuracy: 0.0000e+00 - val_loss: 0.6280 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8359 - accuracy: 0.0000e+00 - val_loss: 0.6287 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8354 - accuracy: 0.0000e+00 - val_loss: 0.6285 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8351 - accuracy: 0.0000e+00 - val_loss: 0.6293 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8348 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8344 - accuracy: 0.0000e+00 - val_loss: 0.6295 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8340 - accuracy: 0.0000e+00 - val_loss: 0.6303 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8339 - accuracy: 0.0000e+00 - val_loss: 0.6309 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8334 - accuracy: 0.0000e+00 - val_loss: 0.6310 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.6316 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8329 - accuracy: 0.0000e+00 - val_loss: 0.6322 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.6321 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8323 - accuracy: 0.0000e+00 - val_loss: 0.6321 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8323 - accuracy: 0.0000e+00 - val_loss: 0.6326 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8316 - accuracy: 0.0000e+00 - val_loss: 0.6328 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8314 - accuracy: 0.0000e+00 - val_loss: 0.6330 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8311 - accuracy: 0.0000e+00 - val_loss: 0.6332 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8308 - accuracy: 0.0000e+00 - val_loss: 0.6337 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8307 - accuracy: 0.0000e+00 - val_loss: 0.6337 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8304 - accuracy: 0.0000e+00 - val_loss: 0.6348 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8299 - accuracy: 0.0000e+00 - val_loss: 0.6351 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8298 - accuracy: 0.0000e+00 - val_loss: 0.6352 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8294 - accuracy: 0.0000e+00 - val_loss: 0.6353 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8292 - accuracy: 0.0000e+00 - val_loss: 0.6360 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8288 - accuracy: 0.0000e+00 - val_loss: 0.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8287 - accuracy: 0.0000e+00 - val_loss: 0.6363 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8283 - accuracy: 0.0000e+00 - val_loss: 0.6368 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8284 - accuracy: 0.0000e+00 - val_loss: 0.6381 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8279 - accuracy: 0.0000e+00 - val_loss: 0.6384 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8276 - accuracy: 0.0000e+00 - val_loss: 0.6386 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8274 - accuracy: 0.0000e+00 - val_loss: 0.6382 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8273 - accuracy: 0.0000e+00 - val_loss: 0.6380 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8268 - accuracy: 0.0000e+00 - val_loss: 0.6383 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8266 - accuracy: 0.0000e+00 - val_loss: 0.6392 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.6399 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8262 - accuracy: 0.0000e+00 - val_loss: 0.6403 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8258 - accuracy: 0.0000e+00 - val_loss: 0.6405 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8256 - accuracy: 0.0000e+00 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8253 - accuracy: 0.0000e+00 - val_loss: 0.6408 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8251 - accuracy: 0.0000e+00 - val_loss: 0.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8248 - accuracy: 0.0000e+00 - val_loss: 0.6416 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8246 - accuracy: 0.0000e+00 - val_loss: 0.6415 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8245 - accuracy: 0.0000e+00 - val_loss: 0.6416 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8242 - accuracy: 0.0000e+00 - val_loss: 0.6423 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8239 - accuracy: 0.0000e+00 - val_loss: 0.6429 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8237 - accuracy: 0.0000e+00 - val_loss: 0.6425 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8235 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8233 - accuracy: 0.0000e+00 - val_loss: 0.6437 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8230 - accuracy: 0.0000e+00 - val_loss: 0.6438 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8227 - accuracy: 0.0000e+00 - val_loss: 0.6443 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8225 - accuracy: 0.0000e+00 - val_loss: 0.6446 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8225 - accuracy: 0.0000e+00 - val_loss: 0.6444 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8222 - accuracy: 0.0000e+00 - val_loss: 0.6446 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8218 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8216 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8215 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8212 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8211 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8209 - accuracy: 0.0000e+00 - val_loss: 0.6466 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8208 - accuracy: 0.0000e+00 - val_loss: 0.6463 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8205 - accuracy: 0.0000e+00 - val_loss: 0.6471 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8204 - accuracy: 0.0000e+00 - val_loss: 0.6470 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8201 - accuracy: 0.0000e+00 - val_loss: 0.6479 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8198 - accuracy: 0.0000e+00 - val_loss: 0.6481 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8198 - accuracy: 0.0000e+00 - val_loss: 0.6473 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8195 - accuracy: 0.0000e+00 - val_loss: 0.6478 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8191 - accuracy: 0.0000e+00 - val_loss: 0.6484 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8190 - accuracy: 0.0000e+00 - val_loss: 0.6486 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8188 - accuracy: 0.0000e+00 - val_loss: 0.6488 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8186 - accuracy: 0.0000e+00 - val_loss: 0.6490 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8184 - accuracy: 0.0000e+00 - val_loss: 0.6494 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8182 - accuracy: 0.0000e+00 - val_loss: 0.6497 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8180 - accuracy: 0.0000e+00 - val_loss: 0.6501 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8178 - accuracy: 0.0000e+00 - val_loss: 0.6500 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8177 - accuracy: 0.0000e+00 - val_loss: 0.6499 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8176 - accuracy: 0.0000e+00 - val_loss: 0.6505 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8173 - accuracy: 0.0000e+00 - val_loss: 0.6503 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8171 - accuracy: 0.0000e+00 - val_loss: 0.6507 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8168 - accuracy: 0.0000e+00 - val_loss: 0.6513 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8168 - accuracy: 0.0000e+00 - val_loss: 0.6516 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8167 - accuracy: 0.0000e+00 - val_loss: 0.6514 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8163 - accuracy: 0.0000e+00 - val_loss: 0.6518 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8161 - accuracy: 0.0000e+00 - val_loss: 0.6517 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8160 - accuracy: 0.0000e+00 - val_loss: 0.6519 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8160 - accuracy: 0.0000e+00 - val_loss: 0.6526 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8156 - accuracy: 0.0000e+00 - val_loss: 0.6526 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8154 - accuracy: 0.0000e+00 - val_loss: 0.6526 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8152 - accuracy: 0.0000e+00 - val_loss: 0.6529 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8151 - accuracy: 0.0000e+00 - val_loss: 0.6533 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8150 - accuracy: 0.0000e+00 - val_loss: 0.6530 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8148 - accuracy: 0.0000e+00 - val_loss: 0.6533 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8147 - accuracy: 0.0000e+00 - val_loss: 0.6537 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8144 - accuracy: 0.0000e+00 - val_loss: 0.6536 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8142 - accuracy: 0.0000e+00 - val_loss: 0.6539 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8140 - accuracy: 0.0000e+00 - val_loss: 0.6539 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8140 - accuracy: 0.0000e+00 - val_loss: 0.6542 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8138 - accuracy: 0.0000e+00 - val_loss: 0.6540 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8137 - accuracy: 0.0000e+00 - val_loss: 0.6543 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8134 - accuracy: 0.0000e+00 - val_loss: 0.6549 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8133 - accuracy: 0.0000e+00 - val_loss: 0.6553 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8131 - accuracy: 0.0000e+00 - val_loss: 0.6551 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8129 - accuracy: 0.0000e+00 - val_loss: 0.6553 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8127 - accuracy: 0.0000e+00 - val_loss: 0.6554 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8126 - accuracy: 0.0000e+00 - val_loss: 0.6554 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8125 - accuracy: 0.0000e+00 - val_loss: 0.6560 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8123 - accuracy: 0.0000e+00 - val_loss: 0.6555 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8121 - accuracy: 0.0000e+00 - val_loss: 0.6557 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8119 - accuracy: 0.0000e+00 - val_loss: 0.6562 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8118 - accuracy: 0.0000e+00 - val_loss: 0.6562 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8117 - accuracy: 0.0000e+00 - val_loss: 0.6566 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8115 - accuracy: 0.0000e+00 - val_loss: 0.6563 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8113 - accuracy: 0.0000e+00 - val_loss: 0.6563 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8111 - accuracy: 0.0000e+00 - val_loss: 0.6567 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8109 - accuracy: 0.0000e+00 - val_loss: 0.6570 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8108 - accuracy: 0.0000e+00 - val_loss: 0.6570 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8107 - accuracy: 0.0000e+00 - val_loss: 0.6569 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_stand, Y_train_stand, epochs=256, batch_size=10, validation_data=(X_test_stand, Y_test_stand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step - loss: 0.8104 - accuracy: 0.0000e+00\n",
      "['loss', 'accuracy']\n",
      "[0.8103904128074646, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training data\n",
    "scores = model.evaluate(X_train_stand, Y_train_stand)\n",
    "print(model.metrics_names)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6569 - accuracy: 0.0000e+00\n",
      "[0.6569053530693054, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "scores = model.evaluate(X_test_stand, Y_test_stand)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what model actually predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28972477  1.40081172  0.11191612 -0.57459258  0.26576755  0.81334972\n",
      "  0.12201413  1.26491947  2.38860399 -0.04313864  0.17471787  0.35129462\n",
      "  0.46555581  0.9071679   0.26296411  0.29240793  0.43920705  0.56511919]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test_stand[0:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.3970236e-03]\n",
      " [4.4592894e-06]\n",
      " [1.2673002e-02]\n",
      " [6.1965394e-03]\n",
      " [1.0566828e-03]\n",
      " [4.6456015e-07]\n",
      " [1.9406963e-03]\n",
      " [1.2213264e-02]\n",
      " [8.4847592e-02]\n",
      " [2.5892408e-05]\n",
      " [8.3134569e-02]\n",
      " [2.3875622e-02]\n",
      " [3.2903091e-03]\n",
      " [5.9481259e-05]\n",
      " [8.5965935e-03]\n",
      " [7.6673185e-03]\n",
      " [6.1599625e-04]\n",
      " [4.9095892e-05]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction[0:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0139802545, 0.025482606)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.mean(), prediction.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5276561523268812"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_stand.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test set - every quarter of each stock, construct top 30 buy and bottom 30 sell based on predicted relative return\n",
    "# to do this we need the model for each stock in the dataset list, X_train, Y_train --> model --> predict\n",
    "# for test set - for every quarter of each stock, find the mean and std of the relative return\n",
    "# each portfolio contains buy and sell for each quarter rank based on relative return\n",
    "# find mean and std of relative return for all 70 stocks for 18 quarters in the predicted portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions for the model\n",
    "\n",
    "Need to fix the dataset when doing standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df_trend_standardize):\n",
    "    \n",
    "    def standardize(df_train, df_test):\n",
    "\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from copy import deepcopy\n",
    "\n",
    "        cols_x = list(df_train.drop(columns=['Quarter end', 'ticker']))\n",
    "        col_y = 'Relative Return DJIA'\n",
    "\n",
    "        df_train = df_train[cols_x]\n",
    "        df_test = df_test[cols_x]\n",
    "\n",
    "        df_train_stand = deepcopy(df_train)\n",
    "        df_test_stand = deepcopy(df_test)\n",
    "\n",
    "        for c in cols_x:\n",
    "            # fit on training data column\n",
    "            scale = StandardScaler().fit(df_train_stand[[c]])\n",
    "\n",
    "            # transform the training data column\n",
    "            df_train_stand[c] = scale.transform(df_train_stand[[c]])\n",
    "\n",
    "            # transform the testing data column\n",
    "            df_test_stand[c] = scale.transform(df_test_stand[[c]])\n",
    "\n",
    "        return df_train_stand, df_test_stand\n",
    "    \n",
    "    df = deepcopy(df_trend_standardize)\n",
    "    ticker = df_trend_standardize['ticker'].iloc[0]\n",
    "    \n",
    "    idx_80percent = int(0.8*88)\n",
    "    idx_20percent = int(0.2*88)\n",
    "    \n",
    "    # split dataset\n",
    "    df_train = df[:idx_80percent]\n",
    "    df_test = df[idx_80percent:]  \n",
    "#     print('shape of df_train', df_train.shape, 'shape of df_test', df_test.shape)\n",
    "    \n",
    "    # standardize dataset, get numpy form of training and test dataset\n",
    "    df_train_stand, df_test_stand = standardize(df_train, df_test)\n",
    "    X_train_stand, X_test_stand = df_train_stand.drop(columns='Relative Return DJIA').to_numpy(), df_test_stand.drop(columns='Relative Return DJIA').to_numpy()\n",
    "    Y_train_stand, Y_test_stand = df_train_stand['Relative Return DJIA'].to_numpy(), df_test_stand['Relative Return DJIA'].to_numpy()\n",
    "    \n",
    "    return [X_train_stand, X_test_stand, Y_train_stand, Y_test_stand]\n",
    "\n",
    "def create_model(df_trend_standardize):\n",
    "    \"\"\"\n",
    "    This function creates the model for each of the stock\n",
    "    80% of dataset for each stock is used as training data\n",
    "    20% of dataset for each stock is used as test data\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    \n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(21, input_dim = len(X_train_stand[0, :]), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # history of model\n",
    "    history = model.fit(X_train_stand, Y_train_stand, epochs=256, batch_size=10, validation_data=(X_test_stand, Y_test_stand))\n",
    "    \n",
    "    # save model\n",
    "    model.save(f'01_Paper_1/Models/{ticker}_FNN_Model.hd5')\n",
    "\n",
    "    # evaluate model on test data\n",
    "    scores = model.evaluate(X_test_stand, Y_test_stand)\n",
    "    print(scores)\n",
    "\n",
    "    # predictions\n",
    "#     prediction = model.predict(X_test_stand)\n",
    "    \n",
    "    return model, history, scores #, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "31\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "40\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(financial_data_trend_standardized):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(n)\n\u001b[0;32m----> 5\u001b[0m     models_dataset\u001b[38;5;241m.\u001b[39mappend(create_dataset(df))\n",
      "Cell \u001b[0;32mIn [30], line 41\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(df_trend_standardize)\u001b[0m\n\u001b[1;32m     37\u001b[0m     df_test \u001b[38;5;241m=\u001b[39m df[idx_80percent:]  \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     print('shape of df_train', df_train.shape, 'shape of df_test', df_test.shape)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# standardize dataset, get numpy form of training and test dataset\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     df_train_stand, df_test_stand \u001b[38;5;241m=\u001b[39m \u001b[43mstandardize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     X_train_stand, X_test_stand \u001b[38;5;241m=\u001b[39m df_train_stand\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Return DJIA\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy(), df_test_stand\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Return DJIA\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     43\u001b[0m     Y_train_stand, Y_test_stand \u001b[38;5;241m=\u001b[39m df_train_stand[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Return DJIA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(), df_test_stand[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Return DJIA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "Cell \u001b[0;32mIn [30], line 25\u001b[0m, in \u001b[0;36mcreate_dataset.<locals>.standardize\u001b[0;34m(df_train, df_test)\u001b[0m\n\u001b[1;32m     22\u001b[0m     df_train_stand[c] \u001b[38;5;241m=\u001b[39m scale\u001b[38;5;241m.\u001b[39mtransform(df_train_stand[[c]])\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# transform the testing data column\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     df_test_stand[c] \u001b[38;5;241m=\u001b[39m \u001b[43mscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test_stand\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_train_stand, df_test_stand\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:975\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    972\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    974\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m--> 975\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# create dataset for models, 0 Xtrain, 1: Xtest, 2: Ytrain, 3: Ytest\n",
    "models_dataset = []\n",
    "for n, df in enumerate(financial_data_trend_standardized):\n",
    "    print(n)\n",
    "    models_dataset.append(create_dataset(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter end</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Current Assets</th>\n",
       "      <th>Liabilities</th>\n",
       "      <th>Current Liabilities</th>\n",
       "      <th>Shareholders equity</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>EPS basic</th>\n",
       "      <th>EPS diluted</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price high</th>\n",
       "      <th>Price low</th>\n",
       "      <th>ROE</th>\n",
       "      <th>P/B ratio</th>\n",
       "      <th>P/E ratio</th>\n",
       "      <th>Cumulative dividends per share</th>\n",
       "      <th>Dividend payout ratio</th>\n",
       "      <th>Long-term debt to equity ratio</th>\n",
       "      <th>Net margin</th>\n",
       "      <th>Asset turnover</th>\n",
       "      <th>Free cash flow per share</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Relative Return DJIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-03-31</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>0.014527</td>\n",
       "      <td>0.068033</td>\n",
       "      <td>-0.456613</td>\n",
       "      <td>-0.388844</td>\n",
       "      <td>0.020935</td>\n",
       "      <td>0.021254</td>\n",
       "      <td>0.032220</td>\n",
       "      <td>-0.014770</td>\n",
       "      <td>0.008203</td>\n",
       "      <td>0.032301</td>\n",
       "      <td>0.039157</td>\n",
       "      <td>0.024716</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.016791</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.471454</td>\n",
       "      <td>GE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-06-30</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.115101</td>\n",
       "      <td>0.257746</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>-1.007812</td>\n",
       "      <td>-0.136059</td>\n",
       "      <td>-0.133409</td>\n",
       "      <td>-0.139257</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>-0.126812</td>\n",
       "      <td>-0.157811</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>-0.003009</td>\n",
       "      <td>-0.049245</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>-0.030303</td>\n",
       "      <td>-4.333333</td>\n",
       "      <td>GE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-09-30</td>\n",
       "      <td>0.029333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023996</td>\n",
       "      <td>0.050089</td>\n",
       "      <td>-0.062893</td>\n",
       "      <td>-0.052174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045198</td>\n",
       "      <td>-0.041748</td>\n",
       "      <td>-0.048536</td>\n",
       "      <td>0.010970</td>\n",
       "      <td>-0.051867</td>\n",
       "      <td>-0.075330</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.001393</td>\n",
       "      <td>-0.042554</td>\n",
       "      <td>-0.010616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.550000</td>\n",
       "      <td>GE</td>\n",
       "      <td>2.615307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>0.093185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.102438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037535</td>\n",
       "      <td>0.148494</td>\n",
       "      <td>0.156040</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.075444</td>\n",
       "      <td>-0.086453</td>\n",
       "      <td>-0.062348</td>\n",
       "      <td>0.015442</td>\n",
       "      <td>-0.100656</td>\n",
       "      <td>-0.103360</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.818182</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.276595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>-0.008568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.007487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.021237</td>\n",
       "      <td>-0.123380</td>\n",
       "      <td>-0.188679</td>\n",
       "      <td>-0.592000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807200</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.698618</td>\n",
       "      <td>0.011919</td>\n",
       "      <td>0.739659</td>\n",
       "      <td>0.749006</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.005344</td>\n",
       "      <td>-0.038111</td>\n",
       "      <td>-0.015234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.941176</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.671290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997-06-30</td>\n",
       "      <td>0.032692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038012</td>\n",
       "      <td>0.091283</td>\n",
       "      <td>0.289207</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>-0.025078</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>0.014622</td>\n",
       "      <td>0.019580</td>\n",
       "      <td>-0.021104</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>-0.006240</td>\n",
       "      <td>-0.027728</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>GE</td>\n",
       "      <td>3.963392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997-09-30</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>-0.000273</td>\n",
       "      <td>-0.068455</td>\n",
       "      <td>-0.060606</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.140777</td>\n",
       "      <td>-0.085611</td>\n",
       "      <td>-0.207926</td>\n",
       "      <td>-0.000801</td>\n",
       "      <td>-0.172840</td>\n",
       "      <td>-0.172139</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>-0.047307</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.768413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>0.065385</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.069760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022020</td>\n",
       "      <td>0.213906</td>\n",
       "      <td>0.166832</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.125835</td>\n",
       "      <td>-0.206154</td>\n",
       "      <td>-0.012971</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>-0.177446</td>\n",
       "      <td>-0.153045</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>-0.040079</td>\n",
       "      <td>-0.006601</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.180573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1998-03-31</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>-0.152426</td>\n",
       "      <td>-0.195319</td>\n",
       "      <td>-0.183099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.693302</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.439299</td>\n",
       "      <td>-0.006349</td>\n",
       "      <td>0.651210</td>\n",
       "      <td>0.635762</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.068135</td>\n",
       "      <td>-0.001107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.043478</td>\n",
       "      <td>GE</td>\n",
       "      <td>-19.229023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1998-06-30</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024943</td>\n",
       "      <td>0.108017</td>\n",
       "      <td>0.295611</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.298246</td>\n",
       "      <td>-0.019778</td>\n",
       "      <td>-0.068110</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>-0.028083</td>\n",
       "      <td>-0.042510</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.086757</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.844755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1998-09-30</td>\n",
       "      <td>0.049213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044401</td>\n",
       "      <td>-0.037256</td>\n",
       "      <td>-0.067755</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>-0.081081</td>\n",
       "      <td>0.019115</td>\n",
       "      <td>-0.050170</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>-0.008794</td>\n",
       "      <td>-0.011779</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.127700</td>\n",
       "      <td>0.008859</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>-5.894737</td>\n",
       "      <td>GE</td>\n",
       "      <td>-9.406185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>0.063842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046146</td>\n",
       "      <td>0.186485</td>\n",
       "      <td>0.169440</td>\n",
       "      <td>0.157143</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>-0.085794</td>\n",
       "      <td>-0.047604</td>\n",
       "      <td>-0.129368</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>-0.120406</td>\n",
       "      <td>-0.106357</td>\n",
       "      <td>0.072848</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>-0.007053</td>\n",
       "      <td>0.015368</td>\n",
       "      <td>-0.032258</td>\n",
       "      <td>-0.182796</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.026139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1999-03-31</td>\n",
       "      <td>0.016298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017527</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.156162</td>\n",
       "      <td>-0.193186</td>\n",
       "      <td>-0.185185</td>\n",
       "      <td>-0.197531</td>\n",
       "      <td>0.319909</td>\n",
       "      <td>0.302978</td>\n",
       "      <td>0.341588</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.263689</td>\n",
       "      <td>0.272914</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.012303</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.644737</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.551643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999-06-30</td>\n",
       "      <td>0.018375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021086</td>\n",
       "      <td>0.134285</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.042314</td>\n",
       "      <td>0.028639</td>\n",
       "      <td>0.058880</td>\n",
       "      <td>0.011802</td>\n",
       "      <td>0.045610</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>-0.015004</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.667926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>-0.007661</td>\n",
       "      <td>-0.059220</td>\n",
       "      <td>-0.058140</td>\n",
       "      <td>-0.070588</td>\n",
       "      <td>0.035902</td>\n",
       "      <td>0.042912</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.015267</td>\n",
       "      <td>-0.002121</td>\n",
       "      <td>0.059140</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.080328</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.275862</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.562338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>0.065688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.064644</td>\n",
       "      <td>0.207904</td>\n",
       "      <td>0.164342</td>\n",
       "      <td>0.160494</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.218075</td>\n",
       "      <td>0.302229</td>\n",
       "      <td>0.117578</td>\n",
       "      <td>0.016794</td>\n",
       "      <td>0.206230</td>\n",
       "      <td>0.174814</td>\n",
       "      <td>0.071066</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.011328</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.761905</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.691119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-03-31</td>\n",
       "      <td>0.040565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.045609</td>\n",
       "      <td>-0.087019</td>\n",
       "      <td>-0.160893</td>\n",
       "      <td>-0.723404</td>\n",
       "      <td>-0.720430</td>\n",
       "      <td>0.057343</td>\n",
       "      <td>0.033666</td>\n",
       "      <td>0.090552</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017865</td>\n",
       "      <td>0.061611</td>\n",
       "      <td>-0.010960</td>\n",
       "      <td>-0.017040</td>\n",
       "      <td>-0.010417</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>-1.139241</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.505297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-06-30</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.032069</td>\n",
       "      <td>0.095546</td>\n",
       "      <td>0.303241</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.072863</td>\n",
       "      <td>0.018559</td>\n",
       "      <td>0.144468</td>\n",
       "      <td>0.011954</td>\n",
       "      <td>0.025824</td>\n",
       "      <td>0.031104</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.001809</td>\n",
       "      <td>0.024245</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.727273</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-09-30</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039630</td>\n",
       "      <td>-0.025805</td>\n",
       "      <td>-0.058615</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>-0.030303</td>\n",
       "      <td>0.061161</td>\n",
       "      <td>0.080743</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028646</td>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>-0.060303</td>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.440699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-12-31</td>\n",
       "      <td>0.013603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057535</td>\n",
       "      <td>0.092678</td>\n",
       "      <td>0.127358</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.026182</td>\n",
       "      <td>-0.009256</td>\n",
       "      <td>-0.046667</td>\n",
       "      <td>-0.003322</td>\n",
       "      <td>-0.062447</td>\n",
       "      <td>-0.071489</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.024372</td>\n",
       "      <td>0.024008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.222222</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.372789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>-0.128298</td>\n",
       "      <td>-0.282287</td>\n",
       "      <td>-0.277778</td>\n",
       "      <td>-0.277778</td>\n",
       "      <td>-0.276886</td>\n",
       "      <td>-0.306139</td>\n",
       "      <td>-0.239669</td>\n",
       "      <td>-0.034444</td>\n",
       "      <td>-0.314131</td>\n",
       "      <td>-0.305239</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.055242</td>\n",
       "      <td>-0.031415</td>\n",
       "      <td>-0.005097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.858156</td>\n",
       "      <td>GE</td>\n",
       "      <td>-8.673656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001-06-30</td>\n",
       "      <td>0.016809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025898</td>\n",
       "      <td>0.048667</td>\n",
       "      <td>0.514574</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.004906</td>\n",
       "      <td>0.192354</td>\n",
       "      <td>-0.212653</td>\n",
       "      <td>0.008055</td>\n",
       "      <td>-0.003937</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>-0.006197</td>\n",
       "      <td>-0.044684</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.627604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>-0.078463</td>\n",
       "      <td>-0.158070</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.255814</td>\n",
       "      <td>0.174974</td>\n",
       "      <td>0.066747</td>\n",
       "      <td>0.365310</td>\n",
       "      <td>-0.020548</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>0.089070</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.024726</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.028376</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>-0.469697</td>\n",
       "      <td>GE</td>\n",
       "      <td>-4.117887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2001-12-31</td>\n",
       "      <td>0.075910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022893</td>\n",
       "      <td>0.152946</td>\n",
       "      <td>0.198720</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.082003</td>\n",
       "      <td>-0.092817</td>\n",
       "      <td>-0.067151</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>-0.105869</td>\n",
       "      <td>-0.082085</td>\n",
       "      <td>0.056962</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>-0.037172</td>\n",
       "      <td>0.034253</td>\n",
       "      <td>-0.068966</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.627786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2002-03-31</td>\n",
       "      <td>0.011541</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>-0.101663</td>\n",
       "      <td>-0.363590</td>\n",
       "      <td>-0.358974</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>-0.411863</td>\n",
       "      <td>-0.416962</td>\n",
       "      <td>-0.405225</td>\n",
       "      <td>-0.024739</td>\n",
       "      <td>-0.424710</td>\n",
       "      <td>-0.428525</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.035692</td>\n",
       "      <td>0.168922</td>\n",
       "      <td>-0.005520</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.841667</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.719395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2002-06-30</td>\n",
       "      <td>0.080186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.064454</td>\n",
       "      <td>0.088234</td>\n",
       "      <td>0.768278</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.134062</td>\n",
       "      <td>0.178699</td>\n",
       "      <td>0.075701</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>0.129754</td>\n",
       "      <td>0.142204</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>-0.008666</td>\n",
       "      <td>0.252938</td>\n",
       "      <td>0.028677</td>\n",
       "      <td>-0.038462</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>GE</td>\n",
       "      <td>-4.805999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2002-09-30</td>\n",
       "      <td>0.027057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.060126</td>\n",
       "      <td>-0.018938</td>\n",
       "      <td>-0.076593</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.164643</td>\n",
       "      <td>0.146149</td>\n",
       "      <td>0.191138</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>0.095050</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.048649</td>\n",
       "      <td>-0.026428</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>0.031475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.303030</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.737625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023258</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>-0.241008</td>\n",
       "      <td>-0.392157</td>\n",
       "      <td>-0.392157</td>\n",
       "      <td>0.170500</td>\n",
       "      <td>0.106878</td>\n",
       "      <td>0.257841</td>\n",
       "      <td>-0.090769</td>\n",
       "      <td>0.103074</td>\n",
       "      <td>0.031561</td>\n",
       "      <td>0.048969</td>\n",
       "      <td>0.075172</td>\n",
       "      <td>0.025361</td>\n",
       "      <td>-0.065388</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.424245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>0.014585</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041629</td>\n",
       "      <td>-0.142998</td>\n",
       "      <td>-0.033204</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>-0.231334</td>\n",
       "      <td>-0.251912</td>\n",
       "      <td>-0.206437</td>\n",
       "      <td>-0.011045</td>\n",
       "      <td>-0.244262</td>\n",
       "      <td>-0.185666</td>\n",
       "      <td>0.046683</td>\n",
       "      <td>-0.019227</td>\n",
       "      <td>0.062922</td>\n",
       "      <td>0.036381</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>-0.819149</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.502775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2003-06-30</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.084541</td>\n",
       "      <td>0.100729</td>\n",
       "      <td>0.265088</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.010907</td>\n",
       "      <td>0.035783</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>-0.091065</td>\n",
       "      <td>-0.030369</td>\n",
       "      <td>-0.033968</td>\n",
       "      <td>0.044601</td>\n",
       "      <td>0.061188</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>-0.044104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.529412</td>\n",
       "      <td>GE</td>\n",
       "      <td>-3.403267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2003-09-30</td>\n",
       "      <td>0.019640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007128</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>-0.038218</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.052632</td>\n",
       "      <td>-0.036413</td>\n",
       "      <td>-0.023442</td>\n",
       "      <td>-0.052045</td>\n",
       "      <td>-0.067108</td>\n",
       "      <td>-0.109620</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.042697</td>\n",
       "      <td>0.048330</td>\n",
       "      <td>-0.042882</td>\n",
       "      <td>-0.037665</td>\n",
       "      <td>-0.043478</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.721103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>0.032779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.092424</td>\n",
       "      <td>0.111008</td>\n",
       "      <td>0.249657</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>-0.137509</td>\n",
       "      <td>-0.115603</td>\n",
       "      <td>-0.164706</td>\n",
       "      <td>0.048126</td>\n",
       "      <td>-0.143216</td>\n",
       "      <td>-0.043085</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>-0.083304</td>\n",
       "      <td>-0.044714</td>\n",
       "      <td>0.093933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>GE</td>\n",
       "      <td>2.609417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2004-03-31</td>\n",
       "      <td>0.022584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.092271</td>\n",
       "      <td>-0.101102</td>\n",
       "      <td>-0.289474</td>\n",
       "      <td>-0.360000</td>\n",
       "      <td>-0.346939</td>\n",
       "      <td>0.427992</td>\n",
       "      <td>0.348214</td>\n",
       "      <td>0.532864</td>\n",
       "      <td>-0.049783</td>\n",
       "      <td>0.313783</td>\n",
       "      <td>0.262368</td>\n",
       "      <td>0.041322</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>-0.087374</td>\n",
       "      <td>-0.006261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.388889</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.044974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2004-06-30</td>\n",
       "      <td>0.052830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.136392</td>\n",
       "      <td>0.110495</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>-0.063068</td>\n",
       "      <td>-0.085298</td>\n",
       "      <td>-0.037672</td>\n",
       "      <td>-0.070193</td>\n",
       "      <td>-0.131696</td>\n",
       "      <td>-0.062968</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>-0.087880</td>\n",
       "      <td>-0.018002</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>-0.072727</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.361881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>0.010809</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035561</td>\n",
       "      <td>0.033401</td>\n",
       "      <td>0.032365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.044269</td>\n",
       "      <td>-0.030119</td>\n",
       "      <td>-0.059516</td>\n",
       "      <td>-0.056346</td>\n",
       "      <td>-0.128535</td>\n",
       "      <td>-0.044173</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>-0.000381</td>\n",
       "      <td>0.024338</td>\n",
       "      <td>-0.008249</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>GE</td>\n",
       "      <td>-6.229696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>0.065123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.062703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.088861</td>\n",
       "      <td>0.155126</td>\n",
       "      <td>0.383362</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>0.032248</td>\n",
       "      <td>-0.022673</td>\n",
       "      <td>-0.018551</td>\n",
       "      <td>-0.029499</td>\n",
       "      <td>-0.006391</td>\n",
       "      <td>0.040441</td>\n",
       "      <td>-0.026276</td>\n",
       "      <td>0.048176</td>\n",
       "      <td>0.016636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.828367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2005-03-31</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>-0.101405</td>\n",
       "      <td>-0.292470</td>\n",
       "      <td>-0.301887</td>\n",
       "      <td>-0.288462</td>\n",
       "      <td>0.087614</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.131233</td>\n",
       "      <td>-0.021855</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.067293</td>\n",
       "      <td>0.038869</td>\n",
       "      <td>-0.008995</td>\n",
       "      <td>-0.016936</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.398148</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.394091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2005-06-30</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004323</td>\n",
       "      <td>0.046066</td>\n",
       "      <td>0.172005</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>-0.005795</td>\n",
       "      <td>-0.015410</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>-0.024242</td>\n",
       "      <td>-0.036161</td>\n",
       "      <td>0.037415</td>\n",
       "      <td>-0.015785</td>\n",
       "      <td>0.035834</td>\n",
       "      <td>0.011797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.646154</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.131659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2005-09-30</td>\n",
       "      <td>-0.104935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.115363</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0.008928</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041970</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.039574</td>\n",
       "      <td>0.008388</td>\n",
       "      <td>0.046584</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>0.036066</td>\n",
       "      <td>-0.010826</td>\n",
       "      <td>-0.014380</td>\n",
       "      <td>0.011659</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>3.434783</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.263720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>0.016070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.032129</td>\n",
       "      <td>-0.355236</td>\n",
       "      <td>-0.268334</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.272727</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>-0.012051</td>\n",
       "      <td>0.023426</td>\n",
       "      <td>-0.112894</td>\n",
       "      <td>-0.002967</td>\n",
       "      <td>-0.029651</td>\n",
       "      <td>0.039557</td>\n",
       "      <td>0.163762</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>-0.014184</td>\n",
       "      <td>-0.086957</td>\n",
       "      <td>-0.147059</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.506192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2006-03-31</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016699</td>\n",
       "      <td>0.406762</td>\n",
       "      <td>0.297487</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.043372</td>\n",
       "      <td>-0.009442</td>\n",
       "      <td>0.040857</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.147363</td>\n",
       "      <td>0.038052</td>\n",
       "      <td>-0.000522</td>\n",
       "      <td>0.068202</td>\n",
       "      <td>0.040468</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>-0.597701</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.544246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2006-06-30</td>\n",
       "      <td>-0.018416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.024725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011783</td>\n",
       "      <td>0.053091</td>\n",
       "      <td>0.113964</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>-0.073598</td>\n",
       "      <td>-0.073785</td>\n",
       "      <td>-0.073946</td>\n",
       "      <td>0.025740</td>\n",
       "      <td>-0.060345</td>\n",
       "      <td>-0.085052</td>\n",
       "      <td>0.036657</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.050248</td>\n",
       "      <td>0.027658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.917277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2006-09-30</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023807</td>\n",
       "      <td>0.016106</td>\n",
       "      <td>-0.015973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.004430</td>\n",
       "      <td>-0.011501</td>\n",
       "      <td>0.022458</td>\n",
       "      <td>0.015056</td>\n",
       "      <td>-0.012232</td>\n",
       "      <td>-0.014554</td>\n",
       "      <td>0.035361</td>\n",
       "      <td>0.013834</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.019344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>GE</td>\n",
       "      <td>14.458542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2006-12-31</td>\n",
       "      <td>0.022247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.096528</td>\n",
       "      <td>0.351140</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>-0.017389</td>\n",
       "      <td>0.169963</td>\n",
       "      <td>-0.027864</td>\n",
       "      <td>-0.026679</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>-0.128944</td>\n",
       "      <td>0.064695</td>\n",
       "      <td>0.051980</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.211268</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.387398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-03-31</td>\n",
       "      <td>0.024154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016516</td>\n",
       "      <td>-0.099191</td>\n",
       "      <td>-0.314477</td>\n",
       "      <td>-0.301587</td>\n",
       "      <td>-0.323077</td>\n",
       "      <td>0.153007</td>\n",
       "      <td>0.182992</td>\n",
       "      <td>0.119839</td>\n",
       "      <td>-0.011622</td>\n",
       "      <td>0.140127</td>\n",
       "      <td>-0.038179</td>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.023106</td>\n",
       "      <td>0.037552</td>\n",
       "      <td>-0.010196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.339286</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.115711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2007-06-30</td>\n",
       "      <td>0.034244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024998</td>\n",
       "      <td>0.052768</td>\n",
       "      <td>0.202307</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>-0.001898</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>-0.013966</td>\n",
       "      <td>-0.023410</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>GE</td>\n",
       "      <td>-11.806498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2007-09-30</td>\n",
       "      <td>0.031377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.044029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.035822</td>\n",
       "      <td>0.005152</td>\n",
       "      <td>0.021956</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>-0.050588</td>\n",
       "      <td>-0.054671</td>\n",
       "      <td>-0.045580</td>\n",
       "      <td>0.028206</td>\n",
       "      <td>-0.079320</td>\n",
       "      <td>-0.077645</td>\n",
       "      <td>0.034314</td>\n",
       "      <td>-0.008772</td>\n",
       "      <td>0.141444</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>-0.126126</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.560726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2007-12-31</td>\n",
       "      <td>0.044152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024178</td>\n",
       "      <td>0.121291</td>\n",
       "      <td>0.217007</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>-0.028794</td>\n",
       "      <td>-0.037465</td>\n",
       "      <td>-0.018813</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>-0.006154</td>\n",
       "      <td>-0.055932</td>\n",
       "      <td>0.036730</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>-0.010008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.385076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008-03-31</td>\n",
       "      <td>0.048474</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>-0.113644</td>\n",
       "      <td>-0.361519</td>\n",
       "      <td>-0.371429</td>\n",
       "      <td>-0.362319</td>\n",
       "      <td>-0.469105</td>\n",
       "      <td>-0.327325</td>\n",
       "      <td>-0.628909</td>\n",
       "      <td>-0.012933</td>\n",
       "      <td>-0.486068</td>\n",
       "      <td>-0.478755</td>\n",
       "      <td>0.035429</td>\n",
       "      <td>0.027392</td>\n",
       "      <td>0.082591</td>\n",
       "      <td>-0.020995</td>\n",
       "      <td>-0.043478</td>\n",
       "      <td>-0.452174</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.805806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2008-06-30</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.109242</td>\n",
       "      <td>0.178439</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.371086</td>\n",
       "      <td>0.180194</td>\n",
       "      <td>0.761526</td>\n",
       "      <td>-0.018868</td>\n",
       "      <td>0.361446</td>\n",
       "      <td>0.370838</td>\n",
       "      <td>0.034216</td>\n",
       "      <td>0.034438</td>\n",
       "      <td>-0.007194</td>\n",
       "      <td>-0.041303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2008-09-30</td>\n",
       "      <td>-0.020588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.051180</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>-0.149842</td>\n",
       "      <td>-0.156863</td>\n",
       "      <td>-0.156863</td>\n",
       "      <td>0.231062</td>\n",
       "      <td>0.267522</td>\n",
       "      <td>0.180054</td>\n",
       "      <td>-0.055556</td>\n",
       "      <td>0.203540</td>\n",
       "      <td>0.242044</td>\n",
       "      <td>0.033084</td>\n",
       "      <td>0.082513</td>\n",
       "      <td>-0.010111</td>\n",
       "      <td>-0.080365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.032967</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.430264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>-0.038311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.033932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.068212</td>\n",
       "      <td>-0.023648</td>\n",
       "      <td>-0.136827</td>\n",
       "      <td>-0.023256</td>\n",
       "      <td>-0.186047</td>\n",
       "      <td>0.072975</td>\n",
       "      <td>-0.020249</td>\n",
       "      <td>0.210325</td>\n",
       "      <td>-0.127262</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.130142</td>\n",
       "      <td>0.032025</td>\n",
       "      <td>0.190311</td>\n",
       "      <td>0.073712</td>\n",
       "      <td>-0.140541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>GE</td>\n",
       "      <td>4.390503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2009-03-31</td>\n",
       "      <td>-0.046332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.047980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.034835</td>\n",
       "      <td>-0.167097</td>\n",
       "      <td>-0.244761</td>\n",
       "      <td>-0.380952</td>\n",
       "      <td>-0.257143</td>\n",
       "      <td>-0.553026</td>\n",
       "      <td>-0.552994</td>\n",
       "      <td>-0.552923</td>\n",
       "      <td>-0.058976</td>\n",
       "      <td>-0.490260</td>\n",
       "      <td>-0.464797</td>\n",
       "      <td>0.031031</td>\n",
       "      <td>0.115849</td>\n",
       "      <td>0.028507</td>\n",
       "      <td>-0.070231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.092391</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.325326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2009-06-30</td>\n",
       "      <td>0.022819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.109880</td>\n",
       "      <td>0.017469</td>\n",
       "      <td>-0.049804</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>-0.096712</td>\n",
       "      <td>0.038530</td>\n",
       "      <td>-0.257951</td>\n",
       "      <td>-0.143939</td>\n",
       "      <td>-0.063694</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>-0.066656</td>\n",
       "      <td>-0.118377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.058824</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.618057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.048252</td>\n",
       "      <td>-0.032828</td>\n",
       "      <td>-0.066267</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>-0.041667</td>\n",
       "      <td>-0.130621</td>\n",
       "      <td>-0.169521</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>-0.152051</td>\n",
       "      <td>-0.217687</td>\n",
       "      <td>0.053097</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>-0.054938</td>\n",
       "      <td>0.006441</td>\n",
       "      <td>-0.090793</td>\n",
       "      <td>-0.045455</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.312684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>-0.007651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002025</td>\n",
       "      <td>0.097675</td>\n",
       "      <td>0.222534</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>-0.051724</td>\n",
       "      <td>0.184880</td>\n",
       "      <td>-0.401020</td>\n",
       "      <td>-0.091082</td>\n",
       "      <td>-0.086957</td>\n",
       "      <td>0.122899</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>-0.202140</td>\n",
       "      <td>-0.053564</td>\n",
       "      <td>-0.037975</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.471803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>-0.005708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.017870</td>\n",
       "      <td>-0.117760</td>\n",
       "      <td>-0.362086</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>-0.392857</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.098608</td>\n",
       "      <td>1.580920</td>\n",
       "      <td>-0.109603</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.579046</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>-0.284153</td>\n",
       "      <td>-0.043869</td>\n",
       "      <td>-0.070175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.303030</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.350600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>-0.035280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.038507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010947</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.598458</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>-0.004106</td>\n",
       "      <td>0.040127</td>\n",
       "      <td>-0.058086</td>\n",
       "      <td>0.041032</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.093602</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>-0.041175</td>\n",
       "      <td>-0.049110</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>GE</td>\n",
       "      <td>-3.504736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>0.011860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014061</td>\n",
       "      <td>-0.041555</td>\n",
       "      <td>-0.339016</td>\n",
       "      <td>-0.357143</td>\n",
       "      <td>-0.357143</td>\n",
       "      <td>-0.103651</td>\n",
       "      <td>-0.152284</td>\n",
       "      <td>-0.036440</td>\n",
       "      <td>-0.038288</td>\n",
       "      <td>-0.089172</td>\n",
       "      <td>-0.141387</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.096984</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061728</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.838943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>-0.010026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.017543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029428</td>\n",
       "      <td>0.122214</td>\n",
       "      <td>1.206813</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>0.120894</td>\n",
       "      <td>0.107186</td>\n",
       "      <td>0.136727</td>\n",
       "      <td>0.146370</td>\n",
       "      <td>0.097902</td>\n",
       "      <td>0.182965</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>-0.049703</td>\n",
       "      <td>-0.073449</td>\n",
       "      <td>0.159754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.081395</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.253457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>-0.031122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.039823</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.038668</td>\n",
       "      <td>-0.045339</td>\n",
       "      <td>-0.242999</td>\n",
       "      <td>-0.255814</td>\n",
       "      <td>-0.279070</td>\n",
       "      <td>0.165299</td>\n",
       "      <td>0.170903</td>\n",
       "      <td>0.159309</td>\n",
       "      <td>0.111338</td>\n",
       "      <td>0.133758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>-0.040731</td>\n",
       "      <td>-0.055875</td>\n",
       "      <td>0.117881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.253165</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.108052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>0.015516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036743</td>\n",
       "      <td>-0.073424</td>\n",
       "      <td>0.096417</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>-0.023642</td>\n",
       "      <td>-0.036952</td>\n",
       "      <td>-0.008278</td>\n",
       "      <td>0.020221</td>\n",
       "      <td>-0.061798</td>\n",
       "      <td>-0.137067</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.044632</td>\n",
       "      <td>-0.068244</td>\n",
       "      <td>0.063981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.067797</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.880700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>-0.001935</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028210</td>\n",
       "      <td>-0.007242</td>\n",
       "      <td>-0.143464</td>\n",
       "      <td>-0.371429</td>\n",
       "      <td>-0.371429</td>\n",
       "      <td>-0.117465</td>\n",
       "      <td>-0.063309</td>\n",
       "      <td>-0.180857</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>-0.149701</td>\n",
       "      <td>-0.166255</td>\n",
       "      <td>0.013216</td>\n",
       "      <td>0.024711</td>\n",
       "      <td>-0.006453</td>\n",
       "      <td>0.030067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>GE</td>\n",
       "      <td>-19.783475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>-0.027732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.064462</td>\n",
       "      <td>0.070489</td>\n",
       "      <td>0.156948</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>-0.057210</td>\n",
       "      <td>-0.064004</td>\n",
       "      <td>-0.047554</td>\n",
       "      <td>-0.047364</td>\n",
       "      <td>-0.035211</td>\n",
       "      <td>-0.085990</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>0.109308</td>\n",
       "      <td>-0.029970</td>\n",
       "      <td>-0.036757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.123288</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.765448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>-0.013277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.021661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029449</td>\n",
       "      <td>-0.070734</td>\n",
       "      <td>-0.186595</td>\n",
       "      <td>-0.171429</td>\n",
       "      <td>-0.171429</td>\n",
       "      <td>0.214861</td>\n",
       "      <td>0.148796</td>\n",
       "      <td>0.300285</td>\n",
       "      <td>-0.017824</td>\n",
       "      <td>0.299270</td>\n",
       "      <td>0.293593</td>\n",
       "      <td>0.014567</td>\n",
       "      <td>0.075376</td>\n",
       "      <td>-0.068392</td>\n",
       "      <td>-0.003367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.270828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>-0.019214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.027142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.037491</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009684</td>\n",
       "      <td>-0.007619</td>\n",
       "      <td>-0.011519</td>\n",
       "      <td>-0.029608</td>\n",
       "      <td>-0.033708</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.014358</td>\n",
       "      <td>0.079350</td>\n",
       "      <td>-0.017814</td>\n",
       "      <td>-0.050676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.063830</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.334144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2012-09-30</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020255</td>\n",
       "      <td>-0.004164</td>\n",
       "      <td>0.124316</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.089038</td>\n",
       "      <td>0.101727</td>\n",
       "      <td>0.074362</td>\n",
       "      <td>0.098425</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.145704</td>\n",
       "      <td>0.014155</td>\n",
       "      <td>-0.059688</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.086595</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.636916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>-0.019391</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.024535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.081928</td>\n",
       "      <td>0.148954</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.017013</td>\n",
       "      <td>0.009582</td>\n",
       "      <td>0.026343</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>-0.071739</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.596069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>-0.021359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.027328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>-0.109772</td>\n",
       "      <td>-0.120668</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>-0.105263</td>\n",
       "      <td>0.035781</td>\n",
       "      <td>0.031061</td>\n",
       "      <td>0.040765</td>\n",
       "      <td>0.027580</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.015360</td>\n",
       "      <td>-0.013356</td>\n",
       "      <td>-0.013080</td>\n",
       "      <td>0.036717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.662338</td>\n",
       "      <td>GE</td>\n",
       "      <td>-5.020961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>-0.015132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.018296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009740</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>-0.111710</td>\n",
       "      <td>-0.117647</td>\n",
       "      <td>-0.117647</td>\n",
       "      <td>0.021983</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>-0.002597</td>\n",
       "      <td>0.010638</td>\n",
       "      <td>-0.016204</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.016544</td>\n",
       "      <td>-0.003168</td>\n",
       "      <td>0.011458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.076923</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.779055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.017140</td>\n",
       "      <td>0.018513</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.047410</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.078162</td>\n",
       "      <td>-0.021701</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.039412</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.020552</td>\n",
       "      <td>-0.016478</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>-0.296296</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.330205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>-0.007375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.023697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.064177</td>\n",
       "      <td>0.124899</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.081308</td>\n",
       "      <td>0.125852</td>\n",
       "      <td>0.032513</td>\n",
       "      <td>-0.071872</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.097906</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>0.093156</td>\n",
       "      <td>-0.081878</td>\n",
       "      <td>-0.063874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.342105</td>\n",
       "      <td>GE</td>\n",
       "      <td>4.865892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>-0.006561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>-0.149526</td>\n",
       "      <td>-0.064566</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.012791</td>\n",
       "      <td>-0.005340</td>\n",
       "      <td>0.034894</td>\n",
       "      <td>-0.056405</td>\n",
       "      <td>-0.056338</td>\n",
       "      <td>0.060309</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>0.073508</td>\n",
       "      <td>-0.012605</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.685393</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.080618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>-0.000587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016513</td>\n",
       "      <td>0.060126</td>\n",
       "      <td>0.182061</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.013395</td>\n",
       "      <td>-0.014674</td>\n",
       "      <td>0.045641</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046670</td>\n",
       "      <td>0.016442</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>-0.034719</td>\n",
       "      <td>0.024334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>GE</td>\n",
       "      <td>-4.105823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>-0.002835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.010340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007350</td>\n",
       "      <td>-0.001628</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020015</td>\n",
       "      <td>-0.013803</td>\n",
       "      <td>-0.026740</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>-0.034826</td>\n",
       "      <td>-0.058523</td>\n",
       "      <td>0.016176</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>-0.024102</td>\n",
       "      <td>0.023756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.433754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>-0.002572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.050660</td>\n",
       "      <td>0.161165</td>\n",
       "      <td>0.456602</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-0.021195</td>\n",
       "      <td>-0.001842</td>\n",
       "      <td>-0.042828</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>-0.025773</td>\n",
       "      <td>-0.050814</td>\n",
       "      <td>0.016643</td>\n",
       "      <td>-0.117213</td>\n",
       "      <td>-0.009689</td>\n",
       "      <td>0.132597</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1.979592</td>\n",
       "      <td>GE</td>\n",
       "      <td>2.487301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>-0.047379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.021862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.152927</td>\n",
       "      <td>-0.301114</td>\n",
       "      <td>-3.634511</td>\n",
       "      <td>-3.647059</td>\n",
       "      <td>-3.700000</td>\n",
       "      <td>-0.022047</td>\n",
       "      <td>-0.030627</td>\n",
       "      <td>-0.011819</td>\n",
       "      <td>-1.092014</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>-0.139293</td>\n",
       "      <td>0.016370</td>\n",
       "      <td>-0.370536</td>\n",
       "      <td>0.121691</td>\n",
       "      <td>-0.461402</td>\n",
       "      <td>-0.043478</td>\n",
       "      <td>-0.712329</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.065693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>-0.010856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.015362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008686</td>\n",
       "      <td>0.079677</td>\n",
       "      <td>-0.899801</td>\n",
       "      <td>-0.903704</td>\n",
       "      <td>-0.903704</td>\n",
       "      <td>0.072061</td>\n",
       "      <td>0.091740</td>\n",
       "      <td>0.049551</td>\n",
       "      <td>3.896226</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>-0.347864</td>\n",
       "      <td>0.016106</td>\n",
       "      <td>-0.588654</td>\n",
       "      <td>-0.035004</td>\n",
       "      <td>-0.856674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.854232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2015-09-30</td>\n",
       "      <td>-0.048477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.063595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015543</td>\n",
       "      <td>-0.000473</td>\n",
       "      <td>-2.842647</td>\n",
       "      <td>-2.923077</td>\n",
       "      <td>-2.923077</td>\n",
       "      <td>-0.123169</td>\n",
       "      <td>-0.047071</td>\n",
       "      <td>-0.211640</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>-0.129555</td>\n",
       "      <td>-0.533422</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043717</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.950820</td>\n",
       "      <td>GE</td>\n",
       "      <td>9.186159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>-0.152445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.155500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.116281</td>\n",
       "      <td>-0.221749</td>\n",
       "      <td>1.514366</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>0.205139</td>\n",
       "      <td>0.152214</td>\n",
       "      <td>0.279814</td>\n",
       "      <td>-0.095912</td>\n",
       "      <td>0.190698</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.086551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>GE</td>\n",
       "      <td>-1.811231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>-0.061903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.059497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.073132</td>\n",
       "      <td>0.129386</td>\n",
       "      <td>-0.969687</td>\n",
       "      <td>-1.015385</td>\n",
       "      <td>-1.015152</td>\n",
       "      <td>0.050817</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>0.093183</td>\n",
       "      <td>-2.243478</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018463</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-1.132075</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.774597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>-0.131400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.149276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.066936</td>\n",
       "      <td>0.202873</td>\n",
       "      <td>14.130890</td>\n",
       "      <td>-31.000000</td>\n",
       "      <td>-31.000000</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>0.030265</td>\n",
       "      <td>0.072325</td>\n",
       "      <td>0.658741</td>\n",
       "      <td>0.117438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>-0.377355</td>\n",
       "      <td>-0.006753</td>\n",
       "      <td>0.533965</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.069506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>-0.034292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.034038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.036569</td>\n",
       "      <td>-0.126232</td>\n",
       "      <td>-0.298616</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-0.266667</td>\n",
       "      <td>-0.012081</td>\n",
       "      <td>-0.009691</td>\n",
       "      <td>-0.014969</td>\n",
       "      <td>0.033727</td>\n",
       "      <td>0.020701</td>\n",
       "      <td>-0.178864</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>-0.019840</td>\n",
       "      <td>-0.024717</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-1.282609</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.247285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>-0.058064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.054567</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.073936</td>\n",
       "      <td>0.130595</td>\n",
       "      <td>0.836704</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>-0.012229</td>\n",
       "      <td>-0.009786</td>\n",
       "      <td>-0.015197</td>\n",
       "      <td>-0.200653</td>\n",
       "      <td>0.020281</td>\n",
       "      <td>-0.217825</td>\n",
       "      <td>0.015326</td>\n",
       "      <td>0.326672</td>\n",
       "      <td>-0.019110</td>\n",
       "      <td>-0.302006</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>-0.538462</td>\n",
       "      <td>GE</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>-0.037077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.043040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.017065</td>\n",
       "      <td>-0.164047</td>\n",
       "      <td>-0.824604</td>\n",
       "      <td>-0.825000</td>\n",
       "      <td>-0.820513</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>-0.016677</td>\n",
       "      <td>0.037602</td>\n",
       "      <td>0.143878</td>\n",
       "      <td>0.076453</td>\n",
       "      <td>0.310931</td>\n",
       "      <td>0.015094</td>\n",
       "      <td>-0.083981</td>\n",
       "      <td>-0.034995</td>\n",
       "      <td>0.089259</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>-1.666667</td>\n",
       "      <td>GE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.005179</td>\n",
       "      <td>0.068619</td>\n",
       "      <td>1.093415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-0.061866</td>\n",
       "      <td>-0.040829</td>\n",
       "      <td>-0.084103</td>\n",
       "      <td>-0.145406</td>\n",
       "      <td>-0.051136</td>\n",
       "      <td>-0.138480</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.213994</td>\n",
       "      <td>0.045536</td>\n",
       "      <td>-0.147222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.277967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>0.063479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026393</td>\n",
       "      <td>0.132418</td>\n",
       "      <td>0.343087</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>-0.107118</td>\n",
       "      <td>-0.096595</td>\n",
       "      <td>-0.119821</td>\n",
       "      <td>-0.007307</td>\n",
       "      <td>-0.104790</td>\n",
       "      <td>0.080369</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>0.032935</td>\n",
       "      <td>0.010799</td>\n",
       "      <td>-0.060261</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.459993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>-0.000246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.155601</td>\n",
       "      <td>-0.061843</td>\n",
       "      <td>-6.251634</td>\n",
       "      <td>-6.428571</td>\n",
       "      <td>-6.428571</td>\n",
       "      <td>-0.176631</td>\n",
       "      <td>-0.097862</td>\n",
       "      <td>-0.268448</td>\n",
       "      <td>-1.905363</td>\n",
       "      <td>-0.197324</td>\n",
       "      <td>-0.166192</td>\n",
       "      <td>0.007220</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.195486</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.995147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quarter end    Assets  Current Assets  Liabilities  Current Liabilities  \\\n",
       "0   1996-03-31  0.006587             NaN     0.005454                  NaN   \n",
       "1   1996-06-30  0.051552             NaN     0.059943                  NaN   \n",
       "2   1996-09-30  0.029333             NaN     0.029985                  NaN   \n",
       "3   1996-12-31  0.093185             NaN     0.102438                  NaN   \n",
       "4   1997-03-31 -0.008568             NaN    -0.007487                  NaN   \n",
       "5   1997-06-30  0.032692             NaN     0.032370                  NaN   \n",
       "6   1997-09-30  0.023152             NaN     0.018072                  NaN   \n",
       "7   1997-12-31  0.065385             NaN     0.069760                  NaN   \n",
       "8   1998-03-31  0.018887             NaN     0.020046                  NaN   \n",
       "9   1998-06-30  0.029469             NaN     0.029920                  NaN   \n",
       "10  1998-09-30  0.049213             NaN     0.049836                  NaN   \n",
       "11  1998-12-31  0.063842             NaN     0.066569                  NaN   \n",
       "12  1999-03-31  0.016298             NaN     0.017527                  NaN   \n",
       "13  1999-06-30  0.018375             NaN     0.016961                  NaN   \n",
       "14  1999-09-30  0.032143             NaN     0.035228                  NaN   \n",
       "15  1999-12-31  0.065688             NaN     0.066755                  NaN   \n",
       "16  2000-03-31  0.040565             NaN     0.041477                  NaN   \n",
       "17  2000-06-30  0.005699             NaN     0.002493                  NaN   \n",
       "18  2000-09-30  0.016746             NaN     0.014299                  NaN   \n",
       "19  2000-12-31  0.013603             NaN     0.008084                  NaN   \n",
       "20  2001-03-31  0.002240             NaN     0.001475                  NaN   \n",
       "21  2001-06-30  0.016809             NaN     0.015897                  NaN   \n",
       "22  2001-09-30  0.033120             NaN     0.034574                  NaN   \n",
       "23  2001-12-31  0.075910             NaN     0.083025                  NaN   \n",
       "24  2002-03-31  0.011541             NaN     0.012322                  NaN   \n",
       "25  2002-06-30  0.080186             NaN     0.082881                  NaN   \n",
       "26  2002-09-30  0.027057             NaN     0.023041                  NaN   \n",
       "27  2002-12-31  0.035500             NaN     0.037386                  NaN   \n",
       "28  2003-03-31  0.014585             NaN     0.011259                  NaN   \n",
       "29  2003-06-30  0.053498             NaN     0.050160                  NaN   \n",
       "30  2003-09-30  0.019640             NaN     0.020892                  NaN   \n",
       "31  2003-12-31  0.032779             NaN     0.025265                  NaN   \n",
       "32  2004-03-31  0.022584             NaN     0.012099                  NaN   \n",
       "33  2004-06-30  0.052830             NaN     0.022382                  NaN   \n",
       "34  2004-09-30  0.010809             NaN     0.007652                  NaN   \n",
       "35  2004-12-31  0.065123             NaN     0.062703                  NaN   \n",
       "36  2005-03-31  0.002286             NaN    -0.004075                  NaN   \n",
       "37  2005-06-30 -0.015768             NaN    -0.016088                  NaN   \n",
       "38  2005-09-30 -0.104935             NaN    -0.115363                  NaN   \n",
       "39  2005-12-31  0.016070             NaN     0.028874                  NaN   \n",
       "40  2006-03-31  0.002293             NaN     0.005902                  NaN   \n",
       "41  2006-06-30 -0.018416             NaN    -0.024725                  NaN   \n",
       "42  2006-09-30  0.029630             NaN     0.031357                  NaN   \n",
       "43  2006-12-31  0.022247             NaN     0.026446                  NaN   \n",
       "44  2007-03-31  0.024154             NaN     0.025482                  NaN   \n",
       "45  2007-06-30  0.034244             NaN     0.037598                  NaN   \n",
       "46  2007-09-30  0.031377             NaN     0.044029                  NaN   \n",
       "47  2007-12-31  0.044152             NaN     0.047410                  NaN   \n",
       "48  2008-03-31  0.048474             NaN     0.056391                  NaN   \n",
       "49  2008-06-30  0.015707             NaN     0.014306                  NaN   \n",
       "50  2008-09-30 -0.020588             NaN    -0.016145                  NaN   \n",
       "51  2008-12-31 -0.038311             NaN    -0.033932                  NaN   \n",
       "52  2009-03-31 -0.046332             NaN    -0.047980                  NaN   \n",
       "53  2009-06-30  0.022819             NaN     0.009792                  NaN   \n",
       "54  2009-09-30  0.012437             NaN     0.006580                  NaN   \n",
       "55  2009-12-31 -0.007651             NaN    -0.008089                  NaN   \n",
       "56  2010-03-31 -0.005708             NaN    -0.003469                  NaN   \n",
       "57  2010-06-30 -0.035280             NaN    -0.038507                  NaN   \n",
       "58  2010-09-30  0.011860             NaN     0.014318                  NaN   \n",
       "59  2010-12-31 -0.010026             NaN    -0.017543                  NaN   \n",
       "60  2011-03-31 -0.031122             NaN    -0.039823                  NaN   \n",
       "61  2011-06-30  0.015516             NaN     0.011104                  NaN   \n",
       "62  2011-09-30 -0.001935             NaN     0.003604                  NaN   \n",
       "63  2011-12-31 -0.027732             NaN    -0.019346                  NaN   \n",
       "64  2012-03-31 -0.013277             NaN    -0.021661                  NaN   \n",
       "65  2012-06-30 -0.019214             NaN    -0.027142                  NaN   \n",
       "66  2012-09-30  0.006856             NaN     0.001126                  NaN   \n",
       "67  2012-12-31 -0.019391             NaN    -0.024535                  NaN   \n",
       "68  2013-03-31 -0.021359             NaN    -0.027328                  NaN   \n",
       "69  2013-06-30 -0.015132             NaN    -0.018296                  NaN   \n",
       "70  2013-09-30  0.001358             NaN     0.001247                  NaN   \n",
       "71  2013-12-31 -0.007375             NaN    -0.023697                  NaN   \n",
       "72  2014-03-31 -0.006561             NaN    -0.010666                  NaN   \n",
       "73  2014-06-30 -0.000587             NaN    -0.004727                  NaN   \n",
       "74  2014-09-30 -0.002835             NaN    -0.010340                  NaN   \n",
       "75  2014-12-31 -0.002572             NaN     0.009883                  NaN   \n",
       "76  2015-03-31 -0.047379             NaN    -0.021862                  NaN   \n",
       "77  2015-06-30 -0.010856             NaN    -0.015362                  NaN   \n",
       "78  2015-09-30 -0.048477             NaN    -0.063595                  NaN   \n",
       "79  2015-12-31 -0.152445             NaN    -0.155500                  NaN   \n",
       "80  2016-03-31 -0.061903             NaN    -0.059497                  NaN   \n",
       "81  2016-06-30 -0.131400             NaN    -0.149276                  NaN   \n",
       "82  2016-09-30 -0.034292             NaN    -0.034038                  NaN   \n",
       "83  2016-12-31 -0.058064             NaN    -0.054567                  NaN   \n",
       "84  2017-03-31 -0.037077             NaN    -0.043040                  NaN   \n",
       "85  2017-06-30  0.010892             NaN     0.014984                  NaN   \n",
       "86  2017-09-30  0.063479             NaN     0.014633                  NaN   \n",
       "87  2017-12-31 -0.000246             NaN     0.042835                  NaN   \n",
       "\n",
       "    Shareholders equity   Revenue   Earnings  EPS basic  EPS diluted  \\\n",
       "0              0.010313  0.014527   0.068033  -0.456613    -0.388844   \n",
       "1              0.000854  0.115101   0.257746   0.263736    -1.007812   \n",
       "2              0.023996  0.050089  -0.062893  -0.052174     1.000000   \n",
       "3              0.037535  0.148494   0.156040   0.146789     1.000000   \n",
       "4             -0.021237 -0.123380  -0.188679  -0.592000     1.000000   \n",
       "5              0.038012  0.091283   0.289207   0.294118     1.000000   \n",
       "6              0.065587 -0.000273  -0.068455  -0.060606     1.000000   \n",
       "7              0.022020  0.213906   0.166832   0.145161     1.000000   \n",
       "8              0.008160 -0.152426  -0.195319  -0.183099     1.000000   \n",
       "9              0.024943  0.108017   0.295611   0.293103     0.298246   \n",
       "10             0.044401 -0.037256  -0.067755  -0.066667    -0.081081   \n",
       "11             0.046146  0.186485   0.169440   0.157143     0.191176   \n",
       "12            -0.002238 -0.156162  -0.193186  -0.185185    -0.197531   \n",
       "13             0.021086  0.134285   0.308585   0.303030     0.307692   \n",
       "14             0.009139 -0.007661  -0.059220  -0.058140    -0.070588   \n",
       "15             0.064644  0.207904   0.164342   0.160494     0.177215   \n",
       "16             0.045609 -0.087019  -0.160893  -0.723404    -0.720430   \n",
       "17             0.032069  0.095546   0.303241   0.307692     0.269231   \n",
       "18             0.039630 -0.025805  -0.058615  -0.029412    -0.030303   \n",
       "19             0.057535  0.092678   0.127358   0.090909     0.125000   \n",
       "20             0.008675 -0.128298  -0.282287  -0.277778    -0.277778   \n",
       "21             0.025898  0.048667   0.514574   0.692308     0.653846   \n",
       "22             0.025800 -0.078463  -0.158070  -0.250000    -0.255814   \n",
       "23             0.022893  0.152946   0.198720   0.181818     0.250000   \n",
       "24             0.006329 -0.101663  -0.363590  -0.358974    -0.375000   \n",
       "25             0.064454  0.088234   0.768278   0.800000     0.760000   \n",
       "26             0.060126 -0.018938  -0.076593   0.133333     0.159091   \n",
       "27             0.023258  0.085714  -0.241008  -0.392157    -0.392157   \n",
       "28             0.041629 -0.142998  -0.033204   0.032258     0.032258   \n",
       "29             0.084541  0.100729   0.265088   0.187500     0.187500   \n",
       "30             0.007128  0.000629  -0.038218  -0.052632    -0.052632   \n",
       "31             0.092424  0.111008   0.249657   0.388889     0.361111   \n",
       "32             0.092271 -0.101102  -0.289474  -0.360000    -0.346939   \n",
       "33             0.136392  0.110495   0.211111   0.187500     0.187500   \n",
       "34             0.035561  0.033401   0.032365   0.000000     0.000000   \n",
       "35             0.088861  0.155126   0.383362   0.394737     0.368421   \n",
       "36             0.018507 -0.101405  -0.292470  -0.301887    -0.288462   \n",
       "37            -0.004323  0.046066   0.172005   0.189189     0.189189   \n",
       "38             0.005312  0.008928   0.006456   0.000000     0.000000   \n",
       "39            -0.032129 -0.355236  -0.268334  -0.250000    -0.272727   \n",
       "40            -0.016699  0.406762   0.297487   0.212121     0.218750   \n",
       "41             0.011783  0.053091   0.113964   0.200000     0.205128   \n",
       "42             0.023807  0.016106  -0.015973   0.000000     0.021277   \n",
       "43             0.008368  0.096528   0.351140   0.312500     0.354167   \n",
       "44             0.016516 -0.099191  -0.314477  -0.301587    -0.323077   \n",
       "45             0.024998  0.052768   0.202307   0.204545     0.204545   \n",
       "46            -0.035822  0.005152   0.021956   0.018868     0.018868   \n",
       "47             0.024178  0.121291   0.217007   0.296296     0.277778   \n",
       "48             0.003816 -0.113644  -0.361519  -0.371429    -0.362319   \n",
       "49             0.020569  0.109242   0.178439   0.159091     0.159091   \n",
       "50            -0.051180  0.007315  -0.149842  -0.156863    -0.156863   \n",
       "51            -0.068212 -0.023648  -0.136827  -0.023256    -0.186047   \n",
       "52            -0.034835 -0.167097  -0.244761  -0.380952    -0.257143   \n",
       "53             0.109880  0.017469  -0.049804  -0.076923    -0.076923   \n",
       "54             0.048252 -0.032828  -0.066267  -0.041667    -0.041667   \n",
       "55            -0.002025  0.097675   0.222534   0.217391     0.217391   \n",
       "56            -0.017870 -0.117760  -0.362086  -0.392857    -0.392857   \n",
       "57            -0.010947  0.022920   0.598458   0.647059     0.647059   \n",
       "58             0.014061 -0.041555  -0.339016  -0.357143    -0.357143   \n",
       "59             0.029428  0.122214   1.206813   1.388889     1.388889   \n",
       "60             0.038668 -0.045339  -0.242999  -0.255814    -0.279070   \n",
       "61             0.036743 -0.073424   0.096417   0.093750     0.129032   \n",
       "62            -0.028210 -0.007242  -0.143464  -0.371429    -0.371429   \n",
       "63            -0.064462  0.070489   0.156948   0.590909     0.590909   \n",
       "64             0.029449 -0.070734  -0.186595  -0.171429    -0.171429   \n",
       "65             0.002102  0.037491   0.023401   0.000000     0.000000   \n",
       "66             0.020255 -0.004164   0.124316   0.137931     0.137931   \n",
       "67             0.003868  0.081928   0.148954   0.151515     0.151515   \n",
       "68             0.005592 -0.109772  -0.120668  -0.105263    -0.105263   \n",
       "69            -0.009740  0.003228  -0.111710  -0.117647    -0.117647   \n",
       "70             0.001494  0.017140   0.018513   0.033333     0.033333   \n",
       "71             0.064177  0.124899   0.004701   0.064516     0.032258   \n",
       "72             0.009727 -0.149526  -0.064566  -0.090909    -0.062500   \n",
       "73             0.016513  0.060126   0.182061   0.166667     0.166667   \n",
       "74             0.007350 -0.001628  -0.002257   0.000000     0.000000   \n",
       "75            -0.050660  0.161165   0.456602   0.457143     0.428571   \n",
       "76            -0.152927 -0.301114  -3.634511  -3.647059    -3.700000   \n",
       "77             0.008686  0.079677  -0.899801  -0.903704    -0.903704   \n",
       "78             0.015543 -0.000473  -2.842647  -2.923077    -2.923077   \n",
       "79            -0.116281 -0.221749   1.514366   1.600000     1.640000   \n",
       "80            -0.073132  0.129386  -0.969687  -1.015385    -1.015152   \n",
       "81            -0.066936  0.202873  14.130890 -31.000000   -31.000000   \n",
       "82            -0.036569 -0.126232  -0.298616  -0.266667    -0.266667   \n",
       "83            -0.073936  0.130595   0.836704   0.818182     0.772727   \n",
       "84            -0.017065 -0.164047  -0.824604  -0.825000    -0.820513   \n",
       "85            -0.005179  0.068619   1.093415   1.000000     0.857143   \n",
       "86             0.026393  0.132418   0.343087   0.500000     0.615385   \n",
       "87            -0.155601 -0.061843  -6.251634  -6.428571    -6.428571   \n",
       "\n",
       "       Price  Price high  Price low       ROE  P/B ratio  P/E ratio  \\\n",
       "0   0.020935    0.021254   0.032220 -0.014770   0.008203   0.032301   \n",
       "1  -0.136059   -0.133409  -0.139257  0.017604  -0.126812  -0.157811   \n",
       "2  -0.045198   -0.041748  -0.048536  0.010970  -0.051867  -0.075330   \n",
       "3  -0.075444   -0.086453  -0.062348  0.015442  -0.100656  -0.103360   \n",
       "4   0.807200    0.901639   0.698618  0.011919   0.739659   0.749006   \n",
       "5   0.003099   -0.025078   0.039146  0.014622   0.019580  -0.021104   \n",
       "6  -0.140777   -0.085611  -0.207926 -0.000801  -0.172840  -0.172139   \n",
       "7  -0.125835   -0.206154  -0.012971  0.009615  -0.177446  -0.153045   \n",
       "8   0.693302    0.918605   0.439299 -0.006349   0.651210   0.635762   \n",
       "9  -0.019778   -0.068110   0.052609  0.004393  -0.028083  -0.042510   \n",
       "10  0.019115   -0.050170   0.111111  0.005964  -0.008794  -0.011779   \n",
       "11 -0.085794   -0.047604  -0.129368  0.004348  -0.120406  -0.106357   \n",
       "12  0.319909    0.302978   0.341588  0.000394   0.263689   0.272914   \n",
       "13  0.042314    0.028639   0.058880  0.011802   0.045610   0.013434   \n",
       "14  0.035902    0.042912   0.027653  0.018663   0.015267  -0.002121   \n",
       "15  0.218075    0.302229   0.117578  0.016794   0.206230   0.174814   \n",
       "16  0.057343    0.033666   0.090552  0.004880   0.000000   0.017865   \n",
       "17  0.072863    0.018559   0.144468  0.011954   0.025824   0.031104   \n",
       "18  0.061161    0.080743   0.037953  0.000000   0.028646   0.018746   \n",
       "19 -0.026182   -0.009256  -0.046667 -0.003322  -0.062447  -0.071489   \n",
       "20 -0.276886   -0.306139  -0.239669 -0.034444  -0.314131  -0.305239   \n",
       "21  0.004906    0.192354  -0.212653  0.008055  -0.003937   0.004918   \n",
       "22  0.174974    0.066747   0.365310 -0.020548   0.144928   0.089070   \n",
       "23 -0.082003   -0.092817  -0.067151  0.005051  -0.105869  -0.082085   \n",
       "24 -0.411863   -0.416962  -0.405225 -0.024739  -0.424710  -0.428525   \n",
       "25  0.134062    0.178699   0.075701  0.008720   0.129754   0.142204   \n",
       "26  0.164643    0.146149   0.191138  0.017289   0.095050   0.156500   \n",
       "27  0.170500    0.106878   0.257841 -0.090769   0.103074   0.031561   \n",
       "28 -0.231334   -0.251912  -0.206437 -0.011045  -0.244262  -0.185666   \n",
       "29  0.010907    0.035783  -0.017172 -0.091065  -0.030369  -0.033968   \n",
       "30 -0.036413   -0.023442  -0.052045 -0.067108  -0.109620   0.001598   \n",
       "31 -0.137509   -0.115603  -0.164706  0.048126  -0.143216  -0.043085   \n",
       "32  0.427992    0.348214   0.532864 -0.049783   0.313783   0.262368   \n",
       "33 -0.063068   -0.085298  -0.037672 -0.070193  -0.131696  -0.062968   \n",
       "34 -0.044269   -0.030119  -0.059516 -0.056346  -0.128535  -0.044173   \n",
       "35  0.006662    0.032248  -0.022673 -0.018551  -0.029499  -0.006391   \n",
       "36  0.087614    0.051200   0.131233 -0.021855   0.003040   0.067293   \n",
       "37 -0.005795   -0.015410   0.005510  0.007850  -0.024242  -0.036161   \n",
       "38  0.041970    0.043600   0.039574  0.008388   0.046584   0.005772   \n",
       "39  0.004755   -0.012051   0.023426 -0.112894  -0.002967  -0.029651   \n",
       "40  0.017539    0.043372  -0.009442  0.040857   0.035714   0.147363   \n",
       "41 -0.073598   -0.073785  -0.073946  0.025740  -0.060345  -0.085052   \n",
       "42  0.004430   -0.011501   0.022458  0.015056  -0.012232  -0.014554   \n",
       "43 -0.002646    0.011067  -0.017389  0.169963  -0.027864  -0.026679   \n",
       "44  0.153007    0.182992   0.119839 -0.011622   0.140127  -0.038179   \n",
       "45  0.000767   -0.001898   0.003604  0.004276  -0.013966  -0.023410   \n",
       "46 -0.050588   -0.054671  -0.045580  0.028206  -0.079320  -0.077645   \n",
       "47 -0.028794   -0.037465  -0.018813  0.000518  -0.006154  -0.055932   \n",
       "48 -0.469105   -0.327325  -0.628909 -0.012933  -0.486068  -0.478755   \n",
       "49  0.371086    0.180194   0.761526 -0.018868   0.361446   0.370838   \n",
       "50  0.231062    0.267522   0.180054 -0.055556   0.203540   0.242044   \n",
       "51  0.072975   -0.020249   0.210325 -0.127262   0.132353   0.130142   \n",
       "52 -0.553026   -0.552994  -0.552923 -0.058976  -0.490260  -0.464797   \n",
       "53 -0.096712    0.038530  -0.257951 -0.143939  -0.063694   0.007804   \n",
       "54 -0.130621   -0.169521  -0.066667 -0.152051  -0.217687   0.053097   \n",
       "55 -0.051724    0.184880  -0.401020 -0.091082  -0.086957   0.122899   \n",
       "56  0.476190    0.098608   1.580920 -0.109603   0.476190   0.579046   \n",
       "57 -0.004106    0.040127  -0.058086  0.041032   0.012903   0.093602   \n",
       "58 -0.103651   -0.152284  -0.036440 -0.038288  -0.089172  -0.141387   \n",
       "59  0.120894    0.107186   0.136727  0.146370   0.097902   0.182965   \n",
       "60  0.165299    0.170903   0.159309  0.111338   0.133758   0.000000   \n",
       "61 -0.023642   -0.036952  -0.008278  0.020221  -0.061798  -0.137067   \n",
       "62 -0.117465   -0.063309  -0.180857  0.008108  -0.149701  -0.166255   \n",
       "63 -0.057210   -0.064004  -0.047554 -0.047364  -0.035211  -0.085990   \n",
       "64  0.214861    0.148796   0.300285 -0.017824   0.299270   0.293593   \n",
       "65 -0.009684   -0.007619  -0.011519 -0.029608  -0.033708   0.006897   \n",
       "66  0.089038    0.101727   0.074362  0.098425   0.081395   0.145704   \n",
       "67  0.017013    0.009582   0.026343  0.007168  -0.010753  -0.071739   \n",
       "68  0.035781    0.031061   0.040765  0.027580   0.021739   0.011710   \n",
       "69  0.021983    0.023013   0.020793 -0.002597   0.010638  -0.016204   \n",
       "70  0.047410    0.020450   0.078162 -0.021701   0.042105   0.039412   \n",
       "71  0.081308    0.125852   0.032513 -0.071872   0.075758   0.097906   \n",
       "72  0.012791   -0.005340   0.034894 -0.056405  -0.056338   0.060309   \n",
       "73  0.013395   -0.014674   0.045641  0.010132   0.000000   0.046670   \n",
       "74 -0.020015   -0.013803  -0.026740  0.003009  -0.034826  -0.058523   \n",
       "75 -0.021195   -0.001842  -0.042828  0.152000  -0.025773  -0.050814   \n",
       "76 -0.022047   -0.030627  -0.011819 -1.092014   0.031746  -0.139293   \n",
       "77  0.072061    0.091740   0.049551  3.896226   0.266667  -0.347864   \n",
       "78 -0.123169   -0.047071  -0.211640  0.225434  -0.129555  -0.533422   \n",
       "79  0.205139    0.152214   0.279814 -0.095912   0.190698   1.000000   \n",
       "80  0.050817    0.017783   0.093183 -2.243478   0.097656   1.000000   \n",
       "81  0.049713    0.030265   0.072325  0.658741   0.117438   1.000000   \n",
       "82 -0.012081   -0.009691  -0.014969  0.033727   0.020701  -0.178864   \n",
       "83 -0.012229   -0.009786  -0.015197 -0.200653   0.020281  -0.217825   \n",
       "84  0.008584   -0.016677   0.037602  0.143878   0.076453   0.310931   \n",
       "85 -0.061866   -0.040829  -0.084103 -0.145406  -0.051136  -0.138480   \n",
       "86 -0.107118   -0.096595  -0.119821 -0.007307  -0.104790   0.080369   \n",
       "87 -0.176631   -0.097862  -0.268448 -1.905363  -0.197324  -0.166192   \n",
       "\n",
       "    Cumulative dividends per share  Dividend payout ratio  \\\n",
       "0                         0.039157               0.024716   \n",
       "1                         0.114754              -0.003009   \n",
       "2                         0.117647              -0.001393   \n",
       "3                         0.118421               0.000465   \n",
       "4                         0.094118               0.005344   \n",
       "5                         0.096774              -0.006240   \n",
       "6                         0.088235              -0.000930   \n",
       "7                         0.090090               0.000931   \n",
       "8                         0.082645               0.010000   \n",
       "9                         0.076336               0.000461   \n",
       "10                        0.070922               0.003222   \n",
       "11                        0.072848               0.006882   \n",
       "12                        0.074074               0.012303   \n",
       "13                        0.068966               0.001801   \n",
       "14                        0.059140               0.000674   \n",
       "15                        0.071066               0.003817   \n",
       "16                        0.061611              -0.010960   \n",
       "17                        0.062500              -0.001809   \n",
       "18                        0.058824               0.001586   \n",
       "19                        0.063492               0.003167   \n",
       "20                        0.059701               0.055242   \n",
       "21                        0.056338              -0.006197   \n",
       "22                        0.053333               0.024726   \n",
       "23                        0.056962               0.005245   \n",
       "24                        0.053892               0.035692   \n",
       "25                        0.051136              -0.008666   \n",
       "26                        0.048649              -0.026428   \n",
       "27                        0.048969               0.075172   \n",
       "28                        0.046683              -0.019227   \n",
       "29                        0.044601               0.061188   \n",
       "30                        0.042697               0.048330   \n",
       "31                        0.043103              -0.083304   \n",
       "32                        0.041322               0.002524   \n",
       "33                        0.039683               0.017625   \n",
       "34                        0.038168              -0.000381   \n",
       "35                        0.040441              -0.026276   \n",
       "36                        0.038869              -0.008995   \n",
       "37                        0.037415              -0.015785   \n",
       "38                        0.036066              -0.010826   \n",
       "39                        0.039557               0.163762   \n",
       "40                        0.038052              -0.000522   \n",
       "41                        0.036657               0.007667   \n",
       "42                        0.035361               0.013834   \n",
       "43                        0.038251              -0.128944   \n",
       "44                        0.036842               0.023106   \n",
       "45                        0.035533               0.003636   \n",
       "46                        0.034314              -0.008772   \n",
       "47                        0.036730               0.011351   \n",
       "48                        0.035429               0.027392   \n",
       "49                        0.034216               0.034438   \n",
       "50                        0.033084               0.082513   \n",
       "51                        0.032025               0.190311   \n",
       "52                        0.031031               0.115849   \n",
       "53                        0.009709              -0.002988   \n",
       "54                        0.009615              -0.054938   \n",
       "55                        0.009524              -0.202140   \n",
       "56                        0.009434              -0.284153   \n",
       "57                        0.009346              -0.041175   \n",
       "58                        0.011111               0.096984   \n",
       "59                        0.012821              -0.049703   \n",
       "60                        0.012658              -0.040731   \n",
       "61                        0.013393               0.044632   \n",
       "62                        0.013216               0.024711   \n",
       "63                        0.014783               0.109308   \n",
       "64                        0.014567               0.075376   \n",
       "65                        0.014358               0.079350   \n",
       "66                        0.014155              -0.059688   \n",
       "67                        0.015599               0.003537   \n",
       "68                        0.015360              -0.013356   \n",
       "69                        0.015127               0.016544   \n",
       "70                        0.014902               0.040318   \n",
       "71                        0.017002               0.093156   \n",
       "72                        0.016717               0.073508   \n",
       "73                        0.016442               0.000151   \n",
       "74                        0.016176               0.006665   \n",
       "75                        0.016643              -0.117213   \n",
       "76                        0.016370              -0.370536   \n",
       "77                        0.016106              -0.588654   \n",
       "78                        0.015851               1.000000   \n",
       "79                        0.015604               1.000000   \n",
       "80                        0.015364               1.000000   \n",
       "81                        0.015132              -0.377355   \n",
       "82                        0.014906               0.011762   \n",
       "83                        0.015326               0.326672   \n",
       "84                        0.015094              -0.083981   \n",
       "85                        0.014870               0.213994   \n",
       "86                        0.014652               0.032935   \n",
       "87                        0.007220              -0.500000   \n",
       "\n",
       "    Long-term debt to equity ratio  Net margin  Asset turnover  \\\n",
       "0                         0.001133    0.016791        0.000774   \n",
       "1                        -0.049245    0.009646       -0.030303   \n",
       "2                        -0.042554   -0.010616        0.000000   \n",
       "3                        -0.027356   -0.013948        0.000000   \n",
       "4                        -0.038111   -0.015234        0.000000   \n",
       "5                        -0.027728   -0.002210        0.000000   \n",
       "6                        -0.047307    0.006645       -0.031250   \n",
       "7                        -0.040079   -0.006601        0.032258   \n",
       "8                        -0.068135   -0.001107        0.000000   \n",
       "9                         0.086757    0.001109        0.000000   \n",
       "10                        0.127700    0.008859       -0.031250   \n",
       "11                       -0.007053    0.015368       -0.032258   \n",
       "12                        0.016357    0.012973        0.000000   \n",
       "13                       -0.015004    0.016009       -0.033333   \n",
       "14                        0.080328    0.007353        0.000000   \n",
       "15                        0.011328    0.001043        0.000000   \n",
       "16                       -0.017040   -0.010417        0.034483   \n",
       "17                        0.024245    0.003158        0.000000   \n",
       "18                       -0.060303    0.005247        0.000000   \n",
       "19                        0.024372    0.024008        0.000000   \n",
       "20                       -0.031415   -0.005097        0.000000   \n",
       "21                       -0.044684    0.047131        0.000000   \n",
       "22                        0.004518    0.028376       -0.033333   \n",
       "23                       -0.037172    0.034253       -0.068966   \n",
       "24                        0.168922   -0.005520       -0.037037   \n",
       "25                        0.252938    0.028677       -0.038462   \n",
       "26                        0.009803    0.031475        0.000000   \n",
       "27                        0.025361   -0.065388       -0.040000   \n",
       "28                        0.062922    0.036381       -0.041667   \n",
       "29                        0.000810   -0.044104        0.000000   \n",
       "30                       -0.042882   -0.037665       -0.043478   \n",
       "31                       -0.044714    0.093933        0.000000   \n",
       "32                       -0.087374   -0.006261        0.000000   \n",
       "33                       -0.087880   -0.018002       -0.045455   \n",
       "34                        0.024338   -0.008249        0.047619   \n",
       "35                        0.048176    0.016636        0.000000   \n",
       "36                       -0.016936    0.001818        0.000000   \n",
       "37                        0.035834    0.011797        0.000000   \n",
       "38                       -0.014380    0.011659        0.045455   \n",
       "39                        0.007944   -0.014184       -0.086957   \n",
       "40                        0.068202    0.040468        0.047619   \n",
       "41                        0.050248    0.027658        0.000000   \n",
       "42                        0.001423    0.019344        0.000000   \n",
       "43                        0.064695    0.051980        0.090909   \n",
       "44                        0.037552   -0.010196        0.000000   \n",
       "45                        0.002988    0.008716        0.000000   \n",
       "46                        0.141444    0.020424       -0.041667   \n",
       "47                        0.000834   -0.010008        0.000000   \n",
       "48                        0.082591   -0.020995       -0.043478   \n",
       "49                       -0.007194   -0.041303        0.000000   \n",
       "50                       -0.010111   -0.080365        0.000000   \n",
       "51                        0.073712   -0.140541        0.000000   \n",
       "52                        0.028507   -0.070231        0.000000   \n",
       "53                       -0.066656   -0.118377        0.000000   \n",
       "54                        0.006441   -0.090793       -0.045455   \n",
       "55                       -0.053564   -0.037975       -0.047619   \n",
       "56                       -0.043869   -0.070175        0.000000   \n",
       "57                       -0.049110    0.056604        0.000000   \n",
       "58                        0.015257   -0.031250        0.000000   \n",
       "59                       -0.073449    0.159754        0.000000   \n",
       "60                       -0.055875    0.117881        0.000000   \n",
       "61                       -0.068244    0.063981        0.000000   \n",
       "62                       -0.006453    0.030067        0.000000   \n",
       "63                       -0.029970   -0.036757        0.000000   \n",
       "64                       -0.068392   -0.003367        0.000000   \n",
       "65                       -0.017814   -0.050676        0.000000   \n",
       "66                        0.000889    0.086595        0.050000   \n",
       "67                        0.002141    0.010917        0.000000   \n",
       "68                       -0.013080    0.036717        0.000000   \n",
       "69                       -0.003168    0.011458        0.000000   \n",
       "70                       -0.020552   -0.016478        0.047619   \n",
       "71                       -0.081878   -0.063874        0.000000   \n",
       "72                       -0.012605   -0.034676        0.000000   \n",
       "73                       -0.034719    0.024334        0.000000   \n",
       "74                       -0.024102    0.023756        0.000000   \n",
       "75                       -0.009689    0.132597        0.045455   \n",
       "76                        0.121691   -0.461402       -0.043478   \n",
       "77                       -0.035004   -0.856674        0.000000   \n",
       "78                       -0.043717    1.000000        0.000000   \n",
       "79                       -0.086551    1.000000       -0.090909   \n",
       "80                       -0.018463    1.000000        0.100000   \n",
       "81                       -0.006753    0.533965        0.090909   \n",
       "82                       -0.019840   -0.024717        0.083333   \n",
       "83                       -0.019110   -0.302006        0.192308   \n",
       "84                       -0.034995    0.089259        0.064516   \n",
       "85                        0.045536   -0.147222        0.000000   \n",
       "86                        0.010799   -0.060261        0.030303   \n",
       "87                        0.195486   -0.500000       -0.029412   \n",
       "\n",
       "    Free cash flow per share ticker  Relative Return DJIA  \n",
       "0                   0.471454     GE                   NaN  \n",
       "1                  -4.333333     GE                   NaN  \n",
       "2                  -1.550000     GE              2.615307  \n",
       "3                   9.818182     GE              1.276595  \n",
       "4                  -0.941176     GE             -0.671290  \n",
       "5                   3.285714     GE              3.963392  \n",
       "6                   0.266667     GE             -0.768413  \n",
       "7                   0.210526     GE             -1.180573  \n",
       "8                  -1.043478     GE            -19.229023  \n",
       "9                   8.500000     GE             -0.844755  \n",
       "10                 -5.894737     GE             -9.406185  \n",
       "11                 -0.182796     GE             -2.026139  \n",
       "12                 -0.644737     GE             -0.551643  \n",
       "13                  0.074074     GE              0.667926  \n",
       "14                 -0.275862     GE             -1.562338  \n",
       "15                  2.761905     GE             -2.691119  \n",
       "16                 -1.139241     GE             -1.505297  \n",
       "17                 -1.727273     GE             -0.143000  \n",
       "18                  2.375000     GE             -1.440699  \n",
       "19                  4.222222     GE             -0.372789  \n",
       "20                 -0.858156     GE             -8.673656  \n",
       "21                  2.300000     GE             -1.627604  \n",
       "22                 -0.469697     GE             -4.117887  \n",
       "23                  2.428571     GE             -1.627786  \n",
       "24                 -0.841667     GE             -0.719395  \n",
       "25                  0.736842     GE             -4.805999  \n",
       "26                  1.303030     GE              0.737625  \n",
       "27                  0.236842     GE             -1.424245  \n",
       "28                 -0.819149     GE             -1.502775  \n",
       "29                  2.529412     GE             -3.403267  \n",
       "30                  0.433333     GE             -0.721103  \n",
       "31                  0.046512     GE              2.609417  \n",
       "32                 -0.388889     GE             -1.044974  \n",
       "33                 -0.072727     GE             -2.361881  \n",
       "34                  0.235294     GE             -6.229696  \n",
       "35                  0.714286     GE             -2.828367  \n",
       "36                 -0.398148     GE             -1.394091  \n",
       "37                 -0.646154     GE             -0.131659  \n",
       "38                  3.434783     GE             -2.263720  \n",
       "39                 -0.147059     GE             -0.506192  \n",
       "40                 -0.597701     GE              1.544246  \n",
       "41                  0.114286     GE             -0.917277  \n",
       "42                  0.820513     GE             14.458542  \n",
       "43                 -0.211268     GE              0.387398  \n",
       "44                 -0.339286     GE             -1.115711  \n",
       "45                  2.000000     GE            -11.806498  \n",
       "46                 -0.126126     GE             -0.560726  \n",
       "47                  0.185567     GE             -2.385076  \n",
       "48                 -0.452174     GE              0.805806  \n",
       "49                  0.444444     GE             -0.112000  \n",
       "50                 -0.032967     GE             -0.430264  \n",
       "51                  1.090909     GE              4.390503  \n",
       "52                 -1.092391     GE             -0.325326  \n",
       "53                 -4.058824     GE             -1.618057  \n",
       "54                  0.461538     GE              0.312684  \n",
       "55                  0.302632     GE             -0.471803  \n",
       "56                 -0.303030     GE             -0.350600  \n",
       "57                  0.173913     GE             -3.504736  \n",
       "58                  0.061728     GE             -1.838943  \n",
       "59                 -0.081395     GE             -0.253457  \n",
       "60                 -0.253165     GE             -0.108052  \n",
       "61                 -0.067797     GE             -0.880700  \n",
       "62                  0.327273     GE            -19.783475  \n",
       "63                 -0.123288     GE             -1.765448  \n",
       "64                 -0.265625     GE             -0.270828  \n",
       "65                 -0.063830     GE             -1.334144  \n",
       "66                  0.045455     GE             -2.636916  \n",
       "67                  0.673913     GE             -1.596069  \n",
       "68                 -0.662338     GE             -5.020961  \n",
       "69                  1.076923     GE             -0.779055  \n",
       "70                 -0.296296     GE             -0.330205  \n",
       "71                  1.342105     GE              4.865892  \n",
       "72                 -0.685393     GE             -1.080618  \n",
       "73                  0.321429     GE             -4.105823  \n",
       "74                  0.324324     GE             -0.433754  \n",
       "75                  1.979592     GE              2.487301  \n",
       "76                 -0.712329     GE             -1.065693  \n",
       "77                  0.452381     GE              1.854232  \n",
       "78                 -0.950820     GE              9.186159  \n",
       "79                 16.666667     GE             -1.811231  \n",
       "80                 -1.132075     GE             -0.774597  \n",
       "81                  5.571429     GE             -0.069506  \n",
       "82                 -1.282609     GE             -0.247285  \n",
       "83                 -0.538462     GE                   inf  \n",
       "84                 -1.666667     GE                   NaN  \n",
       "85                 -6.000000     GE             -0.277967  \n",
       "86                 -0.500000     GE              0.459993  \n",
       "87                  5.600000     GE              0.995147  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_data_trend_standardized[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 21)                462       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 1s 32ms/step - loss: 1.2251 - accuracy: 0.0000e+00 - val_loss: 0.3996 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2013 - accuracy: 0.0000e+00 - val_loss: 0.3980 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1818 - accuracy: 0.0000e+00 - val_loss: 0.3977 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1638 - accuracy: 0.0000e+00 - val_loss: 0.3982 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1458 - accuracy: 0.0000e+00 - val_loss: 0.3992 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1331 - accuracy: 0.0000e+00 - val_loss: 0.4007 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1179 - accuracy: 0.0000e+00 - val_loss: 0.4028 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1069 - accuracy: 0.0000e+00 - val_loss: 0.4045 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0953 - accuracy: 0.0000e+00 - val_loss: 0.4070 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0841 - accuracy: 0.0000e+00 - val_loss: 0.4096 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0755 - accuracy: 0.0000e+00 - val_loss: 0.4124 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0666 - accuracy: 0.0000e+00 - val_loss: 0.4144 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0609 - accuracy: 0.0000e+00 - val_loss: 0.4170 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0518 - accuracy: 0.0000e+00 - val_loss: 0.4195 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0460 - accuracy: 0.0000e+00 - val_loss: 0.4232 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0397 - accuracy: 0.0000e+00 - val_loss: 0.4258 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0342 - accuracy: 0.0000e+00 - val_loss: 0.4294 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0274 - accuracy: 0.0000e+00 - val_loss: 0.4310 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0218 - accuracy: 0.0000e+00 - val_loss: 0.4336 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0182 - accuracy: 0.0000e+00 - val_loss: 0.4366 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0129 - accuracy: 0.0000e+00 - val_loss: 0.4388 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0085 - accuracy: 0.0000e+00 - val_loss: 0.4407 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0044 - accuracy: 0.0000e+00 - val_loss: 0.4435 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9996 - accuracy: 0.0000e+00 - val_loss: 0.4467 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.4499 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9923 - accuracy: 0.0000e+00 - val_loss: 0.4525 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9887 - accuracy: 0.0000e+00 - val_loss: 0.4542 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9844 - accuracy: 0.0000e+00 - val_loss: 0.4571 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9823 - accuracy: 0.0000e+00 - val_loss: 0.4591 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9787 - accuracy: 0.0000e+00 - val_loss: 0.4627 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9751 - accuracy: 0.0000e+00 - val_loss: 0.4644 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9714 - accuracy: 0.0000e+00 - val_loss: 0.4668 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9685 - accuracy: 0.0000e+00 - val_loss: 0.4696 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9656 - accuracy: 0.0000e+00 - val_loss: 0.4710 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9629 - accuracy: 0.0000e+00 - val_loss: 0.4740 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9595 - accuracy: 0.0000e+00 - val_loss: 0.4765 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9576 - accuracy: 0.0000e+00 - val_loss: 0.4795 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9548 - accuracy: 0.0000e+00 - val_loss: 0.4815 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9522 - accuracy: 0.0000e+00 - val_loss: 0.4840 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9497 - accuracy: 0.0000e+00 - val_loss: 0.4866 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9469 - accuracy: 0.0000e+00 - val_loss: 0.4891 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9448 - accuracy: 0.0000e+00 - val_loss: 0.4908 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9425 - accuracy: 0.0000e+00 - val_loss: 0.4927 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9397 - accuracy: 0.0000e+00 - val_loss: 0.4942 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9381 - accuracy: 0.0000e+00 - val_loss: 0.4959 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9354 - accuracy: 0.0000e+00 - val_loss: 0.4979 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9336 - accuracy: 0.0000e+00 - val_loss: 0.4989 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9307 - accuracy: 0.0000e+00 - val_loss: 0.5014 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9288 - accuracy: 0.0000e+00 - val_loss: 0.5028 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9268 - accuracy: 0.0000e+00 - val_loss: 0.5038 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9246 - accuracy: 0.0000e+00 - val_loss: 0.5058 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9231 - accuracy: 0.0000e+00 - val_loss: 0.5065 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9206 - accuracy: 0.0000e+00 - val_loss: 0.5082 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9186 - accuracy: 0.0000e+00 - val_loss: 0.5095 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9169 - accuracy: 0.0000e+00 - val_loss: 0.5114 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9147 - accuracy: 0.0000e+00 - val_loss: 0.5133 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9133 - accuracy: 0.0000e+00 - val_loss: 0.5143 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9112 - accuracy: 0.0000e+00 - val_loss: 0.5173 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9096 - accuracy: 0.0000e+00 - val_loss: 0.5188 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9078 - accuracy: 0.0000e+00 - val_loss: 0.5207 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9065 - accuracy: 0.0000e+00 - val_loss: 0.5217 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9046 - accuracy: 0.0000e+00 - val_loss: 0.5230 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9028 - accuracy: 0.0000e+00 - val_loss: 0.5238 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9015 - accuracy: 0.0000e+00 - val_loss: 0.5255 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.5259 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8977 - accuracy: 0.0000e+00 - val_loss: 0.5278 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8963 - accuracy: 0.0000e+00 - val_loss: 0.5286 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8948 - accuracy: 0.0000e+00 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8930 - accuracy: 0.0000e+00 - val_loss: 0.5312 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8921 - accuracy: 0.0000e+00 - val_loss: 0.5325 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8903 - accuracy: 0.0000e+00 - val_loss: 0.5355 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.5371 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8880 - accuracy: 0.0000e+00 - val_loss: 0.5377 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.5390 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.5393 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8836 - accuracy: 0.0000e+00 - val_loss: 0.5407 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8824 - accuracy: 0.0000e+00 - val_loss: 0.5428 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8810 - accuracy: 0.0000e+00 - val_loss: 0.5441 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8801 - accuracy: 0.0000e+00 - val_loss: 0.5452 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.5460 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8778 - accuracy: 0.0000e+00 - val_loss: 0.5481 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8766 - accuracy: 0.0000e+00 - val_loss: 0.5498 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8759 - accuracy: 0.0000e+00 - val_loss: 0.5500 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8746 - accuracy: 0.0000e+00 - val_loss: 0.5516 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.5531 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.5539 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8718 - accuracy: 0.0000e+00 - val_loss: 0.5542 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8706 - accuracy: 0.0000e+00 - val_loss: 0.5557 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8695 - accuracy: 0.0000e+00 - val_loss: 0.5573 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8687 - accuracy: 0.0000e+00 - val_loss: 0.5593 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8677 - accuracy: 0.0000e+00 - val_loss: 0.5608 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8669 - accuracy: 0.0000e+00 - val_loss: 0.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8659 - accuracy: 0.0000e+00 - val_loss: 0.5626 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8652 - accuracy: 0.0000e+00 - val_loss: 0.5646 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.5655 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8636 - accuracy: 0.0000e+00 - val_loss: 0.5652 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8626 - accuracy: 0.0000e+00 - val_loss: 0.5670 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.5677 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8609 - accuracy: 0.0000e+00 - val_loss: 0.5691 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8604 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8597 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8589 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8582 - accuracy: 0.0000e+00 - val_loss: 0.5732 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8575 - accuracy: 0.0000e+00 - val_loss: 0.5746 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8568 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8562 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8549 - accuracy: 0.0000e+00 - val_loss: 0.5792 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8544 - accuracy: 0.0000e+00 - val_loss: 0.5808 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8536 - accuracy: 0.0000e+00 - val_loss: 0.5817 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8529 - accuracy: 0.0000e+00 - val_loss: 0.5833 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8523 - accuracy: 0.0000e+00 - val_loss: 0.5847 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8517 - accuracy: 0.0000e+00 - val_loss: 0.5864 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8510 - accuracy: 0.0000e+00 - val_loss: 0.5869 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8504 - accuracy: 0.0000e+00 - val_loss: 0.5879 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8496 - accuracy: 0.0000e+00 - val_loss: 0.5883 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.5896 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8487 - accuracy: 0.0000e+00 - val_loss: 0.5912 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8478 - accuracy: 0.0000e+00 - val_loss: 0.5918 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8472 - accuracy: 0.0000e+00 - val_loss: 0.5931 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8467 - accuracy: 0.0000e+00 - val_loss: 0.5946 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8462 - accuracy: 0.0000e+00 - val_loss: 0.5953 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8457 - accuracy: 0.0000e+00 - val_loss: 0.5963 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8452 - accuracy: 0.0000e+00 - val_loss: 0.5975 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8445 - accuracy: 0.0000e+00 - val_loss: 0.5989 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8441 - accuracy: 0.0000e+00 - val_loss: 0.5997 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8438 - accuracy: 0.0000e+00 - val_loss: 0.6006 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8430 - accuracy: 0.0000e+00 - val_loss: 0.6024 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8426 - accuracy: 0.0000e+00 - val_loss: 0.6042 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.6045 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8416 - accuracy: 0.0000e+00 - val_loss: 0.6063 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8412 - accuracy: 0.0000e+00 - val_loss: 0.6077 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8406 - accuracy: 0.0000e+00 - val_loss: 0.6078 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8402 - accuracy: 0.0000e+00 - val_loss: 0.6083 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8399 - accuracy: 0.0000e+00 - val_loss: 0.6095 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8395 - accuracy: 0.0000e+00 - val_loss: 0.6116 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8394 - accuracy: 0.0000e+00 - val_loss: 0.6110 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8385 - accuracy: 0.0000e+00 - val_loss: 0.6122 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8381 - accuracy: 0.0000e+00 - val_loss: 0.6134 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8378 - accuracy: 0.0000e+00 - val_loss: 0.6142 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.6152 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8367 - accuracy: 0.0000e+00 - val_loss: 0.6163 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8365 - accuracy: 0.0000e+00 - val_loss: 0.6172 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8361 - accuracy: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8359 - accuracy: 0.0000e+00 - val_loss: 0.6180 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8354 - accuracy: 0.0000e+00 - val_loss: 0.6195 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8350 - accuracy: 0.0000e+00 - val_loss: 0.6204 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.6210 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8342 - accuracy: 0.0000e+00 - val_loss: 0.6219 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8339 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8334 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.6240 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8329 - accuracy: 0.0000e+00 - val_loss: 0.6251 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8328 - accuracy: 0.0000e+00 - val_loss: 0.6250 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8322 - accuracy: 0.0000e+00 - val_loss: 0.6258 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8317 - accuracy: 0.0000e+00 - val_loss: 0.6270 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8313 - accuracy: 0.0000e+00 - val_loss: 0.6280 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8311 - accuracy: 0.0000e+00 - val_loss: 0.6283 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.6303 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8301 - accuracy: 0.0000e+00 - val_loss: 0.6305 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8297 - accuracy: 0.0000e+00 - val_loss: 0.6312 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8294 - accuracy: 0.0000e+00 - val_loss: 0.6314 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8291 - accuracy: 0.0000e+00 - val_loss: 0.6320 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8291 - accuracy: 0.0000e+00 - val_loss: 0.6334 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8285 - accuracy: 0.0000e+00 - val_loss: 0.6332 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8282 - accuracy: 0.0000e+00 - val_loss: 0.6335 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8279 - accuracy: 0.0000e+00 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8276 - accuracy: 0.0000e+00 - val_loss: 0.6357 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8273 - accuracy: 0.0000e+00 - val_loss: 0.6357 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8269 - accuracy: 0.0000e+00 - val_loss: 0.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8268 - accuracy: 0.0000e+00 - val_loss: 0.6366 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8264 - accuracy: 0.0000e+00 - val_loss: 0.6376 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8260 - accuracy: 0.0000e+00 - val_loss: 0.6384 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8257 - accuracy: 0.0000e+00 - val_loss: 0.6391 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8257 - accuracy: 0.0000e+00 - val_loss: 0.6403 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8253 - accuracy: 0.0000e+00 - val_loss: 0.6404 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8251 - accuracy: 0.0000e+00 - val_loss: 0.6404 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8247 - accuracy: 0.0000e+00 - val_loss: 0.6414 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8245 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8241 - accuracy: 0.0000e+00 - val_loss: 0.6422 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8239 - accuracy: 0.0000e+00 - val_loss: 0.6426 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8236 - accuracy: 0.0000e+00 - val_loss: 0.6427 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8233 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8231 - accuracy: 0.0000e+00 - val_loss: 0.6434 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8228 - accuracy: 0.0000e+00 - val_loss: 0.6442 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8226 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8219 - accuracy: 0.0000e+00 - val_loss: 0.6461 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8217 - accuracy: 0.0000e+00 - val_loss: 0.6463 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8216 - accuracy: 0.0000e+00 - val_loss: 0.6470 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8213 - accuracy: 0.0000e+00 - val_loss: 0.6474 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8211 - accuracy: 0.0000e+00 - val_loss: 0.6477 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8209 - accuracy: 0.0000e+00 - val_loss: 0.6481 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8208 - accuracy: 0.0000e+00 - val_loss: 0.6480 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8204 - accuracy: 0.0000e+00 - val_loss: 0.6486 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8201 - accuracy: 0.0000e+00 - val_loss: 0.6493 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8200 - accuracy: 0.0000e+00 - val_loss: 0.6496 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8197 - accuracy: 0.0000e+00 - val_loss: 0.6500 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8194 - accuracy: 0.0000e+00 - val_loss: 0.6504 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8193 - accuracy: 0.0000e+00 - val_loss: 0.6500 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8191 - accuracy: 0.0000e+00 - val_loss: 0.6504 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 0.6509 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8187 - accuracy: 0.0000e+00 - val_loss: 0.6512 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8184 - accuracy: 0.0000e+00 - val_loss: 0.6517 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8182 - accuracy: 0.0000e+00 - val_loss: 0.6519 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8180 - accuracy: 0.0000e+00 - val_loss: 0.6524 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8179 - accuracy: 0.0000e+00 - val_loss: 0.6524 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8176 - accuracy: 0.0000e+00 - val_loss: 0.6527 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8174 - accuracy: 0.0000e+00 - val_loss: 0.6532 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8172 - accuracy: 0.0000e+00 - val_loss: 0.6535 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8171 - accuracy: 0.0000e+00 - val_loss: 0.6540 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8169 - accuracy: 0.0000e+00 - val_loss: 0.6539 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8166 - accuracy: 0.0000e+00 - val_loss: 0.6542 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8165 - accuracy: 0.0000e+00 - val_loss: 0.6543 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8162 - accuracy: 0.0000e+00 - val_loss: 0.6544 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8161 - accuracy: 0.0000e+00 - val_loss: 0.6548 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8160 - accuracy: 0.0000e+00 - val_loss: 0.6547 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8157 - accuracy: 0.0000e+00 - val_loss: 0.6550 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8155 - accuracy: 0.0000e+00 - val_loss: 0.6553 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8154 - accuracy: 0.0000e+00 - val_loss: 0.6558 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8153 - accuracy: 0.0000e+00 - val_loss: 0.6555 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8151 - accuracy: 0.0000e+00 - val_loss: 0.6557 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8149 - accuracy: 0.0000e+00 - val_loss: 0.6558 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8147 - accuracy: 0.0000e+00 - val_loss: 0.6562 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8145 - accuracy: 0.0000e+00 - val_loss: 0.6564 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8145 - accuracy: 0.0000e+00 - val_loss: 0.6561 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8142 - accuracy: 0.0000e+00 - val_loss: 0.6565 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8140 - accuracy: 0.0000e+00 - val_loss: 0.6566 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8139 - accuracy: 0.0000e+00 - val_loss: 0.6569 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8137 - accuracy: 0.0000e+00 - val_loss: 0.6573 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8136 - accuracy: 0.0000e+00 - val_loss: 0.6576 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8134 - accuracy: 0.0000e+00 - val_loss: 0.6575 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8132 - accuracy: 0.0000e+00 - val_loss: 0.6575 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8131 - accuracy: 0.0000e+00 - val_loss: 0.6579 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8129 - accuracy: 0.0000e+00 - val_loss: 0.6579 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8128 - accuracy: 0.0000e+00 - val_loss: 0.6579 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8126 - accuracy: 0.0000e+00 - val_loss: 0.6578 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8126 - accuracy: 0.0000e+00 - val_loss: 0.6580 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8123 - accuracy: 0.0000e+00 - val_loss: 0.6584 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8122 - accuracy: 0.0000e+00 - val_loss: 0.6583 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8121 - accuracy: 0.0000e+00 - val_loss: 0.6583 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8120 - accuracy: 0.0000e+00 - val_loss: 0.6586 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8118 - accuracy: 0.0000e+00 - val_loss: 0.6585 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8117 - accuracy: 0.0000e+00 - val_loss: 0.6585 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8116 - accuracy: 0.0000e+00 - val_loss: 0.6588 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8115 - accuracy: 0.0000e+00 - val_loss: 0.6587 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8113 - accuracy: 0.0000e+00 - val_loss: 0.6589 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8112 - accuracy: 0.0000e+00 - val_loss: 0.6590 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8111 - accuracy: 0.0000e+00 - val_loss: 0.6588 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8110 - accuracy: 0.0000e+00 - val_loss: 0.6592 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8108 - accuracy: 0.0000e+00 - val_loss: 0.6594 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8107 - accuracy: 0.0000e+00 - val_loss: 0.6595 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8106 - accuracy: 0.0000e+00 - val_loss: 0.6594 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8104 - accuracy: 0.0000e+00 - val_loss: 0.6593 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8104 - accuracy: 0.0000e+00 - val_loss: 0.6591 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/AAPL_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6591 - accuracy: 0.0000e+00\n",
      "[0.6591464281082153, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 21)                462       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.2133 - accuracy: 0.0000e+00 - val_loss: 0.3229 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1941 - accuracy: 0.0000e+00 - val_loss: 0.3226 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1746 - accuracy: 0.0000e+00 - val_loss: 0.3226 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1582 - accuracy: 0.0000e+00 - val_loss: 0.3231 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1427 - accuracy: 0.0000e+00 - val_loss: 0.3253 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1285 - accuracy: 0.0000e+00 - val_loss: 0.3282 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1146 - accuracy: 0.0000e+00 - val_loss: 0.3313 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1026 - accuracy: 0.0000e+00 - val_loss: 0.3346 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0890 - accuracy: 0.0000e+00 - val_loss: 0.3376 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0788 - accuracy: 0.0000e+00 - val_loss: 0.3407 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0698 - accuracy: 0.0000e+00 - val_loss: 0.3446 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0611 - accuracy: 0.0000e+00 - val_loss: 0.3481 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0516 - accuracy: 0.0000e+00 - val_loss: 0.3515 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0437 - accuracy: 0.0000e+00 - val_loss: 0.3542 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0378 - accuracy: 0.0000e+00 - val_loss: 0.3582 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0307 - accuracy: 0.0000e+00 - val_loss: 0.3616 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0251 - accuracy: 0.0000e+00 - val_loss: 0.3648 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0193 - accuracy: 0.0000e+00 - val_loss: 0.3678 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0146 - accuracy: 0.0000e+00 - val_loss: 0.3709 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0104 - accuracy: 0.0000e+00 - val_loss: 0.3740 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0065 - accuracy: 0.0000e+00 - val_loss: 0.3771 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0020 - accuracy: 0.0000e+00 - val_loss: 0.3794 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9989 - accuracy: 0.0000e+00 - val_loss: 0.3820 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9953 - accuracy: 0.0000e+00 - val_loss: 0.3845 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9917 - accuracy: 0.0000e+00 - val_loss: 0.3866 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9892 - accuracy: 0.0000e+00 - val_loss: 0.3888 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9861 - accuracy: 0.0000e+00 - val_loss: 0.3915 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9832 - accuracy: 0.0000e+00 - val_loss: 0.3930 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9806 - accuracy: 0.0000e+00 - val_loss: 0.3946 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9781 - accuracy: 0.0000e+00 - val_loss: 0.3963 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9759 - accuracy: 0.0000e+00 - val_loss: 0.3984 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9735 - accuracy: 0.0000e+00 - val_loss: 0.3998 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9710 - accuracy: 0.0000e+00 - val_loss: 0.4012 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9694 - accuracy: 0.0000e+00 - val_loss: 0.4032 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9671 - accuracy: 0.0000e+00 - val_loss: 0.4046 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9650 - accuracy: 0.0000e+00 - val_loss: 0.4063 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9633 - accuracy: 0.0000e+00 - val_loss: 0.4077 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9612 - accuracy: 0.0000e+00 - val_loss: 0.4091 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9598 - accuracy: 0.0000e+00 - val_loss: 0.4107 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9576 - accuracy: 0.0000e+00 - val_loss: 0.4119 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9565 - accuracy: 0.0000e+00 - val_loss: 0.4130 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9542 - accuracy: 0.0000e+00 - val_loss: 0.4140 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9525 - accuracy: 0.0000e+00 - val_loss: 0.4158 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9512 - accuracy: 0.0000e+00 - val_loss: 0.4164 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9491 - accuracy: 0.0000e+00 - val_loss: 0.4177 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9474 - accuracy: 0.0000e+00 - val_loss: 0.4187 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9458 - accuracy: 0.0000e+00 - val_loss: 0.4194 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9441 - accuracy: 0.0000e+00 - val_loss: 0.4204 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9423 - accuracy: 0.0000e+00 - val_loss: 0.4217 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9408 - accuracy: 0.0000e+00 - val_loss: 0.4222 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9396 - accuracy: 0.0000e+00 - val_loss: 0.4237 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9382 - accuracy: 0.0000e+00 - val_loss: 0.4247 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9368 - accuracy: 0.0000e+00 - val_loss: 0.4263 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9353 - accuracy: 0.0000e+00 - val_loss: 0.4262 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9341 - accuracy: 0.0000e+00 - val_loss: 0.4275 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9328 - accuracy: 0.0000e+00 - val_loss: 0.4278 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9317 - accuracy: 0.0000e+00 - val_loss: 0.4288 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9303 - accuracy: 0.0000e+00 - val_loss: 0.4300 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.4322 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9266 - accuracy: 0.0000e+00 - val_loss: 0.4323 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9254 - accuracy: 0.0000e+00 - val_loss: 0.4333 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9244 - accuracy: 0.0000e+00 - val_loss: 0.4336 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9228 - accuracy: 0.0000e+00 - val_loss: 0.4346 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9217 - accuracy: 0.0000e+00 - val_loss: 0.4352 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9203 - accuracy: 0.0000e+00 - val_loss: 0.4357 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9192 - accuracy: 0.0000e+00 - val_loss: 0.4365 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9178 - accuracy: 0.0000e+00 - val_loss: 0.4367 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9166 - accuracy: 0.0000e+00 - val_loss: 0.4379 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9155 - accuracy: 0.0000e+00 - val_loss: 0.4385 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9143 - accuracy: 0.0000e+00 - val_loss: 0.4388 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9129 - accuracy: 0.0000e+00 - val_loss: 0.4390 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9119 - accuracy: 0.0000e+00 - val_loss: 0.4395 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9106 - accuracy: 0.0000e+00 - val_loss: 0.4398 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9088 - accuracy: 0.0000e+00 - val_loss: 0.4399 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9077 - accuracy: 0.0000e+00 - val_loss: 0.4402 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9063 - accuracy: 0.0000e+00 - val_loss: 0.4410 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9046 - accuracy: 0.0000e+00 - val_loss: 0.4414 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9036 - accuracy: 0.0000e+00 - val_loss: 0.4413 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9026 - accuracy: 0.0000e+00 - val_loss: 0.4413 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9013 - accuracy: 0.0000e+00 - val_loss: 0.4414 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8994 - accuracy: 0.0000e+00 - val_loss: 0.4417 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8980 - accuracy: 0.0000e+00 - val_loss: 0.4417 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8968 - accuracy: 0.0000e+00 - val_loss: 0.4423 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8954 - accuracy: 0.0000e+00 - val_loss: 0.4426 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8947 - accuracy: 0.0000e+00 - val_loss: 0.4424 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8929 - accuracy: 0.0000e+00 - val_loss: 0.4428 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8918 - accuracy: 0.0000e+00 - val_loss: 0.4432 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8908 - accuracy: 0.0000e+00 - val_loss: 0.4428 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8896 - accuracy: 0.0000e+00 - val_loss: 0.4428 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8884 - accuracy: 0.0000e+00 - val_loss: 0.4434 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8875 - accuracy: 0.0000e+00 - val_loss: 0.4433 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.4433 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.4437 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8841 - accuracy: 0.0000e+00 - val_loss: 0.4433 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8829 - accuracy: 0.0000e+00 - val_loss: 0.4434 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8820 - accuracy: 0.0000e+00 - val_loss: 0.4423 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8808 - accuracy: 0.0000e+00 - val_loss: 0.4417 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8798 - accuracy: 0.0000e+00 - val_loss: 0.4424 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8791 - accuracy: 0.0000e+00 - val_loss: 0.4424 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8780 - accuracy: 0.0000e+00 - val_loss: 0.4422 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8770 - accuracy: 0.0000e+00 - val_loss: 0.4420 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8760 - accuracy: 0.0000e+00 - val_loss: 0.4420 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.4413 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8740 - accuracy: 0.0000e+00 - val_loss: 0.4409 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.4415 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8723 - accuracy: 0.0000e+00 - val_loss: 0.4409 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.4406 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.4403 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.4407 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8688 - accuracy: 0.0000e+00 - val_loss: 0.4409 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8680 - accuracy: 0.0000e+00 - val_loss: 0.4407 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8673 - accuracy: 0.0000e+00 - val_loss: 0.4401 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8663 - accuracy: 0.0000e+00 - val_loss: 0.4401 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.4403 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8646 - accuracy: 0.0000e+00 - val_loss: 0.4402 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8638 - accuracy: 0.0000e+00 - val_loss: 0.4398 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8632 - accuracy: 0.0000e+00 - val_loss: 0.4388 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8623 - accuracy: 0.0000e+00 - val_loss: 0.4381 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8615 - accuracy: 0.0000e+00 - val_loss: 0.4387 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8608 - accuracy: 0.0000e+00 - val_loss: 0.4380 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8601 - accuracy: 0.0000e+00 - val_loss: 0.4380 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8592 - accuracy: 0.0000e+00 - val_loss: 0.4386 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8585 - accuracy: 0.0000e+00 - val_loss: 0.4381 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8577 - accuracy: 0.0000e+00 - val_loss: 0.4376 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8572 - accuracy: 0.0000e+00 - val_loss: 0.4367 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8565 - accuracy: 0.0000e+00 - val_loss: 0.4372 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8558 - accuracy: 0.0000e+00 - val_loss: 0.4369 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8552 - accuracy: 0.0000e+00 - val_loss: 0.4372 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8545 - accuracy: 0.0000e+00 - val_loss: 0.4358 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.4348 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8534 - accuracy: 0.0000e+00 - val_loss: 0.4347 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8527 - accuracy: 0.0000e+00 - val_loss: 0.4343 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8521 - accuracy: 0.0000e+00 - val_loss: 0.4336 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8516 - accuracy: 0.0000e+00 - val_loss: 0.4330 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8509 - accuracy: 0.0000e+00 - val_loss: 0.4322 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8504 - accuracy: 0.0000e+00 - val_loss: 0.4325 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8497 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8493 - accuracy: 0.0000e+00 - val_loss: 0.4306 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8488 - accuracy: 0.0000e+00 - val_loss: 0.4307 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8482 - accuracy: 0.0000e+00 - val_loss: 0.4293 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8477 - accuracy: 0.0000e+00 - val_loss: 0.4290 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8472 - accuracy: 0.0000e+00 - val_loss: 0.4281 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8466 - accuracy: 0.0000e+00 - val_loss: 0.4275 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8461 - accuracy: 0.0000e+00 - val_loss: 0.4270 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8456 - accuracy: 0.0000e+00 - val_loss: 0.4262 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8451 - accuracy: 0.0000e+00 - val_loss: 0.4250 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8446 - accuracy: 0.0000e+00 - val_loss: 0.4249 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8441 - accuracy: 0.0000e+00 - val_loss: 0.4246 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8437 - accuracy: 0.0000e+00 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8433 - accuracy: 0.0000e+00 - val_loss: 0.4214 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8428 - accuracy: 0.0000e+00 - val_loss: 0.4212 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8423 - accuracy: 0.0000e+00 - val_loss: 0.4212 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8419 - accuracy: 0.0000e+00 - val_loss: 0.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8414 - accuracy: 0.0000e+00 - val_loss: 0.4195 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8409 - accuracy: 0.0000e+00 - val_loss: 0.4188 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8405 - accuracy: 0.0000e+00 - val_loss: 0.4176 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8401 - accuracy: 0.0000e+00 - val_loss: 0.4177 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8396 - accuracy: 0.0000e+00 - val_loss: 0.4170 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8393 - accuracy: 0.0000e+00 - val_loss: 0.4160 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8388 - accuracy: 0.0000e+00 - val_loss: 0.4152 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8386 - accuracy: 0.0000e+00 - val_loss: 0.4137 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8381 - accuracy: 0.0000e+00 - val_loss: 0.4138 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8376 - accuracy: 0.0000e+00 - val_loss: 0.4130 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8369 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8365 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8361 - accuracy: 0.0000e+00 - val_loss: 0.4107 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8358 - accuracy: 0.0000e+00 - val_loss: 0.4103 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8353 - accuracy: 0.0000e+00 - val_loss: 0.4100 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8350 - accuracy: 0.0000e+00 - val_loss: 0.4089 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.4075 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8342 - accuracy: 0.0000e+00 - val_loss: 0.4072 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8339 - accuracy: 0.0000e+00 - val_loss: 0.4064 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8335 - accuracy: 0.0000e+00 - val_loss: 0.4062 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8332 - accuracy: 0.0000e+00 - val_loss: 0.4052 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8328 - accuracy: 0.0000e+00 - val_loss: 0.4050 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.4048 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8322 - accuracy: 0.0000e+00 - val_loss: 0.4042 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8318 - accuracy: 0.0000e+00 - val_loss: 0.4034 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8315 - accuracy: 0.0000e+00 - val_loss: 0.4032 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.4029 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8308 - accuracy: 0.0000e+00 - val_loss: 0.4032 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.4013 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8302 - accuracy: 0.0000e+00 - val_loss: 0.4004 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8299 - accuracy: 0.0000e+00 - val_loss: 0.4005 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8297 - accuracy: 0.0000e+00 - val_loss: 0.4003 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8293 - accuracy: 0.0000e+00 - val_loss: 0.3999 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8290 - accuracy: 0.0000e+00 - val_loss: 0.3998 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8287 - accuracy: 0.0000e+00 - val_loss: 0.3999 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8284 - accuracy: 0.0000e+00 - val_loss: 0.3995 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8282 - accuracy: 0.0000e+00 - val_loss: 0.3987 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8279 - accuracy: 0.0000e+00 - val_loss: 0.3988 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8276 - accuracy: 0.0000e+00 - val_loss: 0.4001 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8275 - accuracy: 0.0000e+00 - val_loss: 0.3981 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8271 - accuracy: 0.0000e+00 - val_loss: 0.3976 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8268 - accuracy: 0.0000e+00 - val_loss: 0.3980 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8266 - accuracy: 0.0000e+00 - val_loss: 0.3984 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8262 - accuracy: 0.0000e+00 - val_loss: 0.3996 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8260 - accuracy: 0.0000e+00 - val_loss: 0.4002 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8257 - accuracy: 0.0000e+00 - val_loss: 0.3994 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8255 - accuracy: 0.0000e+00 - val_loss: 0.4004 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8252 - accuracy: 0.0000e+00 - val_loss: 0.4005 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8250 - accuracy: 0.0000e+00 - val_loss: 0.4015 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8247 - accuracy: 0.0000e+00 - val_loss: 0.4014 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8244 - accuracy: 0.0000e+00 - val_loss: 0.4027 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8242 - accuracy: 0.0000e+00 - val_loss: 0.4029 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8239 - accuracy: 0.0000e+00 - val_loss: 0.4028 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8237 - accuracy: 0.0000e+00 - val_loss: 0.4025 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8234 - accuracy: 0.0000e+00 - val_loss: 0.4033 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8232 - accuracy: 0.0000e+00 - val_loss: 0.4041 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8230 - accuracy: 0.0000e+00 - val_loss: 0.4034 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8227 - accuracy: 0.0000e+00 - val_loss: 0.4048 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 0.4052 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8222 - accuracy: 0.0000e+00 - val_loss: 0.4059 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8220 - accuracy: 0.0000e+00 - val_loss: 0.4071 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8217 - accuracy: 0.0000e+00 - val_loss: 0.4071 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8215 - accuracy: 0.0000e+00 - val_loss: 0.4076 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8212 - accuracy: 0.0000e+00 - val_loss: 0.4083 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8210 - accuracy: 0.0000e+00 - val_loss: 0.4093 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8208 - accuracy: 0.0000e+00 - val_loss: 0.4101 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8205 - accuracy: 0.0000e+00 - val_loss: 0.4108 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8203 - accuracy: 0.0000e+00 - val_loss: 0.4112 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8200 - accuracy: 0.0000e+00 - val_loss: 0.4114 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8199 - accuracy: 0.0000e+00 - val_loss: 0.4127 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8196 - accuracy: 0.0000e+00 - val_loss: 0.4132 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8193 - accuracy: 0.0000e+00 - val_loss: 0.4137 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8191 - accuracy: 0.0000e+00 - val_loss: 0.4141 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 0.4148 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8186 - accuracy: 0.0000e+00 - val_loss: 0.4165 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8184 - accuracy: 0.0000e+00 - val_loss: 0.4171 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8182 - accuracy: 0.0000e+00 - val_loss: 0.4173 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8180 - accuracy: 0.0000e+00 - val_loss: 0.4191 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8177 - accuracy: 0.0000e+00 - val_loss: 0.4196 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8175 - accuracy: 0.0000e+00 - val_loss: 0.4204 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8173 - accuracy: 0.0000e+00 - val_loss: 0.4213 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8170 - accuracy: 0.0000e+00 - val_loss: 0.4223 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8168 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8166 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8164 - accuracy: 0.0000e+00 - val_loss: 0.4239 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8162 - accuracy: 0.0000e+00 - val_loss: 0.4254 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8160 - accuracy: 0.0000e+00 - val_loss: 0.4267 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8158 - accuracy: 0.0000e+00 - val_loss: 0.4267 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8156 - accuracy: 0.0000e+00 - val_loss: 0.4275 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8154 - accuracy: 0.0000e+00 - val_loss: 0.4280 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8152 - accuracy: 0.0000e+00 - val_loss: 0.4283 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8150 - accuracy: 0.0000e+00 - val_loss: 0.4297 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8148 - accuracy: 0.0000e+00 - val_loss: 0.4307 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8147 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8145 - accuracy: 0.0000e+00 - val_loss: 0.4322 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8143 - accuracy: 0.0000e+00 - val_loss: 0.4334 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8141 - accuracy: 0.0000e+00 - val_loss: 0.4338 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8139 - accuracy: 0.0000e+00 - val_loss: 0.4341 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8138 - accuracy: 0.0000e+00 - val_loss: 0.4353 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8137 - accuracy: 0.0000e+00 - val_loss: 0.4342 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8135 - accuracy: 0.0000e+00 - val_loss: 0.4347 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/MSFT_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4347 - accuracy: 0.0000e+00\n",
      "[0.4346708059310913, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 21)                462       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.2835 - accuracy: 0.0000e+00 - val_loss: 0.7559 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2555 - accuracy: 0.0000e+00 - val_loss: 0.7601 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2332 - accuracy: 0.0000e+00 - val_loss: 0.7643 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2125 - accuracy: 0.0000e+00 - val_loss: 0.7683 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1973 - accuracy: 0.0000e+00 - val_loss: 0.7728 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1806 - accuracy: 0.0000e+00 - val_loss: 0.7773 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1653 - accuracy: 0.0000e+00 - val_loss: 0.7814 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1533 - accuracy: 0.0000e+00 - val_loss: 0.7860 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1415 - accuracy: 0.0000e+00 - val_loss: 0.7905 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1273 - accuracy: 0.0000e+00 - val_loss: 0.7945 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1158 - accuracy: 0.0000e+00 - val_loss: 0.7985 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1064 - accuracy: 0.0000e+00 - val_loss: 0.8028 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0954 - accuracy: 0.0000e+00 - val_loss: 0.8067 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0860 - accuracy: 0.0000e+00 - val_loss: 0.8102 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0781 - accuracy: 0.0000e+00 - val_loss: 0.8141 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0703 - accuracy: 0.0000e+00 - val_loss: 0.8183 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0605 - accuracy: 0.0000e+00 - val_loss: 0.8218 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0537 - accuracy: 0.0000e+00 - val_loss: 0.8254 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0469 - accuracy: 0.0000e+00 - val_loss: 0.8289 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0385 - accuracy: 0.0000e+00 - val_loss: 0.8317 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0324 - accuracy: 0.0000e+00 - val_loss: 0.8343 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0261 - accuracy: 0.0000e+00 - val_loss: 0.8369 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0198 - accuracy: 0.0000e+00 - val_loss: 0.8396 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0141 - accuracy: 0.0000e+00 - val_loss: 0.8420 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0084 - accuracy: 0.0000e+00 - val_loss: 0.8446 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0027 - accuracy: 0.0000e+00 - val_loss: 0.8465 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9981 - accuracy: 0.0000e+00 - val_loss: 0.8485 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9925 - accuracy: 0.0000e+00 - val_loss: 0.8507 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9883 - accuracy: 0.0000e+00 - val_loss: 0.8533 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9830 - accuracy: 0.0000e+00 - val_loss: 0.8550 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9792 - accuracy: 0.0000e+00 - val_loss: 0.8574 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9752 - accuracy: 0.0000e+00 - val_loss: 0.8592 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9715 - accuracy: 0.0000e+00 - val_loss: 0.8609 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9672 - accuracy: 0.0000e+00 - val_loss: 0.8628 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9633 - accuracy: 0.0000e+00 - val_loss: 0.8645 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9601 - accuracy: 0.0000e+00 - val_loss: 0.8664 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9567 - accuracy: 0.0000e+00 - val_loss: 0.8679 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9538 - accuracy: 0.0000e+00 - val_loss: 0.8696 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9502 - accuracy: 0.0000e+00 - val_loss: 0.8717 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9470 - accuracy: 0.0000e+00 - val_loss: 0.8735 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9441 - accuracy: 0.0000e+00 - val_loss: 0.8748 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9412 - accuracy: 0.0000e+00 - val_loss: 0.8762 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9388 - accuracy: 0.0000e+00 - val_loss: 0.8773 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9361 - accuracy: 0.0000e+00 - val_loss: 0.8791 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9328 - accuracy: 0.0000e+00 - val_loss: 0.8807 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9306 - accuracy: 0.0000e+00 - val_loss: 0.8826 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9287 - accuracy: 0.0000e+00 - val_loss: 0.8834 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9256 - accuracy: 0.0000e+00 - val_loss: 0.8851 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9236 - accuracy: 0.0000e+00 - val_loss: 0.8869 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9211 - accuracy: 0.0000e+00 - val_loss: 0.8882 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9195 - accuracy: 0.0000e+00 - val_loss: 0.8889 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9167 - accuracy: 0.0000e+00 - val_loss: 0.8906 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9146 - accuracy: 0.0000e+00 - val_loss: 0.8925 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9126 - accuracy: 0.0000e+00 - val_loss: 0.8942 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9106 - accuracy: 0.0000e+00 - val_loss: 0.8951 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9089 - accuracy: 0.0000e+00 - val_loss: 0.8959 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9068 - accuracy: 0.0000e+00 - val_loss: 0.8967 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9048 - accuracy: 0.0000e+00 - val_loss: 0.8980 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9027 - accuracy: 0.0000e+00 - val_loss: 0.9000 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9010 - accuracy: 0.0000e+00 - val_loss: 0.9006 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8991 - accuracy: 0.0000e+00 - val_loss: 0.9024 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8974 - accuracy: 0.0000e+00 - val_loss: 0.9039 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8960 - accuracy: 0.0000e+00 - val_loss: 0.9051 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8938 - accuracy: 0.0000e+00 - val_loss: 0.9071 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.9081 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8903 - accuracy: 0.0000e+00 - val_loss: 0.9094 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8893 - accuracy: 0.0000e+00 - val_loss: 0.9103 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8873 - accuracy: 0.0000e+00 - val_loss: 0.9118 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.9136 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8841 - accuracy: 0.0000e+00 - val_loss: 0.9150 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8828 - accuracy: 0.0000e+00 - val_loss: 0.9170 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8817 - accuracy: 0.0000e+00 - val_loss: 0.9176 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.9189 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8786 - accuracy: 0.0000e+00 - val_loss: 0.9203 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8774 - accuracy: 0.0000e+00 - val_loss: 0.9212 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8760 - accuracy: 0.0000e+00 - val_loss: 0.9226 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.9247 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.9264 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8724 - accuracy: 0.0000e+00 - val_loss: 0.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8711 - accuracy: 0.0000e+00 - val_loss: 0.9284 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.9298 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8687 - accuracy: 0.0000e+00 - val_loss: 0.9311 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8675 - accuracy: 0.0000e+00 - val_loss: 0.9332 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8665 - accuracy: 0.0000e+00 - val_loss: 0.9341 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.9360 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8646 - accuracy: 0.0000e+00 - val_loss: 0.9366 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8634 - accuracy: 0.0000e+00 - val_loss: 0.9376 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8624 - accuracy: 0.0000e+00 - val_loss: 0.9387 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8615 - accuracy: 0.0000e+00 - val_loss: 0.9410 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8605 - accuracy: 0.0000e+00 - val_loss: 0.9417 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8595 - accuracy: 0.0000e+00 - val_loss: 0.9428 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8585 - accuracy: 0.0000e+00 - val_loss: 0.9439 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8576 - accuracy: 0.0000e+00 - val_loss: 0.9459 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8567 - accuracy: 0.0000e+00 - val_loss: 0.9473 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8560 - accuracy: 0.0000e+00 - val_loss: 0.9482 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8549 - accuracy: 0.0000e+00 - val_loss: 0.9498 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8541 - accuracy: 0.0000e+00 - val_loss: 0.9507 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8533 - accuracy: 0.0000e+00 - val_loss: 0.9523 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.9533 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8515 - accuracy: 0.0000e+00 - val_loss: 0.9540 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8510 - accuracy: 0.0000e+00 - val_loss: 0.9552 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8500 - accuracy: 0.0000e+00 - val_loss: 0.9568 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8492 - accuracy: 0.0000e+00 - val_loss: 0.9585 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8484 - accuracy: 0.0000e+00 - val_loss: 0.9599 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8478 - accuracy: 0.0000e+00 - val_loss: 0.9603 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8469 - accuracy: 0.0000e+00 - val_loss: 0.9612 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8461 - accuracy: 0.0000e+00 - val_loss: 0.9624 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8453 - accuracy: 0.0000e+00 - val_loss: 0.9638 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8446 - accuracy: 0.0000e+00 - val_loss: 0.9647 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8438 - accuracy: 0.0000e+00 - val_loss: 0.9659 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8432 - accuracy: 0.0000e+00 - val_loss: 0.9669 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8425 - accuracy: 0.0000e+00 - val_loss: 0.9684 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8418 - accuracy: 0.0000e+00 - val_loss: 0.9692 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8412 - accuracy: 0.0000e+00 - val_loss: 0.9708 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8404 - accuracy: 0.0000e+00 - val_loss: 0.9721 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8398 - accuracy: 0.0000e+00 - val_loss: 0.9730 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8391 - accuracy: 0.0000e+00 - val_loss: 0.9741 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8385 - accuracy: 0.0000e+00 - val_loss: 0.9754 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8381 - accuracy: 0.0000e+00 - val_loss: 0.9769 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8371 - accuracy: 0.0000e+00 - val_loss: 0.9774 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8367 - accuracy: 0.0000e+00 - val_loss: 0.9783 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8361 - accuracy: 0.0000e+00 - val_loss: 0.9793 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8356 - accuracy: 0.0000e+00 - val_loss: 0.9807 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8348 - accuracy: 0.0000e+00 - val_loss: 0.9812 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8343 - accuracy: 0.0000e+00 - val_loss: 0.9819 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8339 - accuracy: 0.0000e+00 - val_loss: 0.9836 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8332 - accuracy: 0.0000e+00 - val_loss: 0.9842 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8327 - accuracy: 0.0000e+00 - val_loss: 0.9852 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8320 - accuracy: 0.0000e+00 - val_loss: 0.9863 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8315 - accuracy: 0.0000e+00 - val_loss: 0.9874 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8309 - accuracy: 0.0000e+00 - val_loss: 0.9884 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.9889 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8300 - accuracy: 0.0000e+00 - val_loss: 0.9897 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8294 - accuracy: 0.0000e+00 - val_loss: 0.9904 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8289 - accuracy: 0.0000e+00 - val_loss: 0.9920 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8284 - accuracy: 0.0000e+00 - val_loss: 0.9930 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8280 - accuracy: 0.0000e+00 - val_loss: 0.9937 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8276 - accuracy: 0.0000e+00 - val_loss: 0.9942 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8270 - accuracy: 0.0000e+00 - val_loss: 0.9957 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8265 - accuracy: 0.0000e+00 - val_loss: 0.9961 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8261 - accuracy: 0.0000e+00 - val_loss: 0.9971 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8256 - accuracy: 0.0000e+00 - val_loss: 0.9984 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8253 - accuracy: 0.0000e+00 - val_loss: 0.9990 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8247 - accuracy: 0.0000e+00 - val_loss: 0.9999 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8244 - accuracy: 0.0000e+00 - val_loss: 1.0003 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8237 - accuracy: 0.0000e+00 - val_loss: 1.0013 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8236 - accuracy: 0.0000e+00 - val_loss: 1.0025 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8230 - accuracy: 0.0000e+00 - val_loss: 1.0027 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 1.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8221 - accuracy: 0.0000e+00 - val_loss: 1.0046 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8217 - accuracy: 0.0000e+00 - val_loss: 1.0052 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8213 - accuracy: 0.0000e+00 - val_loss: 1.0062 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8208 - accuracy: 0.0000e+00 - val_loss: 1.0069 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8205 - accuracy: 0.0000e+00 - val_loss: 1.0070 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8201 - accuracy: 0.0000e+00 - val_loss: 1.0084 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8197 - accuracy: 0.0000e+00 - val_loss: 1.0089 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8192 - accuracy: 0.0000e+00 - val_loss: 1.0099 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 1.0102 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8184 - accuracy: 0.0000e+00 - val_loss: 1.0110 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8181 - accuracy: 0.0000e+00 - val_loss: 1.0118 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8178 - accuracy: 0.0000e+00 - val_loss: 1.0123 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8174 - accuracy: 0.0000e+00 - val_loss: 1.0131 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8170 - accuracy: 0.0000e+00 - val_loss: 1.0136 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8168 - accuracy: 0.0000e+00 - val_loss: 1.0141 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8163 - accuracy: 0.0000e+00 - val_loss: 1.0148 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8160 - accuracy: 0.0000e+00 - val_loss: 1.0159 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8157 - accuracy: 0.0000e+00 - val_loss: 1.0166 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8152 - accuracy: 0.0000e+00 - val_loss: 1.0175 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8149 - accuracy: 0.0000e+00 - val_loss: 1.0181 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8146 - accuracy: 0.0000e+00 - val_loss: 1.0184 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8142 - accuracy: 0.0000e+00 - val_loss: 1.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8139 - accuracy: 0.0000e+00 - val_loss: 1.0196 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8136 - accuracy: 0.0000e+00 - val_loss: 1.0200 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8133 - accuracy: 0.0000e+00 - val_loss: 1.0209 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8130 - accuracy: 0.0000e+00 - val_loss: 1.0214 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8128 - accuracy: 0.0000e+00 - val_loss: 1.0221 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8123 - accuracy: 0.0000e+00 - val_loss: 1.0223 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8120 - accuracy: 0.0000e+00 - val_loss: 1.0229 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8118 - accuracy: 0.0000e+00 - val_loss: 1.0234 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8115 - accuracy: 0.0000e+00 - val_loss: 1.0239 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8111 - accuracy: 0.0000e+00 - val_loss: 1.0249 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8109 - accuracy: 0.0000e+00 - val_loss: 1.0253 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8107 - accuracy: 0.0000e+00 - val_loss: 1.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8104 - accuracy: 0.0000e+00 - val_loss: 1.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8102 - accuracy: 0.0000e+00 - val_loss: 1.0272 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8098 - accuracy: 0.0000e+00 - val_loss: 1.0277 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8096 - accuracy: 0.0000e+00 - val_loss: 1.0280 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8092 - accuracy: 0.0000e+00 - val_loss: 1.0284 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8090 - accuracy: 0.0000e+00 - val_loss: 1.0285 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8088 - accuracy: 0.0000e+00 - val_loss: 1.0297 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8085 - accuracy: 0.0000e+00 - val_loss: 1.0301 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8082 - accuracy: 0.0000e+00 - val_loss: 1.0306 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8080 - accuracy: 0.0000e+00 - val_loss: 1.0312 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8077 - accuracy: 0.0000e+00 - val_loss: 1.0318 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8075 - accuracy: 0.0000e+00 - val_loss: 1.0321 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8073 - accuracy: 0.0000e+00 - val_loss: 1.0327 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8071 - accuracy: 0.0000e+00 - val_loss: 1.0332 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8068 - accuracy: 0.0000e+00 - val_loss: 1.0336 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8065 - accuracy: 0.0000e+00 - val_loss: 1.0337 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8063 - accuracy: 0.0000e+00 - val_loss: 1.0342 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8061 - accuracy: 0.0000e+00 - val_loss: 1.0348 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8060 - accuracy: 0.0000e+00 - val_loss: 1.0354 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8057 - accuracy: 0.0000e+00 - val_loss: 1.0355 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8055 - accuracy: 0.0000e+00 - val_loss: 1.0358 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8052 - accuracy: 0.0000e+00 - val_loss: 1.0365 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8050 - accuracy: 0.0000e+00 - val_loss: 1.0368 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8049 - accuracy: 0.0000e+00 - val_loss: 1.0371 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8047 - accuracy: 0.0000e+00 - val_loss: 1.0376 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8044 - accuracy: 0.0000e+00 - val_loss: 1.0382 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8043 - accuracy: 0.0000e+00 - val_loss: 1.0385 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8041 - accuracy: 0.0000e+00 - val_loss: 1.0392 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8039 - accuracy: 0.0000e+00 - val_loss: 1.0390 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8037 - accuracy: 0.0000e+00 - val_loss: 1.0395 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8035 - accuracy: 0.0000e+00 - val_loss: 1.0399 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8033 - accuracy: 0.0000e+00 - val_loss: 1.0404 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8031 - accuracy: 0.0000e+00 - val_loss: 1.0409 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8029 - accuracy: 0.0000e+00 - val_loss: 1.0415 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8028 - accuracy: 0.0000e+00 - val_loss: 1.0414 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8026 - accuracy: 0.0000e+00 - val_loss: 1.0421 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8024 - accuracy: 0.0000e+00 - val_loss: 1.0424 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8023 - accuracy: 0.0000e+00 - val_loss: 1.0429 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8021 - accuracy: 0.0000e+00 - val_loss: 1.0425 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8019 - accuracy: 0.0000e+00 - val_loss: 1.0429 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8017 - accuracy: 0.0000e+00 - val_loss: 1.0435 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8015 - accuracy: 0.0000e+00 - val_loss: 1.0438 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8014 - accuracy: 0.0000e+00 - val_loss: 1.0445 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8013 - accuracy: 0.0000e+00 - val_loss: 1.0444 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8011 - accuracy: 0.0000e+00 - val_loss: 1.0448 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8009 - accuracy: 0.0000e+00 - val_loss: 1.0450 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8008 - accuracy: 0.0000e+00 - val_loss: 1.0453 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8006 - accuracy: 0.0000e+00 - val_loss: 1.0456 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8004 - accuracy: 0.0000e+00 - val_loss: 1.0457 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8003 - accuracy: 0.0000e+00 - val_loss: 1.0463 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8001 - accuracy: 0.0000e+00 - val_loss: 1.0462 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8000 - accuracy: 0.0000e+00 - val_loss: 1.0465 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7998 - accuracy: 0.0000e+00 - val_loss: 1.0469 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7997 - accuracy: 0.0000e+00 - val_loss: 1.0474 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7996 - accuracy: 0.0000e+00 - val_loss: 1.0476 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7994 - accuracy: 0.0000e+00 - val_loss: 1.0475 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7993 - accuracy: 0.0000e+00 - val_loss: 1.0479 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7992 - accuracy: 0.0000e+00 - val_loss: 1.0485 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7990 - accuracy: 0.0000e+00 - val_loss: 1.0484 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7989 - accuracy: 0.0000e+00 - val_loss: 1.0488 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7987 - accuracy: 0.0000e+00 - val_loss: 1.0490 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7986 - accuracy: 0.0000e+00 - val_loss: 1.0493 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7985 - accuracy: 0.0000e+00 - val_loss: 1.0493 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.7983 - accuracy: 0.0000e+00 - val_loss: 1.0496 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7982 - accuracy: 0.0000e+00 - val_loss: 1.0495 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7980 - accuracy: 0.0000e+00 - val_loss: 1.0498 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7979 - accuracy: 0.0000e+00 - val_loss: 1.0504 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7978 - accuracy: 0.0000e+00 - val_loss: 1.0505 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7977 - accuracy: 0.0000e+00 - val_loss: 1.0510 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.7975 - accuracy: 0.0000e+00 - val_loss: 1.0511 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7974 - accuracy: 0.0000e+00 - val_loss: 1.0507 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.7973 - accuracy: 0.0000e+00 - val_loss: 1.0511 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.7971 - accuracy: 0.0000e+00 - val_loss: 1.0515 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/UNH_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0515 - accuracy: 0.0000e+00\n",
      "[1.0514696836471558, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 21)                462       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.0922 - accuracy: 0.0000e+00 - val_loss: 0.4416 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0799 - accuracy: 0.0000e+00 - val_loss: 0.4519 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0693 - accuracy: 0.0000e+00 - val_loss: 0.4590 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0615 - accuracy: 0.0000e+00 - val_loss: 0.4671 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0536 - accuracy: 0.0000e+00 - val_loss: 0.4736 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0467 - accuracy: 0.0000e+00 - val_loss: 0.4795 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0406 - accuracy: 0.0000e+00 - val_loss: 0.4852 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0360 - accuracy: 0.0000e+00 - val_loss: 0.4905 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0315 - accuracy: 0.0000e+00 - val_loss: 0.4955 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0257 - accuracy: 0.0000e+00 - val_loss: 0.4996 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0232 - accuracy: 0.0000e+00 - val_loss: 0.5044 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0178 - accuracy: 0.0000e+00 - val_loss: 0.5072 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0147 - accuracy: 0.0000e+00 - val_loss: 0.5096 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0113 - accuracy: 0.0000e+00 - val_loss: 0.5126 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0083 - accuracy: 0.0000e+00 - val_loss: 0.5156 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0052 - accuracy: 0.0000e+00 - val_loss: 0.5182 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0022 - accuracy: 0.0000e+00 - val_loss: 0.5202 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9999 - accuracy: 0.0000e+00 - val_loss: 0.5216 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9973 - accuracy: 0.0000e+00 - val_loss: 0.5236 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9944 - accuracy: 0.0000e+00 - val_loss: 0.5257 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9920 - accuracy: 0.0000e+00 - val_loss: 0.5266 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9900 - accuracy: 0.0000e+00 - val_loss: 0.5284 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9877 - accuracy: 0.0000e+00 - val_loss: 0.5302 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9854 - accuracy: 0.0000e+00 - val_loss: 0.5315 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9836 - accuracy: 0.0000e+00 - val_loss: 0.5323 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9812 - accuracy: 0.0000e+00 - val_loss: 0.5337 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9792 - accuracy: 0.0000e+00 - val_loss: 0.5342 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9778 - accuracy: 0.0000e+00 - val_loss: 0.5356 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9753 - accuracy: 0.0000e+00 - val_loss: 0.5359 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9732 - accuracy: 0.0000e+00 - val_loss: 0.5369 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9716 - accuracy: 0.0000e+00 - val_loss: 0.5379 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9697 - accuracy: 0.0000e+00 - val_loss: 0.5389 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9677 - accuracy: 0.0000e+00 - val_loss: 0.5401 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9660 - accuracy: 0.0000e+00 - val_loss: 0.5401 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9637 - accuracy: 0.0000e+00 - val_loss: 0.5409 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9619 - accuracy: 0.0000e+00 - val_loss: 0.5419 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9602 - accuracy: 0.0000e+00 - val_loss: 0.5423 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9582 - accuracy: 0.0000e+00 - val_loss: 0.5431 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9562 - accuracy: 0.0000e+00 - val_loss: 0.5436 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9547 - accuracy: 0.0000e+00 - val_loss: 0.5447 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9531 - accuracy: 0.0000e+00 - val_loss: 0.5462 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9513 - accuracy: 0.0000e+00 - val_loss: 0.5474 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9493 - accuracy: 0.0000e+00 - val_loss: 0.5476 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9476 - accuracy: 0.0000e+00 - val_loss: 0.5482 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9460 - accuracy: 0.0000e+00 - val_loss: 0.5494 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9441 - accuracy: 0.0000e+00 - val_loss: 0.5504 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9427 - accuracy: 0.0000e+00 - val_loss: 0.5510 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9408 - accuracy: 0.0000e+00 - val_loss: 0.5527 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9389 - accuracy: 0.0000e+00 - val_loss: 0.5537 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9373 - accuracy: 0.0000e+00 - val_loss: 0.5542 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9358 - accuracy: 0.0000e+00 - val_loss: 0.5554 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9342 - accuracy: 0.0000e+00 - val_loss: 0.5566 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9330 - accuracy: 0.0000e+00 - val_loss: 0.5576 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9310 - accuracy: 0.0000e+00 - val_loss: 0.5588 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9295 - accuracy: 0.0000e+00 - val_loss: 0.5592 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9277 - accuracy: 0.0000e+00 - val_loss: 0.5602 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9261 - accuracy: 0.0000e+00 - val_loss: 0.5611 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9245 - accuracy: 0.0000e+00 - val_loss: 0.5622 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9234 - accuracy: 0.0000e+00 - val_loss: 0.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9215 - accuracy: 0.0000e+00 - val_loss: 0.5644 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9200 - accuracy: 0.0000e+00 - val_loss: 0.5656 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9186 - accuracy: 0.0000e+00 - val_loss: 0.5667 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9172 - accuracy: 0.0000e+00 - val_loss: 0.5675 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9156 - accuracy: 0.0000e+00 - val_loss: 0.5692 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9142 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9129 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9112 - accuracy: 0.0000e+00 - val_loss: 0.5722 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9098 - accuracy: 0.0000e+00 - val_loss: 0.5732 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9086 - accuracy: 0.0000e+00 - val_loss: 0.5748 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9070 - accuracy: 0.0000e+00 - val_loss: 0.5757 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9059 - accuracy: 0.0000e+00 - val_loss: 0.5767 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9044 - accuracy: 0.0000e+00 - val_loss: 0.5775 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.5797 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9003 - accuracy: 0.0000e+00 - val_loss: 0.5804 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8993 - accuracy: 0.0000e+00 - val_loss: 0.5815 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8977 - accuracy: 0.0000e+00 - val_loss: 0.5824 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8965 - accuracy: 0.0000e+00 - val_loss: 0.5834 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8952 - accuracy: 0.0000e+00 - val_loss: 0.5843 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8939 - accuracy: 0.0000e+00 - val_loss: 0.5846 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8927 - accuracy: 0.0000e+00 - val_loss: 0.5857 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.5866 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8905 - accuracy: 0.0000e+00 - val_loss: 0.5876 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8893 - accuracy: 0.0000e+00 - val_loss: 0.5879 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8880 - accuracy: 0.0000e+00 - val_loss: 0.5886 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8870 - accuracy: 0.0000e+00 - val_loss: 0.5897 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.5904 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.5906 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8840 - accuracy: 0.0000e+00 - val_loss: 0.5915 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8828 - accuracy: 0.0000e+00 - val_loss: 0.5919 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.5929 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8807 - accuracy: 0.0000e+00 - val_loss: 0.5935 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.5944 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8788 - accuracy: 0.0000e+00 - val_loss: 0.5949 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.5964 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8758 - accuracy: 0.0000e+00 - val_loss: 0.5972 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.5975 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8739 - accuracy: 0.0000e+00 - val_loss: 0.5977 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8729 - accuracy: 0.0000e+00 - val_loss: 0.5988 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8721 - accuracy: 0.0000e+00 - val_loss: 0.5993 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.5996 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8703 - accuracy: 0.0000e+00 - val_loss: 0.6006 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8693 - accuracy: 0.0000e+00 - val_loss: 0.6012 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8684 - accuracy: 0.0000e+00 - val_loss: 0.6023 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8675 - accuracy: 0.0000e+00 - val_loss: 0.6025 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8668 - accuracy: 0.0000e+00 - val_loss: 0.6034 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8657 - accuracy: 0.0000e+00 - val_loss: 0.6038 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8651 - accuracy: 0.0000e+00 - val_loss: 0.6043 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8643 - accuracy: 0.0000e+00 - val_loss: 0.6055 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8635 - accuracy: 0.0000e+00 - val_loss: 0.6060 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8626 - accuracy: 0.0000e+00 - val_loss: 0.6068 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8616 - accuracy: 0.0000e+00 - val_loss: 0.6071 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8608 - accuracy: 0.0000e+00 - val_loss: 0.6079 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8600 - accuracy: 0.0000e+00 - val_loss: 0.6084 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8592 - accuracy: 0.0000e+00 - val_loss: 0.6088 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8586 - accuracy: 0.0000e+00 - val_loss: 0.6098 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8576 - accuracy: 0.0000e+00 - val_loss: 0.6101 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8568 - accuracy: 0.0000e+00 - val_loss: 0.6104 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8561 - accuracy: 0.0000e+00 - val_loss: 0.6105 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.6109 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8548 - accuracy: 0.0000e+00 - val_loss: 0.6122 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.6125 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8532 - accuracy: 0.0000e+00 - val_loss: 0.6128 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8524 - accuracy: 0.0000e+00 - val_loss: 0.6132 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8516 - accuracy: 0.0000e+00 - val_loss: 0.6138 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8509 - accuracy: 0.0000e+00 - val_loss: 0.6135 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8503 - accuracy: 0.0000e+00 - val_loss: 0.6144 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8495 - accuracy: 0.0000e+00 - val_loss: 0.6146 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8487 - accuracy: 0.0000e+00 - val_loss: 0.6152 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8479 - accuracy: 0.0000e+00 - val_loss: 0.6152 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8473 - accuracy: 0.0000e+00 - val_loss: 0.6157 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8466 - accuracy: 0.0000e+00 - val_loss: 0.6166 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8458 - accuracy: 0.0000e+00 - val_loss: 0.6167 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8449 - accuracy: 0.0000e+00 - val_loss: 0.6172 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8444 - accuracy: 0.0000e+00 - val_loss: 0.6169 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8438 - accuracy: 0.0000e+00 - val_loss: 0.6178 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8431 - accuracy: 0.0000e+00 - val_loss: 0.6174 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8423 - accuracy: 0.0000e+00 - val_loss: 0.6180 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8416 - accuracy: 0.0000e+00 - val_loss: 0.6184 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8409 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8403 - accuracy: 0.0000e+00 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8395 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8388 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8384 - accuracy: 0.0000e+00 - val_loss: 0.6195 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8378 - accuracy: 0.0000e+00 - val_loss: 0.6197 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.6202 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8367 - accuracy: 0.0000e+00 - val_loss: 0.6201 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8361 - accuracy: 0.0000e+00 - val_loss: 0.6205 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8357 - accuracy: 0.0000e+00 - val_loss: 0.6215 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8351 - accuracy: 0.0000e+00 - val_loss: 0.6214 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.6212 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8341 - accuracy: 0.0000e+00 - val_loss: 0.6211 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8336 - accuracy: 0.0000e+00 - val_loss: 0.6218 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8332 - accuracy: 0.0000e+00 - val_loss: 0.6225 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8326 - accuracy: 0.0000e+00 - val_loss: 0.6225 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8322 - accuracy: 0.0000e+00 - val_loss: 0.6230 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8317 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8307 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8303 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8299 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8295 - accuracy: 0.0000e+00 - val_loss: 0.6242 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8290 - accuracy: 0.0000e+00 - val_loss: 0.6244 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8286 - accuracy: 0.0000e+00 - val_loss: 0.6246 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8283 - accuracy: 0.0000e+00 - val_loss: 0.6249 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8278 - accuracy: 0.0000e+00 - val_loss: 0.6246 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8274 - accuracy: 0.0000e+00 - val_loss: 0.6248 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8270 - accuracy: 0.0000e+00 - val_loss: 0.6248 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8266 - accuracy: 0.0000e+00 - val_loss: 0.6250 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.6254 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8259 - accuracy: 0.0000e+00 - val_loss: 0.6260 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8257 - accuracy: 0.0000e+00 - val_loss: 0.6256 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8251 - accuracy: 0.0000e+00 - val_loss: 0.6260 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8248 - accuracy: 0.0000e+00 - val_loss: 0.6263 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8245 - accuracy: 0.0000e+00 - val_loss: 0.6264 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8243 - accuracy: 0.0000e+00 - val_loss: 0.6264 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8238 - accuracy: 0.0000e+00 - val_loss: 0.6270 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8235 - accuracy: 0.0000e+00 - val_loss: 0.6272 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8231 - accuracy: 0.0000e+00 - val_loss: 0.6272 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8228 - accuracy: 0.0000e+00 - val_loss: 0.6275 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8225 - accuracy: 0.0000e+00 - val_loss: 0.6283 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8222 - accuracy: 0.0000e+00 - val_loss: 0.6282 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8219 - accuracy: 0.0000e+00 - val_loss: 0.6286 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8215 - accuracy: 0.0000e+00 - val_loss: 0.6285 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8212 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8209 - accuracy: 0.0000e+00 - val_loss: 0.6292 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8207 - accuracy: 0.0000e+00 - val_loss: 0.6291 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8203 - accuracy: 0.0000e+00 - val_loss: 0.6295 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8200 - accuracy: 0.0000e+00 - val_loss: 0.6297 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8198 - accuracy: 0.0000e+00 - val_loss: 0.6299 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8195 - accuracy: 0.0000e+00 - val_loss: 0.6302 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8192 - accuracy: 0.0000e+00 - val_loss: 0.6303 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 0.6309 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8187 - accuracy: 0.0000e+00 - val_loss: 0.6313 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8184 - accuracy: 0.0000e+00 - val_loss: 0.6314 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8181 - accuracy: 0.0000e+00 - val_loss: 0.6313 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8178 - accuracy: 0.0000e+00 - val_loss: 0.6315 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8176 - accuracy: 0.0000e+00 - val_loss: 0.6319 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8174 - accuracy: 0.0000e+00 - val_loss: 0.6319 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8171 - accuracy: 0.0000e+00 - val_loss: 0.6323 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8168 - accuracy: 0.0000e+00 - val_loss: 0.6322 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8166 - accuracy: 0.0000e+00 - val_loss: 0.6327 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8163 - accuracy: 0.0000e+00 - val_loss: 0.6327 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8161 - accuracy: 0.0000e+00 - val_loss: 0.6328 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8158 - accuracy: 0.0000e+00 - val_loss: 0.6329 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8156 - accuracy: 0.0000e+00 - val_loss: 0.6335 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8154 - accuracy: 0.0000e+00 - val_loss: 0.6338 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8152 - accuracy: 0.0000e+00 - val_loss: 0.6341 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8149 - accuracy: 0.0000e+00 - val_loss: 0.6339 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8147 - accuracy: 0.0000e+00 - val_loss: 0.6343 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8145 - accuracy: 0.0000e+00 - val_loss: 0.6350 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8142 - accuracy: 0.0000e+00 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8140 - accuracy: 0.0000e+00 - val_loss: 0.6349 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8138 - accuracy: 0.0000e+00 - val_loss: 0.6350 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8136 - accuracy: 0.0000e+00 - val_loss: 0.6354 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8133 - accuracy: 0.0000e+00 - val_loss: 0.6354 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8131 - accuracy: 0.0000e+00 - val_loss: 0.6356 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8129 - accuracy: 0.0000e+00 - val_loss: 0.6358 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8128 - accuracy: 0.0000e+00 - val_loss: 0.6363 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8126 - accuracy: 0.0000e+00 - val_loss: 0.6362 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8124 - accuracy: 0.0000e+00 - val_loss: 0.6363 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8122 - accuracy: 0.0000e+00 - val_loss: 0.6371 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8120 - accuracy: 0.0000e+00 - val_loss: 0.6376 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8118 - accuracy: 0.0000e+00 - val_loss: 0.6373 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8116 - accuracy: 0.0000e+00 - val_loss: 0.6372 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8114 - accuracy: 0.0000e+00 - val_loss: 0.6375 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8113 - accuracy: 0.0000e+00 - val_loss: 0.6380 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8111 - accuracy: 0.0000e+00 - val_loss: 0.6380 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8109 - accuracy: 0.0000e+00 - val_loss: 0.6384 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8107 - accuracy: 0.0000e+00 - val_loss: 0.6386 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8106 - accuracy: 0.0000e+00 - val_loss: 0.6388 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8104 - accuracy: 0.0000e+00 - val_loss: 0.6387 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8103 - accuracy: 0.0000e+00 - val_loss: 0.6393 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8101 - accuracy: 0.0000e+00 - val_loss: 0.6393 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8099 - accuracy: 0.0000e+00 - val_loss: 0.6398 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8098 - accuracy: 0.0000e+00 - val_loss: 0.6401 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8096 - accuracy: 0.0000e+00 - val_loss: 0.6404 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8095 - accuracy: 0.0000e+00 - val_loss: 0.6402 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8093 - accuracy: 0.0000e+00 - val_loss: 0.6405 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8092 - accuracy: 0.0000e+00 - val_loss: 0.6404 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8090 - accuracy: 0.0000e+00 - val_loss: 0.6409 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8089 - accuracy: 0.0000e+00 - val_loss: 0.6410 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8087 - accuracy: 0.0000e+00 - val_loss: 0.6415 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8086 - accuracy: 0.0000e+00 - val_loss: 0.6416 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8085 - accuracy: 0.0000e+00 - val_loss: 0.6420 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8083 - accuracy: 0.0000e+00 - val_loss: 0.6423 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8081 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8080 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8079 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8078 - accuracy: 0.0000e+00 - val_loss: 0.6425 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8077 - accuracy: 0.0000e+00 - val_loss: 0.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8075 - accuracy: 0.0000e+00 - val_loss: 0.6436 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8074 - accuracy: 0.0000e+00 - val_loss: 0.6437 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8072 - accuracy: 0.0000e+00 - val_loss: 0.6437 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8072 - accuracy: 0.0000e+00 - val_loss: 0.6441 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/JNJ_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6441 - accuracy: 0.0000e+00\n",
      "[0.6441423892974854, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 21)                462       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1.1148 - accuracy: 0.0000e+00 - val_loss: 0.5871 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1070 - accuracy: 0.0000e+00 - val_loss: 0.5912 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0984 - accuracy: 0.0000e+00 - val_loss: 0.5950 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0914 - accuracy: 0.0000e+00 - val_loss: 0.5989 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0857 - accuracy: 0.0000e+00 - val_loss: 0.6027 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0789 - accuracy: 0.0000e+00 - val_loss: 0.6055 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0744 - accuracy: 0.0000e+00 - val_loss: 0.6095 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0668 - accuracy: 0.0000e+00 - val_loss: 0.6126 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0621 - accuracy: 0.0000e+00 - val_loss: 0.6165 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0543 - accuracy: 0.0000e+00 - val_loss: 0.6199 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0502 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.0441 - accuracy: 0.0000e+00 - val_loss: 0.6256 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.0390 - accuracy: 0.0000e+00 - val_loss: 0.6280 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.0355 - accuracy: 0.0000e+00 - val_loss: 0.6308 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0315 - accuracy: 0.0000e+00 - val_loss: 0.6333 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0283 - accuracy: 0.0000e+00 - val_loss: 0.6352 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0250 - accuracy: 0.0000e+00 - val_loss: 0.6373 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0219 - accuracy: 0.0000e+00 - val_loss: 0.6390 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0195 - accuracy: 0.0000e+00 - val_loss: 0.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0169 - accuracy: 0.0000e+00 - val_loss: 0.6428 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0139 - accuracy: 0.0000e+00 - val_loss: 0.6442 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0119 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0101 - accuracy: 0.0000e+00 - val_loss: 0.6471 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0081 - accuracy: 0.0000e+00 - val_loss: 0.6484 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0061 - accuracy: 0.0000e+00 - val_loss: 0.6495 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0044 - accuracy: 0.0000e+00 - val_loss: 0.6508 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0029 - accuracy: 0.0000e+00 - val_loss: 0.6517 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0016 - accuracy: 0.0000e+00 - val_loss: 0.6528 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9996 - accuracy: 0.0000e+00 - val_loss: 0.6536 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9984 - accuracy: 0.0000e+00 - val_loss: 0.6544 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9972 - accuracy: 0.0000e+00 - val_loss: 0.6552 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9959 - accuracy: 0.0000e+00 - val_loss: 0.6560 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9946 - accuracy: 0.0000e+00 - val_loss: 0.6567 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9931 - accuracy: 0.0000e+00 - val_loss: 0.6575 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9919 - accuracy: 0.0000e+00 - val_loss: 0.6580 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9910 - accuracy: 0.0000e+00 - val_loss: 0.6586 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9897 - accuracy: 0.0000e+00 - val_loss: 0.6590 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9886 - accuracy: 0.0000e+00 - val_loss: 0.6593 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9874 - accuracy: 0.0000e+00 - val_loss: 0.6600 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9862 - accuracy: 0.0000e+00 - val_loss: 0.6603 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9855 - accuracy: 0.0000e+00 - val_loss: 0.6608 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9844 - accuracy: 0.0000e+00 - val_loss: 0.6611 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9832 - accuracy: 0.0000e+00 - val_loss: 0.6615 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9827 - accuracy: 0.0000e+00 - val_loss: 0.6619 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9810 - accuracy: 0.0000e+00 - val_loss: 0.6621 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9799 - accuracy: 0.0000e+00 - val_loss: 0.6624 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9789 - accuracy: 0.0000e+00 - val_loss: 0.6625 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9777 - accuracy: 0.0000e+00 - val_loss: 0.6629 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9768 - accuracy: 0.0000e+00 - val_loss: 0.6631 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9755 - accuracy: 0.0000e+00 - val_loss: 0.6632 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9745 - accuracy: 0.0000e+00 - val_loss: 0.6635 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9732 - accuracy: 0.0000e+00 - val_loss: 0.6636 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9722 - accuracy: 0.0000e+00 - val_loss: 0.6639 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9710 - accuracy: 0.0000e+00 - val_loss: 0.6640 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9697 - accuracy: 0.0000e+00 - val_loss: 0.6641 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9689 - accuracy: 0.0000e+00 - val_loss: 0.6643 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9675 - accuracy: 0.0000e+00 - val_loss: 0.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9661 - accuracy: 0.0000e+00 - val_loss: 0.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9644 - accuracy: 0.0000e+00 - val_loss: 0.6648 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9636 - accuracy: 0.0000e+00 - val_loss: 0.6650 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9615 - accuracy: 0.0000e+00 - val_loss: 0.6651 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9604 - accuracy: 0.0000e+00 - val_loss: 0.6653 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9588 - accuracy: 0.0000e+00 - val_loss: 0.6654 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9570 - accuracy: 0.0000e+00 - val_loss: 0.6655 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9558 - accuracy: 0.0000e+00 - val_loss: 0.6656 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9540 - accuracy: 0.0000e+00 - val_loss: 0.6656 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9522 - accuracy: 0.0000e+00 - val_loss: 0.6656 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9507 - accuracy: 0.0000e+00 - val_loss: 0.6658 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9490 - accuracy: 0.0000e+00 - val_loss: 0.6658 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9480 - accuracy: 0.0000e+00 - val_loss: 0.6659 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9455 - accuracy: 0.0000e+00 - val_loss: 0.6660 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9441 - accuracy: 0.0000e+00 - val_loss: 0.6659 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9417 - accuracy: 0.0000e+00 - val_loss: 0.6658 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9404 - accuracy: 0.0000e+00 - val_loss: 0.6659 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9384 - accuracy: 0.0000e+00 - val_loss: 0.6658 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9366 - accuracy: 0.0000e+00 - val_loss: 0.6657 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9349 - accuracy: 0.0000e+00 - val_loss: 0.6654 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9330 - accuracy: 0.0000e+00 - val_loss: 0.6654 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9316 - accuracy: 0.0000e+00 - val_loss: 0.6655 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9304 - accuracy: 0.0000e+00 - val_loss: 0.6651 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9288 - accuracy: 0.0000e+00 - val_loss: 0.6647 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9274 - accuracy: 0.0000e+00 - val_loss: 0.6648 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9259 - accuracy: 0.0000e+00 - val_loss: 0.6645 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9242 - accuracy: 0.0000e+00 - val_loss: 0.6642 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9232 - accuracy: 0.0000e+00 - val_loss: 0.6643 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9219 - accuracy: 0.0000e+00 - val_loss: 0.6643 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9208 - accuracy: 0.0000e+00 - val_loss: 0.6643 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9197 - accuracy: 0.0000e+00 - val_loss: 0.6640 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9186 - accuracy: 0.0000e+00 - val_loss: 0.6636 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9174 - accuracy: 0.0000e+00 - val_loss: 0.6632 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9167 - accuracy: 0.0000e+00 - val_loss: 0.6632 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9153 - accuracy: 0.0000e+00 - val_loss: 0.6630 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9145 - accuracy: 0.0000e+00 - val_loss: 0.6630 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9133 - accuracy: 0.0000e+00 - val_loss: 0.6627 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9123 - accuracy: 0.0000e+00 - val_loss: 0.6625 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9112 - accuracy: 0.0000e+00 - val_loss: 0.6623 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9103 - accuracy: 0.0000e+00 - val_loss: 0.6618 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9094 - accuracy: 0.0000e+00 - val_loss: 0.6612 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9083 - accuracy: 0.0000e+00 - val_loss: 0.6614 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9074 - accuracy: 0.0000e+00 - val_loss: 0.6612 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9065 - accuracy: 0.0000e+00 - val_loss: 0.6605 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9057 - accuracy: 0.0000e+00 - val_loss: 0.6601 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9049 - accuracy: 0.0000e+00 - val_loss: 0.6599 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9041 - accuracy: 0.0000e+00 - val_loss: 0.6599 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9034 - accuracy: 0.0000e+00 - val_loss: 0.6595 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9024 - accuracy: 0.0000e+00 - val_loss: 0.6592 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9016 - accuracy: 0.0000e+00 - val_loss: 0.6590 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9010 - accuracy: 0.0000e+00 - val_loss: 0.6585 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.6587 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8996 - accuracy: 0.0000e+00 - val_loss: 0.6578 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8989 - accuracy: 0.0000e+00 - val_loss: 0.6582 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8980 - accuracy: 0.0000e+00 - val_loss: 0.6579 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8972 - accuracy: 0.0000e+00 - val_loss: 0.6574 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8964 - accuracy: 0.0000e+00 - val_loss: 0.6570 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8955 - accuracy: 0.0000e+00 - val_loss: 0.6570 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8949 - accuracy: 0.0000e+00 - val_loss: 0.6567 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8943 - accuracy: 0.0000e+00 - val_loss: 0.6566 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8936 - accuracy: 0.0000e+00 - val_loss: 0.6559 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8929 - accuracy: 0.0000e+00 - val_loss: 0.6556 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8921 - accuracy: 0.0000e+00 - val_loss: 0.6555 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8914 - accuracy: 0.0000e+00 - val_loss: 0.6556 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8908 - accuracy: 0.0000e+00 - val_loss: 0.6555 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8902 - accuracy: 0.0000e+00 - val_loss: 0.6551 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8894 - accuracy: 0.0000e+00 - val_loss: 0.6551 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.6546 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.6543 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8879 - accuracy: 0.0000e+00 - val_loss: 0.6537 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8870 - accuracy: 0.0000e+00 - val_loss: 0.6537 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8868 - accuracy: 0.0000e+00 - val_loss: 0.6534 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.6531 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.6530 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8847 - accuracy: 0.0000e+00 - val_loss: 0.6530 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8839 - accuracy: 0.0000e+00 - val_loss: 0.6526 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8833 - accuracy: 0.0000e+00 - val_loss: 0.6521 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8827 - accuracy: 0.0000e+00 - val_loss: 0.6519 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8822 - accuracy: 0.0000e+00 - val_loss: 0.6520 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8818 - accuracy: 0.0000e+00 - val_loss: 0.6515 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8811 - accuracy: 0.0000e+00 - val_loss: 0.6512 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8804 - accuracy: 0.0000e+00 - val_loss: 0.6512 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8800 - accuracy: 0.0000e+00 - val_loss: 0.6512 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8795 - accuracy: 0.0000e+00 - val_loss: 0.6511 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.6508 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8782 - accuracy: 0.0000e+00 - val_loss: 0.6503 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8779 - accuracy: 0.0000e+00 - val_loss: 0.6501 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8773 - accuracy: 0.0000e+00 - val_loss: 0.6497 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.6492 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8763 - accuracy: 0.0000e+00 - val_loss: 0.6491 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8758 - accuracy: 0.0000e+00 - val_loss: 0.6490 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8755 - accuracy: 0.0000e+00 - val_loss: 0.6488 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8751 - accuracy: 0.0000e+00 - val_loss: 0.6486 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.6486 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.6484 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.6482 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.6483 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8728 - accuracy: 0.0000e+00 - val_loss: 0.6480 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.6479 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8721 - accuracy: 0.0000e+00 - val_loss: 0.6478 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8717 - accuracy: 0.0000e+00 - val_loss: 0.6476 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.6475 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8711 - accuracy: 0.0000e+00 - val_loss: 0.6476 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.6474 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.6472 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.6472 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8694 - accuracy: 0.0000e+00 - val_loss: 0.6471 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8690 - accuracy: 0.0000e+00 - val_loss: 0.6470 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8686 - accuracy: 0.0000e+00 - val_loss: 0.6469 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8683 - accuracy: 0.0000e+00 - val_loss: 0.6467 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8679 - accuracy: 0.0000e+00 - val_loss: 0.6467 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8676 - accuracy: 0.0000e+00 - val_loss: 0.6466 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8673 - accuracy: 0.0000e+00 - val_loss: 0.6465 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8669 - accuracy: 0.0000e+00 - val_loss: 0.6465 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8666 - accuracy: 0.0000e+00 - val_loss: 0.6464 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8663 - accuracy: 0.0000e+00 - val_loss: 0.6463 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8660 - accuracy: 0.0000e+00 - val_loss: 0.6462 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8659 - accuracy: 0.0000e+00 - val_loss: 0.6463 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.6462 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8651 - accuracy: 0.0000e+00 - val_loss: 0.6461 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8647 - accuracy: 0.0000e+00 - val_loss: 0.6460 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.6460 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8642 - accuracy: 0.0000e+00 - val_loss: 0.6460 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8639 - accuracy: 0.0000e+00 - val_loss: 0.6459 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8635 - accuracy: 0.0000e+00 - val_loss: 0.6458 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8633 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.8629 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8626 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8624 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8621 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8616 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8614 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8610 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8609 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8606 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8603 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8601 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8598 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8594 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8591 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8588 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8587 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8584 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8582 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8581 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8578 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8576 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8573 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8571 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8570 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8568 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8565 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8564 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8562 - accuracy: 0.0000e+00 - val_loss: 0.6448 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8559 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8558 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8556 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8555 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8551 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8550 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8547 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8545 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8544 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8542 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8540 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8537 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8536 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8534 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8531 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8530 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8529 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8527 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8524 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8523 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8522 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8520 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8519 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8517 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8516 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8515 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8514 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8512 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8511 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8510 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8509 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8508 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - val_loss: 0.6458 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8505 - accuracy: 0.0000e+00 - val_loss: 0.6459 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8505 - accuracy: 0.0000e+00 - val_loss: 0.6459 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8504 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8502 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8501 - accuracy: 0.0000e+00 - val_loss: 0.6458 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8500 - accuracy: 0.0000e+00 - val_loss: 0.6459 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/XOM_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6459 - accuracy: 0.0000e+00\n",
      "[0.6459115147590637, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 24ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/JPM_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 25ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1.3658 - accuracy: 0.0000e+00 - val_loss: 0.3757 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3481 - accuracy: 0.0000e+00 - val_loss: 0.3778 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3311 - accuracy: 0.0000e+00 - val_loss: 0.3803 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3161 - accuracy: 0.0000e+00 - val_loss: 0.3831 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3019 - accuracy: 0.0000e+00 - val_loss: 0.3867 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2878 - accuracy: 0.0000e+00 - val_loss: 0.3902 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2742 - accuracy: 0.0000e+00 - val_loss: 0.3943 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2608 - accuracy: 0.0000e+00 - val_loss: 0.3989 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2488 - accuracy: 0.0000e+00 - val_loss: 0.4045 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2352 - accuracy: 0.0000e+00 - val_loss: 0.4090 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2245 - accuracy: 0.0000e+00 - val_loss: 0.4136 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2148 - accuracy: 0.0000e+00 - val_loss: 0.4212 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2025 - accuracy: 0.0000e+00 - val_loss: 0.4292 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1923 - accuracy: 0.0000e+00 - val_loss: 0.4359 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1835 - accuracy: 0.0000e+00 - val_loss: 0.4437 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1730 - accuracy: 0.0000e+00 - val_loss: 0.4509 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1631 - accuracy: 0.0000e+00 - val_loss: 0.4583 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1536 - accuracy: 0.0000e+00 - val_loss: 0.4625 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1466 - accuracy: 0.0000e+00 - val_loss: 0.4702 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1380 - accuracy: 0.0000e+00 - val_loss: 0.4773 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1297 - accuracy: 0.0000e+00 - val_loss: 0.4844 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1208 - accuracy: 0.0000e+00 - val_loss: 0.4907 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1140 - accuracy: 0.0000e+00 - val_loss: 0.4980 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1061 - accuracy: 0.0000e+00 - val_loss: 0.5047 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0978 - accuracy: 0.0000e+00 - val_loss: 0.5119 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0898 - accuracy: 0.0000e+00 - val_loss: 0.5180 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0817 - accuracy: 0.0000e+00 - val_loss: 0.5242 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0749 - accuracy: 0.0000e+00 - val_loss: 0.5296 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0684 - accuracy: 0.0000e+00 - val_loss: 0.5347 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0648 - accuracy: 0.0000e+00 - val_loss: 0.5409 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0596 - accuracy: 0.0000e+00 - val_loss: 0.5458 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0556 - accuracy: 0.0000e+00 - val_loss: 0.5511 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0505 - accuracy: 0.0000e+00 - val_loss: 0.5550 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0472 - accuracy: 0.0000e+00 - val_loss: 0.5587 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0437 - accuracy: 0.0000e+00 - val_loss: 0.5625 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0410 - accuracy: 0.0000e+00 - val_loss: 0.5666 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0376 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0346 - accuracy: 0.0000e+00 - val_loss: 0.5734 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0327 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0302 - accuracy: 0.0000e+00 - val_loss: 0.5804 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0276 - accuracy: 0.0000e+00 - val_loss: 0.5831 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0251 - accuracy: 0.0000e+00 - val_loss: 0.5857 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0231 - accuracy: 0.0000e+00 - val_loss: 0.5883 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0212 - accuracy: 0.0000e+00 - val_loss: 0.5902 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0193 - accuracy: 0.0000e+00 - val_loss: 0.5925 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0179 - accuracy: 0.0000e+00 - val_loss: 0.5950 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0161 - accuracy: 0.0000e+00 - val_loss: 0.5972 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0143 - accuracy: 0.0000e+00 - val_loss: 0.5988 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0131 - accuracy: 0.0000e+00 - val_loss: 0.6009 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0117 - accuracy: 0.0000e+00 - val_loss: 0.6027 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0103 - accuracy: 0.0000e+00 - val_loss: 0.6044 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0088 - accuracy: 0.0000e+00 - val_loss: 0.6057 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0078 - accuracy: 0.0000e+00 - val_loss: 0.6073 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0068 - accuracy: 0.0000e+00 - val_loss: 0.6090 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0055 - accuracy: 0.0000e+00 - val_loss: 0.6106 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0045 - accuracy: 0.0000e+00 - val_loss: 0.6121 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0032 - accuracy: 0.0000e+00 - val_loss: 0.6133 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0022 - accuracy: 0.0000e+00 - val_loss: 0.6145 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0012 - accuracy: 0.0000e+00 - val_loss: 0.6155 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0004 - accuracy: 0.0000e+00 - val_loss: 0.6169 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9994 - accuracy: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9987 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9978 - accuracy: 0.0000e+00 - val_loss: 0.6204 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9969 - accuracy: 0.0000e+00 - val_loss: 0.6214 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9961 - accuracy: 0.0000e+00 - val_loss: 0.6222 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9953 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9947 - accuracy: 0.0000e+00 - val_loss: 0.6241 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9938 - accuracy: 0.0000e+00 - val_loss: 0.6247 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9930 - accuracy: 0.0000e+00 - val_loss: 0.6255 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9925 - accuracy: 0.0000e+00 - val_loss: 0.6264 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9917 - accuracy: 0.0000e+00 - val_loss: 0.6272 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9909 - accuracy: 0.0000e+00 - val_loss: 0.6279 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9904 - accuracy: 0.0000e+00 - val_loss: 0.6284 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9894 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9889 - accuracy: 0.0000e+00 - val_loss: 0.6298 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9881 - accuracy: 0.0000e+00 - val_loss: 0.6305 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9873 - accuracy: 0.0000e+00 - val_loss: 0.6311 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9867 - accuracy: 0.0000e+00 - val_loss: 0.6320 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9858 - accuracy: 0.0000e+00 - val_loss: 0.6325 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9849 - accuracy: 0.0000e+00 - val_loss: 0.6330 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9842 - accuracy: 0.0000e+00 - val_loss: 0.6337 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9834 - accuracy: 0.0000e+00 - val_loss: 0.6341 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9823 - accuracy: 0.0000e+00 - val_loss: 0.6346 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9815 - accuracy: 0.0000e+00 - val_loss: 0.6351 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9805 - accuracy: 0.0000e+00 - val_loss: 0.6356 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9796 - accuracy: 0.0000e+00 - val_loss: 0.6361 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9786 - accuracy: 0.0000e+00 - val_loss: 0.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9774 - accuracy: 0.0000e+00 - val_loss: 0.6368 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9765 - accuracy: 0.0000e+00 - val_loss: 0.6372 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9752 - accuracy: 0.0000e+00 - val_loss: 0.6376 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9741 - accuracy: 0.0000e+00 - val_loss: 0.6380 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9732 - accuracy: 0.0000e+00 - val_loss: 0.6382 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9718 - accuracy: 0.0000e+00 - val_loss: 0.6387 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9705 - accuracy: 0.0000e+00 - val_loss: 0.6389 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9697 - accuracy: 0.0000e+00 - val_loss: 0.6393 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9684 - accuracy: 0.0000e+00 - val_loss: 0.6396 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9674 - accuracy: 0.0000e+00 - val_loss: 0.6399 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9661 - accuracy: 0.0000e+00 - val_loss: 0.6403 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9651 - accuracy: 0.0000e+00 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9646 - accuracy: 0.0000e+00 - val_loss: 0.6410 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9632 - accuracy: 0.0000e+00 - val_loss: 0.6413 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9624 - accuracy: 0.0000e+00 - val_loss: 0.6415 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9615 - accuracy: 0.0000e+00 - val_loss: 0.6417 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9606 - accuracy: 0.0000e+00 - val_loss: 0.6422 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9599 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9591 - accuracy: 0.0000e+00 - val_loss: 0.6428 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9581 - accuracy: 0.0000e+00 - val_loss: 0.6432 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9574 - accuracy: 0.0000e+00 - val_loss: 0.6436 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9565 - accuracy: 0.0000e+00 - val_loss: 0.6440 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9558 - accuracy: 0.0000e+00 - val_loss: 0.6440 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9551 - accuracy: 0.0000e+00 - val_loss: 0.6445 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9544 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9538 - accuracy: 0.0000e+00 - val_loss: 0.6451 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9532 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9525 - accuracy: 0.0000e+00 - val_loss: 0.6459 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9521 - accuracy: 0.0000e+00 - val_loss: 0.6464 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9512 - accuracy: 0.0000e+00 - val_loss: 0.6467 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9505 - accuracy: 0.0000e+00 - val_loss: 0.6470 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9499 - accuracy: 0.0000e+00 - val_loss: 0.6474 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9493 - accuracy: 0.0000e+00 - val_loss: 0.6477 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9487 - accuracy: 0.0000e+00 - val_loss: 0.6480 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9480 - accuracy: 0.0000e+00 - val_loss: 0.6485 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9474 - accuracy: 0.0000e+00 - val_loss: 0.6489 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9469 - accuracy: 0.0000e+00 - val_loss: 0.6491 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9461 - accuracy: 0.0000e+00 - val_loss: 0.6494 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9456 - accuracy: 0.0000e+00 - val_loss: 0.6499 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9449 - accuracy: 0.0000e+00 - val_loss: 0.6503 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9442 - accuracy: 0.0000e+00 - val_loss: 0.6508 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9437 - accuracy: 0.0000e+00 - val_loss: 0.6513 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9428 - accuracy: 0.0000e+00 - val_loss: 0.6516 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9424 - accuracy: 0.0000e+00 - val_loss: 0.6522 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9417 - accuracy: 0.0000e+00 - val_loss: 0.6526 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9413 - accuracy: 0.0000e+00 - val_loss: 0.6528 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9402 - accuracy: 0.0000e+00 - val_loss: 0.6532 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9398 - accuracy: 0.0000e+00 - val_loss: 0.6537 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9391 - accuracy: 0.0000e+00 - val_loss: 0.6540 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9387 - accuracy: 0.0000e+00 - val_loss: 0.6545 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9379 - accuracy: 0.0000e+00 - val_loss: 0.6547 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9373 - accuracy: 0.0000e+00 - val_loss: 0.6553 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9367 - accuracy: 0.0000e+00 - val_loss: 0.6555 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9360 - accuracy: 0.0000e+00 - val_loss: 0.6557 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9354 - accuracy: 0.0000e+00 - val_loss: 0.6564 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9348 - accuracy: 0.0000e+00 - val_loss: 0.6567 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9340 - accuracy: 0.0000e+00 - val_loss: 0.6573 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9334 - accuracy: 0.0000e+00 - val_loss: 0.6576 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.6578 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9328 - accuracy: 0.0000e+00 - val_loss: 0.6583 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9315 - accuracy: 0.0000e+00 - val_loss: 0.6586 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9309 - accuracy: 0.0000e+00 - val_loss: 0.6589 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9302 - accuracy: 0.0000e+00 - val_loss: 0.6593 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9296 - accuracy: 0.0000e+00 - val_loss: 0.6595 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9288 - accuracy: 0.0000e+00 - val_loss: 0.6599 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9283 - accuracy: 0.0000e+00 - val_loss: 0.6600 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9278 - accuracy: 0.0000e+00 - val_loss: 0.6608 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9269 - accuracy: 0.0000e+00 - val_loss: 0.6611 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9263 - accuracy: 0.0000e+00 - val_loss: 0.6611 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9256 - accuracy: 0.0000e+00 - val_loss: 0.6614 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9248 - accuracy: 0.0000e+00 - val_loss: 0.6618 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.6620 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9238 - accuracy: 0.0000e+00 - val_loss: 0.6621 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9234 - accuracy: 0.0000e+00 - val_loss: 0.6623 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9226 - accuracy: 0.0000e+00 - val_loss: 0.6627 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9216 - accuracy: 0.0000e+00 - val_loss: 0.6629 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9210 - accuracy: 0.0000e+00 - val_loss: 0.6629 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9204 - accuracy: 0.0000e+00 - val_loss: 0.6630 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9193 - accuracy: 0.0000e+00 - val_loss: 0.6631 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9191 - accuracy: 0.0000e+00 - val_loss: 0.6635 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9180 - accuracy: 0.0000e+00 - val_loss: 0.6635 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9172 - accuracy: 0.0000e+00 - val_loss: 0.6637 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9168 - accuracy: 0.0000e+00 - val_loss: 0.6640 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9163 - accuracy: 0.0000e+00 - val_loss: 0.6639 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9153 - accuracy: 0.0000e+00 - val_loss: 0.6640 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9147 - accuracy: 0.0000e+00 - val_loss: 0.6642 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9141 - accuracy: 0.0000e+00 - val_loss: 0.6645 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9138 - accuracy: 0.0000e+00 - val_loss: 0.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9130 - accuracy: 0.0000e+00 - val_loss: 0.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9123 - accuracy: 0.0000e+00 - val_loss: 0.6647 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9121 - accuracy: 0.0000e+00 - val_loss: 0.6651 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9109 - accuracy: 0.0000e+00 - val_loss: 0.6652 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9104 - accuracy: 0.0000e+00 - val_loss: 0.6653 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9098 - accuracy: 0.0000e+00 - val_loss: 0.6655 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9093 - accuracy: 0.0000e+00 - val_loss: 0.6656 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9093 - accuracy: 0.0000e+00 - val_loss: 0.6659 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9084 - accuracy: 0.0000e+00 - val_loss: 0.6661 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9076 - accuracy: 0.0000e+00 - val_loss: 0.6662 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9071 - accuracy: 0.0000e+00 - val_loss: 0.6662 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9066 - accuracy: 0.0000e+00 - val_loss: 0.6664 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9060 - accuracy: 0.0000e+00 - val_loss: 0.6668 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9056 - accuracy: 0.0000e+00 - val_loss: 0.6669 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9054 - accuracy: 0.0000e+00 - val_loss: 0.6669 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9047 - accuracy: 0.0000e+00 - val_loss: 0.6671 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9042 - accuracy: 0.0000e+00 - val_loss: 0.6671 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9041 - accuracy: 0.0000e+00 - val_loss: 0.6674 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9030 - accuracy: 0.0000e+00 - val_loss: 0.6677 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9025 - accuracy: 0.0000e+00 - val_loss: 0.6679 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9019 - accuracy: 0.0000e+00 - val_loss: 0.6681 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.6681 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9014 - accuracy: 0.0000e+00 - val_loss: 0.6684 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9008 - accuracy: 0.0000e+00 - val_loss: 0.6684 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.6686 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8999 - accuracy: 0.0000e+00 - val_loss: 0.6688 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9000 - accuracy: 0.0000e+00 - val_loss: 0.6691 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8990 - accuracy: 0.0000e+00 - val_loss: 0.6693 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8985 - accuracy: 0.0000e+00 - val_loss: 0.6693 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8979 - accuracy: 0.0000e+00 - val_loss: 0.6695 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8976 - accuracy: 0.0000e+00 - val_loss: 0.6696 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8971 - accuracy: 0.0000e+00 - val_loss: 0.6698 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8967 - accuracy: 0.0000e+00 - val_loss: 0.6699 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8962 - accuracy: 0.0000e+00 - val_loss: 0.6701 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8959 - accuracy: 0.0000e+00 - val_loss: 0.6703 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8955 - accuracy: 0.0000e+00 - val_loss: 0.6705 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8953 - accuracy: 0.0000e+00 - val_loss: 0.6708 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8948 - accuracy: 0.0000e+00 - val_loss: 0.6709 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8942 - accuracy: 0.0000e+00 - val_loss: 0.6711 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8941 - accuracy: 0.0000e+00 - val_loss: 0.6711 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8935 - accuracy: 0.0000e+00 - val_loss: 0.6713 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8932 - accuracy: 0.0000e+00 - val_loss: 0.6716 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8928 - accuracy: 0.0000e+00 - val_loss: 0.6718 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8927 - accuracy: 0.0000e+00 - val_loss: 0.6719 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.6720 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8915 - accuracy: 0.0000e+00 - val_loss: 0.6723 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8912 - accuracy: 0.0000e+00 - val_loss: 0.6725 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.6725 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.6726 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.6730 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8901 - accuracy: 0.0000e+00 - val_loss: 0.6734 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8895 - accuracy: 0.0000e+00 - val_loss: 0.6734 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8892 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8890 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8885 - accuracy: 0.0000e+00 - val_loss: 0.6738 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.6743 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8879 - accuracy: 0.0000e+00 - val_loss: 0.6745 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8875 - accuracy: 0.0000e+00 - val_loss: 0.6745 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8872 - accuracy: 0.0000e+00 - val_loss: 0.6747 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8871 - accuracy: 0.0000e+00 - val_loss: 0.6751 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8865 - accuracy: 0.0000e+00 - val_loss: 0.6754 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8863 - accuracy: 0.0000e+00 - val_loss: 0.6755 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8858 - accuracy: 0.0000e+00 - val_loss: 0.6758 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.6760 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.6763 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8848 - accuracy: 0.0000e+00 - val_loss: 0.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.6770 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8842 - accuracy: 0.0000e+00 - val_loss: 0.6771 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8838 - accuracy: 0.0000e+00 - val_loss: 0.6775 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8835 - accuracy: 0.0000e+00 - val_loss: 0.6779 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8832 - accuracy: 0.0000e+00 - val_loss: 0.6783 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8830 - accuracy: 0.0000e+00 - val_loss: 0.6787 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8828 - accuracy: 0.0000e+00 - val_loss: 0.6787 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8825 - accuracy: 0.0000e+00 - val_loss: 0.6791 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8823 - accuracy: 0.0000e+00 - val_loss: 0.6794 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8822 - accuracy: 0.0000e+00 - val_loss: 0.6800 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.6801 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.6805 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8813 - accuracy: 0.0000e+00 - val_loss: 0.6805 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8809 - accuracy: 0.0000e+00 - val_loss: 0.6809 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/CVX_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6809 - accuracy: 0.0000e+00\n",
      "[0.6808753609657288, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.2711 - accuracy: 0.0000e+00 - val_loss: 0.3002 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2412 - accuracy: 0.0000e+00 - val_loss: 0.3007 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2155 - accuracy: 0.0000e+00 - val_loss: 0.3019 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1957 - accuracy: 0.0000e+00 - val_loss: 0.3032 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1721 - accuracy: 0.0000e+00 - val_loss: 0.3048 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1534 - accuracy: 0.0000e+00 - val_loss: 0.3065 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1350 - accuracy: 0.0000e+00 - val_loss: 0.3077 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1178 - accuracy: 0.0000e+00 - val_loss: 0.3092 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1009 - accuracy: 0.0000e+00 - val_loss: 0.3114 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0868 - accuracy: 0.0000e+00 - val_loss: 0.3129 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0714 - accuracy: 0.0000e+00 - val_loss: 0.3147 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0609 - accuracy: 0.0000e+00 - val_loss: 0.3173 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0487 - accuracy: 0.0000e+00 - val_loss: 0.3202 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0361 - accuracy: 0.0000e+00 - val_loss: 0.3230 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0272 - accuracy: 0.0000e+00 - val_loss: 0.3257 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0179 - accuracy: 0.0000e+00 - val_loss: 0.3291 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0092 - accuracy: 0.0000e+00 - val_loss: 0.3312 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0011 - accuracy: 0.0000e+00 - val_loss: 0.3339 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9935 - accuracy: 0.0000e+00 - val_loss: 0.3370 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9871 - accuracy: 0.0000e+00 - val_loss: 0.3407 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9803 - accuracy: 0.0000e+00 - val_loss: 0.3444 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9738 - accuracy: 0.0000e+00 - val_loss: 0.3483 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9694 - accuracy: 0.0000e+00 - val_loss: 0.3517 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9636 - accuracy: 0.0000e+00 - val_loss: 0.3561 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9589 - accuracy: 0.0000e+00 - val_loss: 0.3594 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9545 - accuracy: 0.0000e+00 - val_loss: 0.3633 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9497 - accuracy: 0.0000e+00 - val_loss: 0.3662 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9459 - accuracy: 0.0000e+00 - val_loss: 0.3707 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9430 - accuracy: 0.0000e+00 - val_loss: 0.3732 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9392 - accuracy: 0.0000e+00 - val_loss: 0.3765 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9355 - accuracy: 0.0000e+00 - val_loss: 0.3786 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.3805 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9299 - accuracy: 0.0000e+00 - val_loss: 0.3833 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9275 - accuracy: 0.0000e+00 - val_loss: 0.3857 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9244 - accuracy: 0.0000e+00 - val_loss: 0.3880 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9220 - accuracy: 0.0000e+00 - val_loss: 0.3903 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9196 - accuracy: 0.0000e+00 - val_loss: 0.3925 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9172 - accuracy: 0.0000e+00 - val_loss: 0.3937 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9153 - accuracy: 0.0000e+00 - val_loss: 0.3945 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9134 - accuracy: 0.0000e+00 - val_loss: 0.3951 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9112 - accuracy: 0.0000e+00 - val_loss: 0.3966 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9094 - accuracy: 0.0000e+00 - val_loss: 0.3981 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9079 - accuracy: 0.0000e+00 - val_loss: 0.3977 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9059 - accuracy: 0.0000e+00 - val_loss: 0.3990 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9046 - accuracy: 0.0000e+00 - val_loss: 0.3998 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.4005 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9015 - accuracy: 0.0000e+00 - val_loss: 0.4016 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8999 - accuracy: 0.0000e+00 - val_loss: 0.4010 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8986 - accuracy: 0.0000e+00 - val_loss: 0.4014 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8973 - accuracy: 0.0000e+00 - val_loss: 0.4017 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8961 - accuracy: 0.0000e+00 - val_loss: 0.4024 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8949 - accuracy: 0.0000e+00 - val_loss: 0.4033 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8936 - accuracy: 0.0000e+00 - val_loss: 0.4049 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8924 - accuracy: 0.0000e+00 - val_loss: 0.4051 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8914 - accuracy: 0.0000e+00 - val_loss: 0.4064 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8905 - accuracy: 0.0000e+00 - val_loss: 0.4079 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8895 - accuracy: 0.0000e+00 - val_loss: 0.4084 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8886 - accuracy: 0.0000e+00 - val_loss: 0.4078 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8876 - accuracy: 0.0000e+00 - val_loss: 0.4081 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 0.4091 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8858 - accuracy: 0.0000e+00 - val_loss: 0.4095 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.4089 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8842 - accuracy: 0.0000e+00 - val_loss: 0.4096 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8833 - accuracy: 0.0000e+00 - val_loss: 0.4103 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8826 - accuracy: 0.0000e+00 - val_loss: 0.4101 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8817 - accuracy: 0.0000e+00 - val_loss: 0.4102 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8810 - accuracy: 0.0000e+00 - val_loss: 0.4115 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8804 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8797 - accuracy: 0.0000e+00 - val_loss: 0.4124 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.4133 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8782 - accuracy: 0.0000e+00 - val_loss: 0.4128 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.4137 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8770 - accuracy: 0.0000e+00 - val_loss: 0.4135 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8762 - accuracy: 0.0000e+00 - val_loss: 0.4133 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8757 - accuracy: 0.0000e+00 - val_loss: 0.4138 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8751 - accuracy: 0.0000e+00 - val_loss: 0.4141 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.4137 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8738 - accuracy: 0.0000e+00 - val_loss: 0.4143 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8732 - accuracy: 0.0000e+00 - val_loss: 0.4150 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8728 - accuracy: 0.0000e+00 - val_loss: 0.4153 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8721 - accuracy: 0.0000e+00 - val_loss: 0.4149 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8710 - accuracy: 0.0000e+00 - val_loss: 0.4149 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.4161 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.4156 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8694 - accuracy: 0.0000e+00 - val_loss: 0.4163 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8688 - accuracy: 0.0000e+00 - val_loss: 0.4170 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8681 - accuracy: 0.0000e+00 - val_loss: 0.4169 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8676 - accuracy: 0.0000e+00 - val_loss: 0.4181 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8670 - accuracy: 0.0000e+00 - val_loss: 0.4184 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8666 - accuracy: 0.0000e+00 - val_loss: 0.4185 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8660 - accuracy: 0.0000e+00 - val_loss: 0.4190 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8656 - accuracy: 0.0000e+00 - val_loss: 0.4204 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8651 - accuracy: 0.0000e+00 - val_loss: 0.4188 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8646 - accuracy: 0.0000e+00 - val_loss: 0.4184 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8642 - accuracy: 0.0000e+00 - val_loss: 0.4186 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8636 - accuracy: 0.0000e+00 - val_loss: 0.4198 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8633 - accuracy: 0.0000e+00 - val_loss: 0.4208 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8627 - accuracy: 0.0000e+00 - val_loss: 0.4198 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8622 - accuracy: 0.0000e+00 - val_loss: 0.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.4209 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8612 - accuracy: 0.0000e+00 - val_loss: 0.4215 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8608 - accuracy: 0.0000e+00 - val_loss: 0.4223 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8602 - accuracy: 0.0000e+00 - val_loss: 0.4214 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8597 - accuracy: 0.0000e+00 - val_loss: 0.4217 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8593 - accuracy: 0.0000e+00 - val_loss: 0.4219 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8588 - accuracy: 0.0000e+00 - val_loss: 0.4222 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8584 - accuracy: 0.0000e+00 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8582 - accuracy: 0.0000e+00 - val_loss: 0.4226 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8577 - accuracy: 0.0000e+00 - val_loss: 0.4226 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8571 - accuracy: 0.0000e+00 - val_loss: 0.4233 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8566 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8564 - accuracy: 0.0000e+00 - val_loss: 0.4234 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8557 - accuracy: 0.0000e+00 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8552 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8548 - accuracy: 0.0000e+00 - val_loss: 0.4238 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8545 - accuracy: 0.0000e+00 - val_loss: 0.4226 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.4233 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8536 - accuracy: 0.0000e+00 - val_loss: 0.4232 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8531 - accuracy: 0.0000e+00 - val_loss: 0.4232 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8527 - accuracy: 0.0000e+00 - val_loss: 0.4226 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8522 - accuracy: 0.0000e+00 - val_loss: 0.4237 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8519 - accuracy: 0.0000e+00 - val_loss: 0.4245 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8513 - accuracy: 0.0000e+00 - val_loss: 0.4238 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8510 - accuracy: 0.0000e+00 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8508 - accuracy: 0.0000e+00 - val_loss: 0.4230 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8501 - accuracy: 0.0000e+00 - val_loss: 0.4229 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8497 - accuracy: 0.0000e+00 - val_loss: 0.4238 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8493 - accuracy: 0.0000e+00 - val_loss: 0.4239 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8489 - accuracy: 0.0000e+00 - val_loss: 0.4235 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8485 - accuracy: 0.0000e+00 - val_loss: 0.4223 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8481 - accuracy: 0.0000e+00 - val_loss: 0.4224 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8478 - accuracy: 0.0000e+00 - val_loss: 0.4221 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8473 - accuracy: 0.0000e+00 - val_loss: 0.4219 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8470 - accuracy: 0.0000e+00 - val_loss: 0.4218 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8466 - accuracy: 0.0000e+00 - val_loss: 0.4225 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8463 - accuracy: 0.0000e+00 - val_loss: 0.4222 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8458 - accuracy: 0.0000e+00 - val_loss: 0.4218 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8456 - accuracy: 0.0000e+00 - val_loss: 0.4207 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8452 - accuracy: 0.0000e+00 - val_loss: 0.4208 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8448 - accuracy: 0.0000e+00 - val_loss: 0.4208 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8445 - accuracy: 0.0000e+00 - val_loss: 0.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8443 - accuracy: 0.0000e+00 - val_loss: 0.4211 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8437 - accuracy: 0.0000e+00 - val_loss: 0.4204 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8435 - accuracy: 0.0000e+00 - val_loss: 0.4197 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8431 - accuracy: 0.0000e+00 - val_loss: 0.4197 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8428 - accuracy: 0.0000e+00 - val_loss: 0.4192 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.4174 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8422 - accuracy: 0.0000e+00 - val_loss: 0.4170 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8418 - accuracy: 0.0000e+00 - val_loss: 0.4169 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8414 - accuracy: 0.0000e+00 - val_loss: 0.4159 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8411 - accuracy: 0.0000e+00 - val_loss: 0.4168 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8408 - accuracy: 0.0000e+00 - val_loss: 0.4165 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8404 - accuracy: 0.0000e+00 - val_loss: 0.4157 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8401 - accuracy: 0.0000e+00 - val_loss: 0.4161 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8399 - accuracy: 0.0000e+00 - val_loss: 0.4164 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8395 - accuracy: 0.0000e+00 - val_loss: 0.4166 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8393 - accuracy: 0.0000e+00 - val_loss: 0.4168 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8390 - accuracy: 0.0000e+00 - val_loss: 0.4157 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8386 - accuracy: 0.0000e+00 - val_loss: 0.4150 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8383 - accuracy: 0.0000e+00 - val_loss: 0.4148 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8381 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8377 - accuracy: 0.0000e+00 - val_loss: 0.4140 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8374 - accuracy: 0.0000e+00 - val_loss: 0.4136 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8372 - accuracy: 0.0000e+00 - val_loss: 0.4128 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8369 - accuracy: 0.0000e+00 - val_loss: 0.4134 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8366 - accuracy: 0.0000e+00 - val_loss: 0.4125 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8363 - accuracy: 0.0000e+00 - val_loss: 0.4124 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8361 - accuracy: 0.0000e+00 - val_loss: 0.4130 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8358 - accuracy: 0.0000e+00 - val_loss: 0.4113 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8357 - accuracy: 0.0000e+00 - val_loss: 0.4111 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8353 - accuracy: 0.0000e+00 - val_loss: 0.4102 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8351 - accuracy: 0.0000e+00 - val_loss: 0.4108 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8348 - accuracy: 0.0000e+00 - val_loss: 0.4112 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8345 - accuracy: 0.0000e+00 - val_loss: 0.4107 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8342 - accuracy: 0.0000e+00 - val_loss: 0.4111 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8340 - accuracy: 0.0000e+00 - val_loss: 0.4111 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8337 - accuracy: 0.0000e+00 - val_loss: 0.4102 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8335 - accuracy: 0.0000e+00 - val_loss: 0.4101 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8332 - accuracy: 0.0000e+00 - val_loss: 0.4102 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8329 - accuracy: 0.0000e+00 - val_loss: 0.4100 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8327 - accuracy: 0.0000e+00 - val_loss: 0.4097 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.4100 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8322 - accuracy: 0.0000e+00 - val_loss: 0.4096 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8320 - accuracy: 0.0000e+00 - val_loss: 0.4087 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8317 - accuracy: 0.0000e+00 - val_loss: 0.4089 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8315 - accuracy: 0.0000e+00 - val_loss: 0.4096 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.4084 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8310 - accuracy: 0.0000e+00 - val_loss: 0.4088 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8307 - accuracy: 0.0000e+00 - val_loss: 0.4078 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8305 - accuracy: 0.0000e+00 - val_loss: 0.4082 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8303 - accuracy: 0.0000e+00 - val_loss: 0.4093 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8300 - accuracy: 0.0000e+00 - val_loss: 0.4096 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8298 - accuracy: 0.0000e+00 - val_loss: 0.4091 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8295 - accuracy: 0.0000e+00 - val_loss: 0.4084 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8293 - accuracy: 0.0000e+00 - val_loss: 0.4079 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8291 - accuracy: 0.0000e+00 - val_loss: 0.4083 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8289 - accuracy: 0.0000e+00 - val_loss: 0.4086 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8286 - accuracy: 0.0000e+00 - val_loss: 0.4089 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8284 - accuracy: 0.0000e+00 - val_loss: 0.4083 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8282 - accuracy: 0.0000e+00 - val_loss: 0.4093 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8280 - accuracy: 0.0000e+00 - val_loss: 0.4089 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8277 - accuracy: 0.0000e+00 - val_loss: 0.4096 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8275 - accuracy: 0.0000e+00 - val_loss: 0.4095 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8273 - accuracy: 0.0000e+00 - val_loss: 0.4097 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8271 - accuracy: 0.0000e+00 - val_loss: 0.4101 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8269 - accuracy: 0.0000e+00 - val_loss: 0.4102 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8267 - accuracy: 0.0000e+00 - val_loss: 0.4103 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8264 - accuracy: 0.0000e+00 - val_loss: 0.4101 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.4103 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8260 - accuracy: 0.0000e+00 - val_loss: 0.4109 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8259 - accuracy: 0.0000e+00 - val_loss: 0.4104 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8256 - accuracy: 0.0000e+00 - val_loss: 0.4111 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8254 - accuracy: 0.0000e+00 - val_loss: 0.4118 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8252 - accuracy: 0.0000e+00 - val_loss: 0.4121 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8251 - accuracy: 0.0000e+00 - val_loss: 0.4121 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8249 - accuracy: 0.0000e+00 - val_loss: 0.4110 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8246 - accuracy: 0.0000e+00 - val_loss: 0.4117 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8245 - accuracy: 0.0000e+00 - val_loss: 0.4116 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8242 - accuracy: 0.0000e+00 - val_loss: 0.4113 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8240 - accuracy: 0.0000e+00 - val_loss: 0.4122 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8239 - accuracy: 0.0000e+00 - val_loss: 0.4128 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8237 - accuracy: 0.0000e+00 - val_loss: 0.4137 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8235 - accuracy: 0.0000e+00 - val_loss: 0.4142 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8233 - accuracy: 0.0000e+00 - val_loss: 0.4132 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8231 - accuracy: 0.0000e+00 - val_loss: 0.4137 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8229 - accuracy: 0.0000e+00 - val_loss: 0.4140 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8228 - accuracy: 0.0000e+00 - val_loss: 0.4141 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8225 - accuracy: 0.0000e+00 - val_loss: 0.4150 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 0.4141 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8222 - accuracy: 0.0000e+00 - val_loss: 0.4149 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8220 - accuracy: 0.0000e+00 - val_loss: 0.4143 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8219 - accuracy: 0.0000e+00 - val_loss: 0.4151 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8217 - accuracy: 0.0000e+00 - val_loss: 0.4162 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8215 - accuracy: 0.0000e+00 - val_loss: 0.4160 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8213 - accuracy: 0.0000e+00 - val_loss: 0.4162 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8212 - accuracy: 0.0000e+00 - val_loss: 0.4164 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8210 - accuracy: 0.0000e+00 - val_loss: 0.4169 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8208 - accuracy: 0.0000e+00 - val_loss: 0.4176 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8207 - accuracy: 0.0000e+00 - val_loss: 0.4178 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8205 - accuracy: 0.0000e+00 - val_loss: 0.4196 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8203 - accuracy: 0.0000e+00 - val_loss: 0.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8201 - accuracy: 0.0000e+00 - val_loss: 0.4206 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8200 - accuracy: 0.0000e+00 - val_loss: 0.4199 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8199 - accuracy: 0.0000e+00 - val_loss: 0.4199 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8197 - accuracy: 0.0000e+00 - val_loss: 0.4209 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8195 - accuracy: 0.0000e+00 - val_loss: 0.4207 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8193 - accuracy: 0.0000e+00 - val_loss: 0.4211 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8192 - accuracy: 0.0000e+00 - val_loss: 0.4217 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8190 - accuracy: 0.0000e+00 - val_loss: 0.4213 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 0.4222 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8187 - accuracy: 0.0000e+00 - val_loss: 0.4230 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8185 - accuracy: 0.0000e+00 - val_loss: 0.4231 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8184 - accuracy: 0.0000e+00 - val_loss: 0.4244 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8182 - accuracy: 0.0000e+00 - val_loss: 0.4240 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8181 - accuracy: 0.0000e+00 - val_loss: 0.4232 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/PG_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4232 - accuracy: 0.0000e+00\n",
      "[0.42318281531333923, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.2986 - accuracy: 0.0000e+00 - val_loss: 0.2864 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2671 - accuracy: 0.0000e+00 - val_loss: 0.2834 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2442 - accuracy: 0.0000e+00 - val_loss: 0.2809 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2126 - accuracy: 0.0000e+00 - val_loss: 0.2789 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1923 - accuracy: 0.0000e+00 - val_loss: 0.2774 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1676 - accuracy: 0.0000e+00 - val_loss: 0.2765 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1471 - accuracy: 0.0000e+00 - val_loss: 0.2762 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1295 - accuracy: 0.0000e+00 - val_loss: 0.2767 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1144 - accuracy: 0.0000e+00 - val_loss: 0.2777 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0961 - accuracy: 0.0000e+00 - val_loss: 0.2790 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0833 - accuracy: 0.0000e+00 - val_loss: 0.2812 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0708 - accuracy: 0.0000e+00 - val_loss: 0.2833 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0591 - accuracy: 0.0000e+00 - val_loss: 0.2858 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0502 - accuracy: 0.0000e+00 - val_loss: 0.2883 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0403 - accuracy: 0.0000e+00 - val_loss: 0.2912 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0315 - accuracy: 0.0000e+00 - val_loss: 0.2944 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0244 - accuracy: 0.0000e+00 - val_loss: 0.2975 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0179 - accuracy: 0.0000e+00 - val_loss: 0.3006 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0117 - accuracy: 0.0000e+00 - val_loss: 0.3040 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0054 - accuracy: 0.0000e+00 - val_loss: 0.3068 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9993 - accuracy: 0.0000e+00 - val_loss: 0.3096 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9940 - accuracy: 0.0000e+00 - val_loss: 0.3121 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9886 - accuracy: 0.0000e+00 - val_loss: 0.3157 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9842 - accuracy: 0.0000e+00 - val_loss: 0.3183 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9796 - accuracy: 0.0000e+00 - val_loss: 0.3217 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9763 - accuracy: 0.0000e+00 - val_loss: 0.3251 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9725 - accuracy: 0.0000e+00 - val_loss: 0.3290 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9692 - accuracy: 0.0000e+00 - val_loss: 0.3323 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9659 - accuracy: 0.0000e+00 - val_loss: 0.3357 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9630 - accuracy: 0.0000e+00 - val_loss: 0.3388 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9603 - accuracy: 0.0000e+00 - val_loss: 0.3423 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9576 - accuracy: 0.0000e+00 - val_loss: 0.3455 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9552 - accuracy: 0.0000e+00 - val_loss: 0.3484 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9527 - accuracy: 0.0000e+00 - val_loss: 0.3510 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9507 - accuracy: 0.0000e+00 - val_loss: 0.3541 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9484 - accuracy: 0.0000e+00 - val_loss: 0.3571 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9463 - accuracy: 0.0000e+00 - val_loss: 0.3599 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9446 - accuracy: 0.0000e+00 - val_loss: 0.3623 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9426 - accuracy: 0.0000e+00 - val_loss: 0.3654 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9410 - accuracy: 0.0000e+00 - val_loss: 0.3680 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9393 - accuracy: 0.0000e+00 - val_loss: 0.3705 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9375 - accuracy: 0.0000e+00 - val_loss: 0.3728 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9362 - accuracy: 0.0000e+00 - val_loss: 0.3751 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9345 - accuracy: 0.0000e+00 - val_loss: 0.3771 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9330 - accuracy: 0.0000e+00 - val_loss: 0.3792 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9318 - accuracy: 0.0000e+00 - val_loss: 0.3816 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9305 - accuracy: 0.0000e+00 - val_loss: 0.3835 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.3861 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9279 - accuracy: 0.0000e+00 - val_loss: 0.3879 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9270 - accuracy: 0.0000e+00 - val_loss: 0.3898 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9259 - accuracy: 0.0000e+00 - val_loss: 0.3921 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9249 - accuracy: 0.0000e+00 - val_loss: 0.3940 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9236 - accuracy: 0.0000e+00 - val_loss: 0.3960 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9224 - accuracy: 0.0000e+00 - val_loss: 0.3975 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9215 - accuracy: 0.0000e+00 - val_loss: 0.3992 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9206 - accuracy: 0.0000e+00 - val_loss: 0.4011 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9196 - accuracy: 0.0000e+00 - val_loss: 0.4028 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9189 - accuracy: 0.0000e+00 - val_loss: 0.4048 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9180 - accuracy: 0.0000e+00 - val_loss: 0.4063 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9168 - accuracy: 0.0000e+00 - val_loss: 0.4077 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9161 - accuracy: 0.0000e+00 - val_loss: 0.4094 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9152 - accuracy: 0.0000e+00 - val_loss: 0.4111 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9142 - accuracy: 0.0000e+00 - val_loss: 0.4127 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9136 - accuracy: 0.0000e+00 - val_loss: 0.4142 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9126 - accuracy: 0.0000e+00 - val_loss: 0.4156 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9116 - accuracy: 0.0000e+00 - val_loss: 0.4171 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9110 - accuracy: 0.0000e+00 - val_loss: 0.4187 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9103 - accuracy: 0.0000e+00 - val_loss: 0.4201 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9096 - accuracy: 0.0000e+00 - val_loss: 0.4217 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9087 - accuracy: 0.0000e+00 - val_loss: 0.4230 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9082 - accuracy: 0.0000e+00 - val_loss: 0.4247 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9074 - accuracy: 0.0000e+00 - val_loss: 0.4261 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9066 - accuracy: 0.0000e+00 - val_loss: 0.4274 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9061 - accuracy: 0.0000e+00 - val_loss: 0.4288 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9056 - accuracy: 0.0000e+00 - val_loss: 0.4302 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9050 - accuracy: 0.0000e+00 - val_loss: 0.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9045 - accuracy: 0.0000e+00 - val_loss: 0.4328 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9037 - accuracy: 0.0000e+00 - val_loss: 0.4341 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9032 - accuracy: 0.0000e+00 - val_loss: 0.4354 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9027 - accuracy: 0.0000e+00 - val_loss: 0.4365 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9022 - accuracy: 0.0000e+00 - val_loss: 0.4379 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9017 - accuracy: 0.0000e+00 - val_loss: 0.4389 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9012 - accuracy: 0.0000e+00 - val_loss: 0.4399 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9007 - accuracy: 0.0000e+00 - val_loss: 0.4410 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9003 - accuracy: 0.0000e+00 - val_loss: 0.4421 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8996 - accuracy: 0.0000e+00 - val_loss: 0.4432 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8992 - accuracy: 0.0000e+00 - val_loss: 0.4439 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8987 - accuracy: 0.0000e+00 - val_loss: 0.4449 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8983 - accuracy: 0.0000e+00 - val_loss: 0.4461 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8979 - accuracy: 0.0000e+00 - val_loss: 0.4474 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8974 - accuracy: 0.0000e+00 - val_loss: 0.4482 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8970 - accuracy: 0.0000e+00 - val_loss: 0.4491 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8966 - accuracy: 0.0000e+00 - val_loss: 0.4501 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8962 - accuracy: 0.0000e+00 - val_loss: 0.4511 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8957 - accuracy: 0.0000e+00 - val_loss: 0.4520 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8953 - accuracy: 0.0000e+00 - val_loss: 0.4525 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8951 - accuracy: 0.0000e+00 - val_loss: 0.4533 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8946 - accuracy: 0.0000e+00 - val_loss: 0.4543 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8942 - accuracy: 0.0000e+00 - val_loss: 0.4551 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8937 - accuracy: 0.0000e+00 - val_loss: 0.4560 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8933 - accuracy: 0.0000e+00 - val_loss: 0.4568 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8929 - accuracy: 0.0000e+00 - val_loss: 0.4575 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8925 - accuracy: 0.0000e+00 - val_loss: 0.4581 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.4590 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8918 - accuracy: 0.0000e+00 - val_loss: 0.4599 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8916 - accuracy: 0.0000e+00 - val_loss: 0.4605 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.4611 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8908 - accuracy: 0.0000e+00 - val_loss: 0.4617 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8904 - accuracy: 0.0000e+00 - val_loss: 0.4626 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8899 - accuracy: 0.0000e+00 - val_loss: 0.4632 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8895 - accuracy: 0.0000e+00 - val_loss: 0.4639 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8892 - accuracy: 0.0000e+00 - val_loss: 0.4647 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8889 - accuracy: 0.0000e+00 - val_loss: 0.4654 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8885 - accuracy: 0.0000e+00 - val_loss: 0.4660 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.4664 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.4668 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8874 - accuracy: 0.0000e+00 - val_loss: 0.4673 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8871 - accuracy: 0.0000e+00 - val_loss: 0.4681 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 0.4687 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8864 - accuracy: 0.0000e+00 - val_loss: 0.4691 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.4696 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.4700 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8854 - accuracy: 0.0000e+00 - val_loss: 0.4704 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8851 - accuracy: 0.0000e+00 - val_loss: 0.4711 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8847 - accuracy: 0.0000e+00 - val_loss: 0.4714 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.4720 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8841 - accuracy: 0.0000e+00 - val_loss: 0.4725 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8838 - accuracy: 0.0000e+00 - val_loss: 0.4730 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8835 - accuracy: 0.0000e+00 - val_loss: 0.4733 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8832 - accuracy: 0.0000e+00 - val_loss: 0.4739 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8829 - accuracy: 0.0000e+00 - val_loss: 0.4740 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8826 - accuracy: 0.0000e+00 - val_loss: 0.4746 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8822 - accuracy: 0.0000e+00 - val_loss: 0.4749 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8820 - accuracy: 0.0000e+00 - val_loss: 0.4753 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.4756 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - val_loss: 0.4759 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8811 - accuracy: 0.0000e+00 - val_loss: 0.4764 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8808 - accuracy: 0.0000e+00 - val_loss: 0.4766 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8805 - accuracy: 0.0000e+00 - val_loss: 0.4770 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8802 - accuracy: 0.0000e+00 - val_loss: 0.4773 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.4776 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8797 - accuracy: 0.0000e+00 - val_loss: 0.4780 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8793 - accuracy: 0.0000e+00 - val_loss: 0.4783 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8791 - accuracy: 0.0000e+00 - val_loss: 0.4786 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8788 - accuracy: 0.0000e+00 - val_loss: 0.4788 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8785 - accuracy: 0.0000e+00 - val_loss: 0.4793 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8782 - accuracy: 0.0000e+00 - val_loss: 0.4796 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8780 - accuracy: 0.0000e+00 - val_loss: 0.4798 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.4802 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8774 - accuracy: 0.0000e+00 - val_loss: 0.4804 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8772 - accuracy: 0.0000e+00 - val_loss: 0.4806 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8769 - accuracy: 0.0000e+00 - val_loss: 0.4809 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8766 - accuracy: 0.0000e+00 - val_loss: 0.4812 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8765 - accuracy: 0.0000e+00 - val_loss: 0.4813 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8761 - accuracy: 0.0000e+00 - val_loss: 0.4816 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8759 - accuracy: 0.0000e+00 - val_loss: 0.4817 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8757 - accuracy: 0.0000e+00 - val_loss: 0.4821 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.4823 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8751 - accuracy: 0.0000e+00 - val_loss: 0.4825 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8749 - accuracy: 0.0000e+00 - val_loss: 0.4828 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8746 - accuracy: 0.0000e+00 - val_loss: 0.4830 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.4832 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.4834 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8740 - accuracy: 0.0000e+00 - val_loss: 0.4836 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.4837 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.4838 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.4841 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.4843 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8728 - accuracy: 0.0000e+00 - val_loss: 0.4846 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8726 - accuracy: 0.0000e+00 - val_loss: 0.4846 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8724 - accuracy: 0.0000e+00 - val_loss: 0.4848 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8721 - accuracy: 0.0000e+00 - val_loss: 0.4849 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8720 - accuracy: 0.0000e+00 - val_loss: 0.4851 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8718 - accuracy: 0.0000e+00 - val_loss: 0.4852 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.4852 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8711 - accuracy: 0.0000e+00 - val_loss: 0.4855 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.4856 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8707 - accuracy: 0.0000e+00 - val_loss: 0.4857 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.4857 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8704 - accuracy: 0.0000e+00 - val_loss: 0.4859 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8702 - accuracy: 0.0000e+00 - val_loss: 0.4859 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8699 - accuracy: 0.0000e+00 - val_loss: 0.4862 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.4863 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.4866 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8694 - accuracy: 0.0000e+00 - val_loss: 0.4867 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8692 - accuracy: 0.0000e+00 - val_loss: 0.4865 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8690 - accuracy: 0.0000e+00 - val_loss: 0.4864 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8688 - accuracy: 0.0000e+00 - val_loss: 0.4865 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8687 - accuracy: 0.0000e+00 - val_loss: 0.4867 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8685 - accuracy: 0.0000e+00 - val_loss: 0.4867 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8683 - accuracy: 0.0000e+00 - val_loss: 0.4868 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8681 - accuracy: 0.0000e+00 - val_loss: 0.4869 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8680 - accuracy: 0.0000e+00 - val_loss: 0.4870 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8678 - accuracy: 0.0000e+00 - val_loss: 0.4872 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8676 - accuracy: 0.0000e+00 - val_loss: 0.4871 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8675 - accuracy: 0.0000e+00 - val_loss: 0.4872 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8672 - accuracy: 0.0000e+00 - val_loss: 0.4872 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8672 - accuracy: 0.0000e+00 - val_loss: 0.4872 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8669 - accuracy: 0.0000e+00 - val_loss: 0.4874 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8667 - accuracy: 0.0000e+00 - val_loss: 0.4876 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8666 - accuracy: 0.0000e+00 - val_loss: 0.4875 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8664 - accuracy: 0.0000e+00 - val_loss: 0.4876 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8662 - accuracy: 0.0000e+00 - val_loss: 0.4877 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8661 - accuracy: 0.0000e+00 - val_loss: 0.4878 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8660 - accuracy: 0.0000e+00 - val_loss: 0.4877 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8658 - accuracy: 0.0000e+00 - val_loss: 0.4877 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8656 - accuracy: 0.0000e+00 - val_loss: 0.4879 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8655 - accuracy: 0.0000e+00 - val_loss: 0.4879 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8653 - accuracy: 0.0000e+00 - val_loss: 0.4879 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8652 - accuracy: 0.0000e+00 - val_loss: 0.4879 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8650 - accuracy: 0.0000e+00 - val_loss: 0.4879 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8649 - accuracy: 0.0000e+00 - val_loss: 0.4879 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8647 - accuracy: 0.0000e+00 - val_loss: 0.4880 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.4882 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8644 - accuracy: 0.0000e+00 - val_loss: 0.4882 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8643 - accuracy: 0.0000e+00 - val_loss: 0.4882 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8640 - accuracy: 0.0000e+00 - val_loss: 0.4885 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8638 - accuracy: 0.0000e+00 - val_loss: 0.4883 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8637 - accuracy: 0.0000e+00 - val_loss: 0.4883 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8635 - accuracy: 0.0000e+00 - val_loss: 0.4883 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8634 - accuracy: 0.0000e+00 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8632 - accuracy: 0.0000e+00 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8631 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8629 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8628 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8627 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8625 - accuracy: 0.0000e+00 - val_loss: 0.4889 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8624 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8623 - accuracy: 0.0000e+00 - val_loss: 0.4886 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8622 - accuracy: 0.0000e+00 - val_loss: 0.4886 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8620 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8619 - accuracy: 0.0000e+00 - val_loss: 0.4889 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8617 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8616 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8615 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8613 - accuracy: 0.0000e+00 - val_loss: 0.4888 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8612 - accuracy: 0.0000e+00 - val_loss: 0.4888 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8610 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8609 - accuracy: 0.0000e+00 - val_loss: 0.4889 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8608 - accuracy: 0.0000e+00 - val_loss: 0.4888 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8606 - accuracy: 0.0000e+00 - val_loss: 0.4888 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8605 - accuracy: 0.0000e+00 - val_loss: 0.4890 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8604 - accuracy: 0.0000e+00 - val_loss: 0.4888 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8603 - accuracy: 0.0000e+00 - val_loss: 0.4890 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8602 - accuracy: 0.0000e+00 - val_loss: 0.4891 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8600 - accuracy: 0.0000e+00 - val_loss: 0.4890 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8599 - accuracy: 0.0000e+00 - val_loss: 0.4889 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8598 - accuracy: 0.0000e+00 - val_loss: 0.4886 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8594 - accuracy: 0.0000e+00 - val_loss: 0.4886 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8593 - accuracy: 0.0000e+00 - val_loss: 0.4886 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8592 - accuracy: 0.0000e+00 - val_loss: 0.4888 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8591 - accuracy: 0.0000e+00 - val_loss: 0.4888 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8590 - accuracy: 0.0000e+00 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/LLY_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4887 - accuracy: 0.0000e+00\n",
      "[0.4886719882488251, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.4592 - accuracy: 0.0000e+00 - val_loss: 0.3127 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.4326 - accuracy: 0.0000e+00 - val_loss: 0.3090 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.4075 - accuracy: 0.0000e+00 - val_loss: 0.3051 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.3883 - accuracy: 0.0000e+00 - val_loss: 0.3006 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3628 - accuracy: 0.0000e+00 - val_loss: 0.2962 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3416 - accuracy: 0.0000e+00 - val_loss: 0.2916 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.3215 - accuracy: 0.0000e+00 - val_loss: 0.2870 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.3006 - accuracy: 0.0000e+00 - val_loss: 0.2819 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2842 - accuracy: 0.0000e+00 - val_loss: 0.2766 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2640 - accuracy: 0.0000e+00 - val_loss: 0.2719 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2489 - accuracy: 0.0000e+00 - val_loss: 0.2672 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2326 - accuracy: 0.0000e+00 - val_loss: 0.2639 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2165 - accuracy: 0.0000e+00 - val_loss: 0.2624 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2016 - accuracy: 0.0000e+00 - val_loss: 0.2615 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1865 - accuracy: 0.0000e+00 - val_loss: 0.2618 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1721 - accuracy: 0.0000e+00 - val_loss: 0.2620 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1556 - accuracy: 0.0000e+00 - val_loss: 0.2630 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1414 - accuracy: 0.0000e+00 - val_loss: 0.2646 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1269 - accuracy: 0.0000e+00 - val_loss: 0.2661 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1139 - accuracy: 0.0000e+00 - val_loss: 0.2682 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1020 - accuracy: 0.0000e+00 - val_loss: 0.2704 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0887 - accuracy: 0.0000e+00 - val_loss: 0.2726 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0782 - accuracy: 0.0000e+00 - val_loss: 0.2747 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0681 - accuracy: 0.0000e+00 - val_loss: 0.2772 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0599 - accuracy: 0.0000e+00 - val_loss: 0.2794 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0520 - accuracy: 0.0000e+00 - val_loss: 0.2817 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0435 - accuracy: 0.0000e+00 - val_loss: 0.2838 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0368 - accuracy: 0.0000e+00 - val_loss: 0.2858 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0290 - accuracy: 0.0000e+00 - val_loss: 0.2881 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0241 - accuracy: 0.0000e+00 - val_loss: 0.2902 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0191 - accuracy: 0.0000e+00 - val_loss: 0.2922 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0139 - accuracy: 0.0000e+00 - val_loss: 0.2943 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0099 - accuracy: 0.0000e+00 - val_loss: 0.2965 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0052 - accuracy: 0.0000e+00 - val_loss: 0.2987 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0011 - accuracy: 0.0000e+00 - val_loss: 0.3005 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9976 - accuracy: 0.0000e+00 - val_loss: 0.3021 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9944 - accuracy: 0.0000e+00 - val_loss: 0.3040 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9916 - accuracy: 0.0000e+00 - val_loss: 0.3055 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9893 - accuracy: 0.0000e+00 - val_loss: 0.3073 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9867 - accuracy: 0.0000e+00 - val_loss: 0.3092 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9843 - accuracy: 0.0000e+00 - val_loss: 0.3106 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9822 - accuracy: 0.0000e+00 - val_loss: 0.3120 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9802 - accuracy: 0.0000e+00 - val_loss: 0.3132 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9779 - accuracy: 0.0000e+00 - val_loss: 0.3146 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9764 - accuracy: 0.0000e+00 - val_loss: 0.3161 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9747 - accuracy: 0.0000e+00 - val_loss: 0.3175 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9730 - accuracy: 0.0000e+00 - val_loss: 0.3187 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9715 - accuracy: 0.0000e+00 - val_loss: 0.3199 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9699 - accuracy: 0.0000e+00 - val_loss: 0.3210 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9686 - accuracy: 0.0000e+00 - val_loss: 0.3219 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9672 - accuracy: 0.0000e+00 - val_loss: 0.3232 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9658 - accuracy: 0.0000e+00 - val_loss: 0.3240 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9646 - accuracy: 0.0000e+00 - val_loss: 0.3251 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9636 - accuracy: 0.0000e+00 - val_loss: 0.3262 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9625 - accuracy: 0.0000e+00 - val_loss: 0.3272 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9614 - accuracy: 0.0000e+00 - val_loss: 0.3282 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9601 - accuracy: 0.0000e+00 - val_loss: 0.3289 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9592 - accuracy: 0.0000e+00 - val_loss: 0.3296 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9581 - accuracy: 0.0000e+00 - val_loss: 0.3305 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9573 - accuracy: 0.0000e+00 - val_loss: 0.3313 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9562 - accuracy: 0.0000e+00 - val_loss: 0.3321 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9553 - accuracy: 0.0000e+00 - val_loss: 0.3328 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9544 - accuracy: 0.0000e+00 - val_loss: 0.3334 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9535 - accuracy: 0.0000e+00 - val_loss: 0.3344 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9525 - accuracy: 0.0000e+00 - val_loss: 0.3349 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9517 - accuracy: 0.0000e+00 - val_loss: 0.3358 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9508 - accuracy: 0.0000e+00 - val_loss: 0.3362 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9500 - accuracy: 0.0000e+00 - val_loss: 0.3370 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9490 - accuracy: 0.0000e+00 - val_loss: 0.3376 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9482 - accuracy: 0.0000e+00 - val_loss: 0.3383 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9475 - accuracy: 0.0000e+00 - val_loss: 0.3390 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9466 - accuracy: 0.0000e+00 - val_loss: 0.3397 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9459 - accuracy: 0.0000e+00 - val_loss: 0.3399 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.3404 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9443 - accuracy: 0.0000e+00 - val_loss: 0.3407 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9433 - accuracy: 0.0000e+00 - val_loss: 0.3412 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9425 - accuracy: 0.0000e+00 - val_loss: 0.3417 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9418 - accuracy: 0.0000e+00 - val_loss: 0.3422 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9410 - accuracy: 0.0000e+00 - val_loss: 0.3428 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9402 - accuracy: 0.0000e+00 - val_loss: 0.3433 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9396 - accuracy: 0.0000e+00 - val_loss: 0.3438 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9388 - accuracy: 0.0000e+00 - val_loss: 0.3440 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9381 - accuracy: 0.0000e+00 - val_loss: 0.3444 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9373 - accuracy: 0.0000e+00 - val_loss: 0.3451 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9367 - accuracy: 0.0000e+00 - val_loss: 0.3453 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9357 - accuracy: 0.0000e+00 - val_loss: 0.3454 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9352 - accuracy: 0.0000e+00 - val_loss: 0.3457 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9342 - accuracy: 0.0000e+00 - val_loss: 0.3458 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9337 - accuracy: 0.0000e+00 - val_loss: 0.3459 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9328 - accuracy: 0.0000e+00 - val_loss: 0.3463 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9321 - accuracy: 0.0000e+00 - val_loss: 0.3466 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9315 - accuracy: 0.0000e+00 - val_loss: 0.3466 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9306 - accuracy: 0.0000e+00 - val_loss: 0.3467 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9299 - accuracy: 0.0000e+00 - val_loss: 0.3466 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.3470 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9283 - accuracy: 0.0000e+00 - val_loss: 0.3473 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.3473 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9269 - accuracy: 0.0000e+00 - val_loss: 0.3471 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9262 - accuracy: 0.0000e+00 - val_loss: 0.3469 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9255 - accuracy: 0.0000e+00 - val_loss: 0.3472 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9248 - accuracy: 0.0000e+00 - val_loss: 0.3471 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9241 - accuracy: 0.0000e+00 - val_loss: 0.3471 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9233 - accuracy: 0.0000e+00 - val_loss: 0.3468 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9225 - accuracy: 0.0000e+00 - val_loss: 0.3471 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9219 - accuracy: 0.0000e+00 - val_loss: 0.3470 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9211 - accuracy: 0.0000e+00 - val_loss: 0.3469 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9208 - accuracy: 0.0000e+00 - val_loss: 0.3467 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9198 - accuracy: 0.0000e+00 - val_loss: 0.3465 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9191 - accuracy: 0.0000e+00 - val_loss: 0.3464 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9183 - accuracy: 0.0000e+00 - val_loss: 0.3470 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9178 - accuracy: 0.0000e+00 - val_loss: 0.3467 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9171 - accuracy: 0.0000e+00 - val_loss: 0.3472 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9165 - accuracy: 0.0000e+00 - val_loss: 0.3468 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9158 - accuracy: 0.0000e+00 - val_loss: 0.3468 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9151 - accuracy: 0.0000e+00 - val_loss: 0.3470 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9144 - accuracy: 0.0000e+00 - val_loss: 0.3471 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9138 - accuracy: 0.0000e+00 - val_loss: 0.3474 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9135 - accuracy: 0.0000e+00 - val_loss: 0.3470 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9127 - accuracy: 0.0000e+00 - val_loss: 0.3471 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9121 - accuracy: 0.0000e+00 - val_loss: 0.3469 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9117 - accuracy: 0.0000e+00 - val_loss: 0.3466 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9112 - accuracy: 0.0000e+00 - val_loss: 0.3465 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9104 - accuracy: 0.0000e+00 - val_loss: 0.3468 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9100 - accuracy: 0.0000e+00 - val_loss: 0.3473 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9095 - accuracy: 0.0000e+00 - val_loss: 0.3473 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9090 - accuracy: 0.0000e+00 - val_loss: 0.3471 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9085 - accuracy: 0.0000e+00 - val_loss: 0.3474 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9080 - accuracy: 0.0000e+00 - val_loss: 0.3480 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9075 - accuracy: 0.0000e+00 - val_loss: 0.3482 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9071 - accuracy: 0.0000e+00 - val_loss: 0.3483 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9067 - accuracy: 0.0000e+00 - val_loss: 0.3482 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9062 - accuracy: 0.0000e+00 - val_loss: 0.3483 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9059 - accuracy: 0.0000e+00 - val_loss: 0.3493 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9054 - accuracy: 0.0000e+00 - val_loss: 0.3493 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9052 - accuracy: 0.0000e+00 - val_loss: 0.3496 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9046 - accuracy: 0.0000e+00 - val_loss: 0.3502 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9043 - accuracy: 0.0000e+00 - val_loss: 0.3506 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9039 - accuracy: 0.0000e+00 - val_loss: 0.3509 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9035 - accuracy: 0.0000e+00 - val_loss: 0.3514 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9032 - accuracy: 0.0000e+00 - val_loss: 0.3517 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.3517 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9024 - accuracy: 0.0000e+00 - val_loss: 0.3522 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9022 - accuracy: 0.0000e+00 - val_loss: 0.3525 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.3533 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9015 - accuracy: 0.0000e+00 - val_loss: 0.3537 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9011 - accuracy: 0.0000e+00 - val_loss: 0.3546 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9009 - accuracy: 0.0000e+00 - val_loss: 0.3549 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.3555 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.3561 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9000 - accuracy: 0.0000e+00 - val_loss: 0.3571 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8997 - accuracy: 0.0000e+00 - val_loss: 0.3576 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8994 - accuracy: 0.0000e+00 - val_loss: 0.3581 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8992 - accuracy: 0.0000e+00 - val_loss: 0.3587 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8988 - accuracy: 0.0000e+00 - val_loss: 0.3595 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8986 - accuracy: 0.0000e+00 - val_loss: 0.3602 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8983 - accuracy: 0.0000e+00 - val_loss: 0.3607 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.3614 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8979 - accuracy: 0.0000e+00 - val_loss: 0.3616 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8976 - accuracy: 0.0000e+00 - val_loss: 0.3625 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8973 - accuracy: 0.0000e+00 - val_loss: 0.3630 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8971 - accuracy: 0.0000e+00 - val_loss: 0.3634 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8969 - accuracy: 0.0000e+00 - val_loss: 0.3641 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8966 - accuracy: 0.0000e+00 - val_loss: 0.3647 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8964 - accuracy: 0.0000e+00 - val_loss: 0.3656 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8961 - accuracy: 0.0000e+00 - val_loss: 0.3660 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8959 - accuracy: 0.0000e+00 - val_loss: 0.3665 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8957 - accuracy: 0.0000e+00 - val_loss: 0.3676 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8955 - accuracy: 0.0000e+00 - val_loss: 0.3682 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8952 - accuracy: 0.0000e+00 - val_loss: 0.3688 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8951 - accuracy: 0.0000e+00 - val_loss: 0.3694 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8948 - accuracy: 0.0000e+00 - val_loss: 0.3699 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8947 - accuracy: 0.0000e+00 - val_loss: 0.3709 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8944 - accuracy: 0.0000e+00 - val_loss: 0.3713 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8942 - accuracy: 0.0000e+00 - val_loss: 0.3718 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8941 - accuracy: 0.0000e+00 - val_loss: 0.3721 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8939 - accuracy: 0.0000e+00 - val_loss: 0.3728 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8936 - accuracy: 0.0000e+00 - val_loss: 0.3736 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8935 - accuracy: 0.0000e+00 - val_loss: 0.3742 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8933 - accuracy: 0.0000e+00 - val_loss: 0.3748 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8931 - accuracy: 0.0000e+00 - val_loss: 0.3754 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8929 - accuracy: 0.0000e+00 - val_loss: 0.3759 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8927 - accuracy: 0.0000e+00 - val_loss: 0.3769 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8926 - accuracy: 0.0000e+00 - val_loss: 0.3774 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8924 - accuracy: 0.0000e+00 - val_loss: 0.3777 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.3780 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8920 - accuracy: 0.0000e+00 - val_loss: 0.3787 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8919 - accuracy: 0.0000e+00 - val_loss: 0.3793 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8917 - accuracy: 0.0000e+00 - val_loss: 0.3801 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8915 - accuracy: 0.0000e+00 - val_loss: 0.3806 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8914 - accuracy: 0.0000e+00 - val_loss: 0.3810 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.3818 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.3821 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8910 - accuracy: 0.0000e+00 - val_loss: 0.3829 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8908 - accuracy: 0.0000e+00 - val_loss: 0.3833 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.3836 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8905 - accuracy: 0.0000e+00 - val_loss: 0.3840 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8904 - accuracy: 0.0000e+00 - val_loss: 0.3845 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8902 - accuracy: 0.0000e+00 - val_loss: 0.3852 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8901 - accuracy: 0.0000e+00 - val_loss: 0.3857 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8900 - accuracy: 0.0000e+00 - val_loss: 0.3857 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8898 - accuracy: 0.0000e+00 - val_loss: 0.3862 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8897 - accuracy: 0.0000e+00 - val_loss: 0.3867 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8896 - accuracy: 0.0000e+00 - val_loss: 0.3872 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8894 - accuracy: 0.0000e+00 - val_loss: 0.3876 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8893 - accuracy: 0.0000e+00 - val_loss: 0.3880 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8892 - accuracy: 0.0000e+00 - val_loss: 0.3885 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8891 - accuracy: 0.0000e+00 - val_loss: 0.3889 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8889 - accuracy: 0.0000e+00 - val_loss: 0.3895 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8888 - accuracy: 0.0000e+00 - val_loss: 0.3897 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.3901 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8886 - accuracy: 0.0000e+00 - val_loss: 0.3904 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8885 - accuracy: 0.0000e+00 - val_loss: 0.3907 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8884 - accuracy: 0.0000e+00 - val_loss: 0.3911 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8883 - accuracy: 0.0000e+00 - val_loss: 0.3921 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8881 - accuracy: 0.0000e+00 - val_loss: 0.3922 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8880 - accuracy: 0.0000e+00 - val_loss: 0.3927 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8879 - accuracy: 0.0000e+00 - val_loss: 0.3927 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.3930 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8877 - accuracy: 0.0000e+00 - val_loss: 0.3932 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8876 - accuracy: 0.0000e+00 - val_loss: 0.3938 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8875 - accuracy: 0.0000e+00 - val_loss: 0.3941 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8874 - accuracy: 0.0000e+00 - val_loss: 0.3944 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8873 - accuracy: 0.0000e+00 - val_loss: 0.3948 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8872 - accuracy: 0.0000e+00 - val_loss: 0.3950 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8871 - accuracy: 0.0000e+00 - val_loss: 0.3951 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8870 - accuracy: 0.0000e+00 - val_loss: 0.3954 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8869 - accuracy: 0.0000e+00 - val_loss: 0.3958 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8868 - accuracy: 0.0000e+00 - val_loss: 0.3962 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8868 - accuracy: 0.0000e+00 - val_loss: 0.3964 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 0.3970 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8866 - accuracy: 0.0000e+00 - val_loss: 0.3968 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8865 - accuracy: 0.0000e+00 - val_loss: 0.3971 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8864 - accuracy: 0.0000e+00 - val_loss: 0.3975 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8863 - accuracy: 0.0000e+00 - val_loss: 0.3979 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8862 - accuracy: 0.0000e+00 - val_loss: 0.3981 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8862 - accuracy: 0.0000e+00 - val_loss: 0.3980 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.3983 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8860 - accuracy: 0.0000e+00 - val_loss: 0.3987 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.3991 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.3995 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8858 - accuracy: 0.0000e+00 - val_loss: 0.3994 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.3995 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8856 - accuracy: 0.0000e+00 - val_loss: 0.3998 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8855 - accuracy: 0.0000e+00 - val_loss: 0.4001 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8854 - accuracy: 0.0000e+00 - val_loss: 0.4004 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8854 - accuracy: 0.0000e+00 - val_loss: 0.4005 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8853 - accuracy: 0.0000e+00 - val_loss: 0.4005 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.4008 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.4008 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8851 - accuracy: 0.0000e+00 - val_loss: 0.4011 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.4012 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.4011 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.4014 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8848 - accuracy: 0.0000e+00 - val_loss: 0.4017 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8847 - accuracy: 0.0000e+00 - val_loss: 0.4017 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8847 - accuracy: 0.0000e+00 - val_loss: 0.4019 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/PFE_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4019 - accuracy: 0.0000e+00\n",
      "[0.4019092917442322, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.3448 - accuracy: 0.0000e+00 - val_loss: 0.4032 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.3114 - accuracy: 0.0000e+00 - val_loss: 0.4043 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2879 - accuracy: 0.0000e+00 - val_loss: 0.4060 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2571 - accuracy: 0.0000e+00 - val_loss: 0.4067 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2334 - accuracy: 0.0000e+00 - val_loss: 0.4076 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2115 - accuracy: 0.0000e+00 - val_loss: 0.4081 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1870 - accuracy: 0.0000e+00 - val_loss: 0.4068 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1706 - accuracy: 0.0000e+00 - val_loss: 0.4079 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1498 - accuracy: 0.0000e+00 - val_loss: 0.4140 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1372 - accuracy: 0.0000e+00 - val_loss: 0.4263 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1184 - accuracy: 0.0000e+00 - val_loss: 0.4336 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1092 - accuracy: 0.0000e+00 - val_loss: 0.4414 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0955 - accuracy: 0.0000e+00 - val_loss: 0.4473 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0832 - accuracy: 0.0000e+00 - val_loss: 0.4514 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0745 - accuracy: 0.0000e+00 - val_loss: 0.4549 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0650 - accuracy: 0.0000e+00 - val_loss: 0.4579 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0577 - accuracy: 0.0000e+00 - val_loss: 0.4607 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0509 - accuracy: 0.0000e+00 - val_loss: 0.4635 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0434 - accuracy: 0.0000e+00 - val_loss: 0.4659 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0369 - accuracy: 0.0000e+00 - val_loss: 0.4685 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0320 - accuracy: 0.0000e+00 - val_loss: 0.4710 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0265 - accuracy: 0.0000e+00 - val_loss: 0.4722 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0216 - accuracy: 0.0000e+00 - val_loss: 0.4732 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0179 - accuracy: 0.0000e+00 - val_loss: 0.4757 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0132 - accuracy: 0.0000e+00 - val_loss: 0.4771 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0101 - accuracy: 0.0000e+00 - val_loss: 0.4796 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0066 - accuracy: 0.0000e+00 - val_loss: 0.4917 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0035 - accuracy: 0.0000e+00 - val_loss: 0.5314 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0008 - accuracy: 0.0000e+00 - val_loss: 0.5338 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9976 - accuracy: 0.0000e+00 - val_loss: 0.5342 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9956 - accuracy: 0.0000e+00 - val_loss: 0.5354 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9934 - accuracy: 0.0000e+00 - val_loss: 0.5377 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9910 - accuracy: 0.0000e+00 - val_loss: 0.5381 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9891 - accuracy: 0.0000e+00 - val_loss: 0.5395 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9868 - accuracy: 0.0000e+00 - val_loss: 0.5401 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9853 - accuracy: 0.0000e+00 - val_loss: 0.5413 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9834 - accuracy: 0.0000e+00 - val_loss: 0.5420 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9819 - accuracy: 0.0000e+00 - val_loss: 0.5425 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9804 - accuracy: 0.0000e+00 - val_loss: 0.5429 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9793 - accuracy: 0.0000e+00 - val_loss: 0.5440 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9773 - accuracy: 0.0000e+00 - val_loss: 0.5445 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9759 - accuracy: 0.0000e+00 - val_loss: 0.5441 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9750 - accuracy: 0.0000e+00 - val_loss: 0.5445 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9735 - accuracy: 0.0000e+00 - val_loss: 0.5442 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9725 - accuracy: 0.0000e+00 - val_loss: 0.5452 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9712 - accuracy: 0.0000e+00 - val_loss: 0.5457 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9702 - accuracy: 0.0000e+00 - val_loss: 0.5458 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9691 - accuracy: 0.0000e+00 - val_loss: 0.5458 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9679 - accuracy: 0.0000e+00 - val_loss: 0.5462 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9670 - accuracy: 0.0000e+00 - val_loss: 0.5458 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9659 - accuracy: 0.0000e+00 - val_loss: 0.5459 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9650 - accuracy: 0.0000e+00 - val_loss: 0.5463 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9640 - accuracy: 0.0000e+00 - val_loss: 0.5468 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9629 - accuracy: 0.0000e+00 - val_loss: 0.5466 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9621 - accuracy: 0.0000e+00 - val_loss: 0.5461 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9611 - accuracy: 0.0000e+00 - val_loss: 0.5465 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9603 - accuracy: 0.0000e+00 - val_loss: 0.5466 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9594 - accuracy: 0.0000e+00 - val_loss: 0.5468 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9585 - accuracy: 0.0000e+00 - val_loss: 0.5471 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9575 - accuracy: 0.0000e+00 - val_loss: 0.5474 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9567 - accuracy: 0.0000e+00 - val_loss: 0.5468 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9558 - accuracy: 0.0000e+00 - val_loss: 0.5468 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9551 - accuracy: 0.0000e+00 - val_loss: 0.5466 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9543 - accuracy: 0.0000e+00 - val_loss: 0.5465 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9532 - accuracy: 0.0000e+00 - val_loss: 0.5470 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9524 - accuracy: 0.0000e+00 - val_loss: 0.5474 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9519 - accuracy: 0.0000e+00 - val_loss: 0.5475 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9510 - accuracy: 0.0000e+00 - val_loss: 0.5471 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9499 - accuracy: 0.0000e+00 - val_loss: 0.5473 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9493 - accuracy: 0.0000e+00 - val_loss: 0.5473 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9482 - accuracy: 0.0000e+00 - val_loss: 0.5474 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9476 - accuracy: 0.0000e+00 - val_loss: 0.5469 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9466 - accuracy: 0.0000e+00 - val_loss: 0.5467 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9457 - accuracy: 0.0000e+00 - val_loss: 0.5474 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9449 - accuracy: 0.0000e+00 - val_loss: 0.5466 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9441 - accuracy: 0.0000e+00 - val_loss: 0.5469 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9433 - accuracy: 0.0000e+00 - val_loss: 0.5466 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9425 - accuracy: 0.0000e+00 - val_loss: 0.5463 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9417 - accuracy: 0.0000e+00 - val_loss: 0.5460 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9407 - accuracy: 0.0000e+00 - val_loss: 0.5462 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9401 - accuracy: 0.0000e+00 - val_loss: 0.5462 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9391 - accuracy: 0.0000e+00 - val_loss: 0.5458 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9385 - accuracy: 0.0000e+00 - val_loss: 0.5449 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9376 - accuracy: 0.0000e+00 - val_loss: 0.5451 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9367 - accuracy: 0.0000e+00 - val_loss: 0.5449 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9360 - accuracy: 0.0000e+00 - val_loss: 0.5452 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9350 - accuracy: 0.0000e+00 - val_loss: 0.5449 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9343 - accuracy: 0.0000e+00 - val_loss: 0.5441 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9335 - accuracy: 0.0000e+00 - val_loss: 0.5444 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.5443 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9319 - accuracy: 0.0000e+00 - val_loss: 0.5447 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9313 - accuracy: 0.0000e+00 - val_loss: 0.5446 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9301 - accuracy: 0.0000e+00 - val_loss: 0.5443 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9294 - accuracy: 0.0000e+00 - val_loss: 0.5445 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9287 - accuracy: 0.0000e+00 - val_loss: 0.5434 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9278 - accuracy: 0.0000e+00 - val_loss: 0.5433 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9273 - accuracy: 0.0000e+00 - val_loss: 0.5440 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9262 - accuracy: 0.0000e+00 - val_loss: 0.5433 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9253 - accuracy: 0.0000e+00 - val_loss: 0.5439 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9246 - accuracy: 0.0000e+00 - val_loss: 0.5442 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9235 - accuracy: 0.0000e+00 - val_loss: 0.5441 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9228 - accuracy: 0.0000e+00 - val_loss: 0.5438 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9218 - accuracy: 0.0000e+00 - val_loss: 0.5441 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9210 - accuracy: 0.0000e+00 - val_loss: 0.5445 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9201 - accuracy: 0.0000e+00 - val_loss: 0.5444 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9192 - accuracy: 0.0000e+00 - val_loss: 0.5440 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9183 - accuracy: 0.0000e+00 - val_loss: 0.5439 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9174 - accuracy: 0.0000e+00 - val_loss: 0.5445 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9163 - accuracy: 0.0000e+00 - val_loss: 0.5451 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9154 - accuracy: 0.0000e+00 - val_loss: 0.5450 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9144 - accuracy: 0.0000e+00 - val_loss: 0.5463 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9135 - accuracy: 0.0000e+00 - val_loss: 0.5461 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9125 - accuracy: 0.0000e+00 - val_loss: 0.5465 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9114 - accuracy: 0.0000e+00 - val_loss: 0.5473 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9104 - accuracy: 0.0000e+00 - val_loss: 0.5475 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9093 - accuracy: 0.0000e+00 - val_loss: 0.5477 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9085 - accuracy: 0.0000e+00 - val_loss: 0.5479 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9079 - accuracy: 0.0000e+00 - val_loss: 0.5486 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9069 - accuracy: 0.0000e+00 - val_loss: 0.5489 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9061 - accuracy: 0.0000e+00 - val_loss: 0.5495 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9051 - accuracy: 0.0000e+00 - val_loss: 0.5494 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9042 - accuracy: 0.0000e+00 - val_loss: 0.5500 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9035 - accuracy: 0.0000e+00 - val_loss: 0.5503 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9028 - accuracy: 0.0000e+00 - val_loss: 0.5512 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.5506 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9010 - accuracy: 0.0000e+00 - val_loss: 0.5515 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.5525 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8996 - accuracy: 0.0000e+00 - val_loss: 0.5526 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8986 - accuracy: 0.0000e+00 - val_loss: 0.5529 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.5538 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8973 - accuracy: 0.0000e+00 - val_loss: 0.5535 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8965 - accuracy: 0.0000e+00 - val_loss: 0.5539 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8959 - accuracy: 0.0000e+00 - val_loss: 0.5547 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8953 - accuracy: 0.0000e+00 - val_loss: 0.5548 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8942 - accuracy: 0.0000e+00 - val_loss: 0.5549 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8937 - accuracy: 0.0000e+00 - val_loss: 0.5549 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8932 - accuracy: 0.0000e+00 - val_loss: 0.5545 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8923 - accuracy: 0.0000e+00 - val_loss: 0.5557 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8918 - accuracy: 0.0000e+00 - val_loss: 0.5570 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.5565 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8904 - accuracy: 0.0000e+00 - val_loss: 0.5562 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8898 - accuracy: 0.0000e+00 - val_loss: 0.5572 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8892 - accuracy: 0.0000e+00 - val_loss: 0.5571 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.5576 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8880 - accuracy: 0.0000e+00 - val_loss: 0.5565 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8871 - accuracy: 0.0000e+00 - val_loss: 0.5568 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 0.5566 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.5573 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8854 - accuracy: 0.0000e+00 - val_loss: 0.5578 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8848 - accuracy: 0.0000e+00 - val_loss: 0.5574 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.5574 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8835 - accuracy: 0.0000e+00 - val_loss: 0.5575 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8829 - accuracy: 0.0000e+00 - val_loss: 0.5579 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8823 - accuracy: 0.0000e+00 - val_loss: 0.5585 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8818 - accuracy: 0.0000e+00 - val_loss: 0.5583 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8813 - accuracy: 0.0000e+00 - val_loss: 0.5571 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8806 - accuracy: 0.0000e+00 - val_loss: 0.5576 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8801 - accuracy: 0.0000e+00 - val_loss: 0.5583 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8795 - accuracy: 0.0000e+00 - val_loss: 0.5591 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.5594 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8785 - accuracy: 0.0000e+00 - val_loss: 0.5593 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8778 - accuracy: 0.0000e+00 - val_loss: 0.5594 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8773 - accuracy: 0.0000e+00 - val_loss: 0.5598 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.5594 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8761 - accuracy: 0.0000e+00 - val_loss: 0.5599 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8756 - accuracy: 0.0000e+00 - val_loss: 0.5602 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.5606 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8744 - accuracy: 0.0000e+00 - val_loss: 0.5602 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.5614 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8727 - accuracy: 0.0000e+00 - val_loss: 0.5603 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8720 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.5614 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.5611 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8702 - accuracy: 0.0000e+00 - val_loss: 0.5612 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.5617 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8691 - accuracy: 0.0000e+00 - val_loss: 0.5619 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8685 - accuracy: 0.0000e+00 - val_loss: 0.5620 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8678 - accuracy: 0.0000e+00 - val_loss: 0.5628 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8674 - accuracy: 0.0000e+00 - val_loss: 0.5629 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8668 - accuracy: 0.0000e+00 - val_loss: 0.5628 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8662 - accuracy: 0.0000e+00 - val_loss: 0.5635 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8658 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8649 - accuracy: 0.0000e+00 - val_loss: 0.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.5641 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.5644 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8638 - accuracy: 0.0000e+00 - val_loss: 0.5651 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8633 - accuracy: 0.0000e+00 - val_loss: 0.5648 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8629 - accuracy: 0.0000e+00 - val_loss: 0.5648 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8624 - accuracy: 0.0000e+00 - val_loss: 0.5649 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8622 - accuracy: 0.0000e+00 - val_loss: 0.5651 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.5650 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8614 - accuracy: 0.0000e+00 - val_loss: 0.5655 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8611 - accuracy: 0.0000e+00 - val_loss: 0.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8606 - accuracy: 0.0000e+00 - val_loss: 0.5661 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8603 - accuracy: 0.0000e+00 - val_loss: 0.5660 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8600 - accuracy: 0.0000e+00 - val_loss: 0.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.5658 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8592 - accuracy: 0.0000e+00 - val_loss: 0.5658 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8589 - accuracy: 0.0000e+00 - val_loss: 0.5654 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8585 - accuracy: 0.0000e+00 - val_loss: 0.5656 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8582 - accuracy: 0.0000e+00 - val_loss: 0.5659 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8578 - accuracy: 0.0000e+00 - val_loss: 0.5659 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8576 - accuracy: 0.0000e+00 - val_loss: 0.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8572 - accuracy: 0.0000e+00 - val_loss: 0.5658 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8569 - accuracy: 0.0000e+00 - val_loss: 0.5659 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8566 - accuracy: 0.0000e+00 - val_loss: 0.5662 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8562 - accuracy: 0.0000e+00 - val_loss: 0.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8560 - accuracy: 0.0000e+00 - val_loss: 0.5654 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8557 - accuracy: 0.0000e+00 - val_loss: 0.5647 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.5648 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8550 - accuracy: 0.0000e+00 - val_loss: 0.5648 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8547 - accuracy: 0.0000e+00 - val_loss: 0.5651 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8544 - accuracy: 0.0000e+00 - val_loss: 0.5653 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8542 - accuracy: 0.0000e+00 - val_loss: 0.5645 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8538 - accuracy: 0.0000e+00 - val_loss: 0.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.5642 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8533 - accuracy: 0.0000e+00 - val_loss: 0.5645 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8529 - accuracy: 0.0000e+00 - val_loss: 0.5647 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.5647 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8522 - accuracy: 0.0000e+00 - val_loss: 0.5645 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8519 - accuracy: 0.0000e+00 - val_loss: 0.5645 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8516 - accuracy: 0.0000e+00 - val_loss: 0.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8514 - accuracy: 0.0000e+00 - val_loss: 0.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8508 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8504 - accuracy: 0.0000e+00 - val_loss: 0.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8502 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8496 - accuracy: 0.0000e+00 - val_loss: 0.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8485 - accuracy: 0.0000e+00 - val_loss: 0.5631 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8480 - accuracy: 0.0000e+00 - val_loss: 0.5623 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8468 - accuracy: 0.0000e+00 - val_loss: 0.5623 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8457 - accuracy: 0.0000e+00 - val_loss: 0.5617 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8450 - accuracy: 0.0000e+00 - val_loss: 0.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8437 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8415 - accuracy: 0.0000e+00 - val_loss: 0.5602 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8400 - accuracy: 0.0000e+00 - val_loss: 0.5599 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8383 - accuracy: 0.0000e+00 - val_loss: 0.5594 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8371 - accuracy: 0.0000e+00 - val_loss: 0.5593 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8366 - accuracy: 0.0000e+00 - val_loss: 0.5598 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8357 - accuracy: 0.0000e+00 - val_loss: 0.5613 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8355 - accuracy: 0.0000e+00 - val_loss: 0.5618 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.5632 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8342 - accuracy: 0.0000e+00 - val_loss: 0.5655 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.5663 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8330 - accuracy: 0.0000e+00 - val_loss: 0.5686 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8321 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8320 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8302 - accuracy: 0.0000e+00 - val_loss: 0.5733 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8298 - accuracy: 0.0000e+00 - val_loss: 0.5741 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8293 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8289 - accuracy: 0.0000e+00 - val_loss: 0.5759 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8282 - accuracy: 0.0000e+00 - val_loss: 0.5766 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/MRK_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5766 - accuracy: 0.0000e+00\n",
      "[0.5766395330429077, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 18ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/BAC_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 22ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.3466 - accuracy: 0.0000e+00 - val_loss: 0.3992 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.3175 - accuracy: 0.0000e+00 - val_loss: 0.3998 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2924 - accuracy: 0.0000e+00 - val_loss: 0.4008 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2676 - accuracy: 0.0000e+00 - val_loss: 0.4020 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2464 - accuracy: 0.0000e+00 - val_loss: 0.4075 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2235 - accuracy: 0.0000e+00 - val_loss: 0.4138 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2021 - accuracy: 0.0000e+00 - val_loss: 0.4220 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1855 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1670 - accuracy: 0.0000e+00 - val_loss: 0.4395 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1502 - accuracy: 0.0000e+00 - val_loss: 0.4466 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1337 - accuracy: 0.0000e+00 - val_loss: 0.4517 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1197 - accuracy: 0.0000e+00 - val_loss: 0.4571 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1084 - accuracy: 0.0000e+00 - val_loss: 0.4625 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0973 - accuracy: 0.0000e+00 - val_loss: 0.4669 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0851 - accuracy: 0.0000e+00 - val_loss: 0.4713 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0767 - accuracy: 0.0000e+00 - val_loss: 0.4753 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0665 - accuracy: 0.0000e+00 - val_loss: 0.4798 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0595 - accuracy: 0.0000e+00 - val_loss: 0.4848 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0504 - accuracy: 0.0000e+00 - val_loss: 0.4882 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0443 - accuracy: 0.0000e+00 - val_loss: 0.4931 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0386 - accuracy: 0.0000e+00 - val_loss: 0.4981 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0320 - accuracy: 0.0000e+00 - val_loss: 0.5027 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0267 - accuracy: 0.0000e+00 - val_loss: 0.5066 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0220 - accuracy: 0.0000e+00 - val_loss: 0.5116 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0172 - accuracy: 0.0000e+00 - val_loss: 0.5165 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0119 - accuracy: 0.0000e+00 - val_loss: 0.5204 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0078 - accuracy: 0.0000e+00 - val_loss: 0.5245 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0037 - accuracy: 0.0000e+00 - val_loss: 0.5287 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0002 - accuracy: 0.0000e+00 - val_loss: 0.5323 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9965 - accuracy: 0.0000e+00 - val_loss: 0.5368 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9925 - accuracy: 0.0000e+00 - val_loss: 0.5404 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9894 - accuracy: 0.0000e+00 - val_loss: 0.5439 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9858 - accuracy: 0.0000e+00 - val_loss: 0.5476 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9828 - accuracy: 0.0000e+00 - val_loss: 0.5506 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9801 - accuracy: 0.0000e+00 - val_loss: 0.5546 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9772 - accuracy: 0.0000e+00 - val_loss: 0.5578 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9743 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9718 - accuracy: 0.0000e+00 - val_loss: 0.5636 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9694 - accuracy: 0.0000e+00 - val_loss: 0.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9669 - accuracy: 0.0000e+00 - val_loss: 0.5687 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9643 - accuracy: 0.0000e+00 - val_loss: 0.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9620 - accuracy: 0.0000e+00 - val_loss: 0.5736 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9597 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9580 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9557 - accuracy: 0.0000e+00 - val_loss: 0.5805 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9536 - accuracy: 0.0000e+00 - val_loss: 0.5826 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9517 - accuracy: 0.0000e+00 - val_loss: 0.5841 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9497 - accuracy: 0.0000e+00 - val_loss: 0.5848 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9482 - accuracy: 0.0000e+00 - val_loss: 0.5870 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9465 - accuracy: 0.0000e+00 - val_loss: 0.5888 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9445 - accuracy: 0.0000e+00 - val_loss: 0.5894 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9431 - accuracy: 0.0000e+00 - val_loss: 0.5911 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9409 - accuracy: 0.0000e+00 - val_loss: 0.5919 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9396 - accuracy: 0.0000e+00 - val_loss: 0.5933 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9379 - accuracy: 0.0000e+00 - val_loss: 0.5940 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9365 - accuracy: 0.0000e+00 - val_loss: 0.5945 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9349 - accuracy: 0.0000e+00 - val_loss: 0.5943 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9334 - accuracy: 0.0000e+00 - val_loss: 0.5953 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9320 - accuracy: 0.0000e+00 - val_loss: 0.5958 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9305 - accuracy: 0.0000e+00 - val_loss: 0.5956 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.5958 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9265 - accuracy: 0.0000e+00 - val_loss: 0.5950 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9245 - accuracy: 0.0000e+00 - val_loss: 0.5952 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9233 - accuracy: 0.0000e+00 - val_loss: 0.5965 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9217 - accuracy: 0.0000e+00 - val_loss: 0.5958 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9204 - accuracy: 0.0000e+00 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9189 - accuracy: 0.0000e+00 - val_loss: 0.5958 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9174 - accuracy: 0.0000e+00 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9165 - accuracy: 0.0000e+00 - val_loss: 0.5947 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9148 - accuracy: 0.0000e+00 - val_loss: 0.5953 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9137 - accuracy: 0.0000e+00 - val_loss: 0.5937 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9120 - accuracy: 0.0000e+00 - val_loss: 0.5931 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9107 - accuracy: 0.0000e+00 - val_loss: 0.5937 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9090 - accuracy: 0.0000e+00 - val_loss: 0.5934 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9085 - accuracy: 0.0000e+00 - val_loss: 0.5920 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9064 - accuracy: 0.0000e+00 - val_loss: 0.5912 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9054 - accuracy: 0.0000e+00 - val_loss: 0.5914 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9038 - accuracy: 0.0000e+00 - val_loss: 0.5904 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.5904 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9016 - accuracy: 0.0000e+00 - val_loss: 0.5891 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.5889 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8996 - accuracy: 0.0000e+00 - val_loss: 0.5877 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.5874 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8970 - accuracy: 0.0000e+00 - val_loss: 0.5873 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8959 - accuracy: 0.0000e+00 - val_loss: 0.5866 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8950 - accuracy: 0.0000e+00 - val_loss: 0.5862 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8940 - accuracy: 0.0000e+00 - val_loss: 0.5868 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8931 - accuracy: 0.0000e+00 - val_loss: 0.5862 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.5862 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8912 - accuracy: 0.0000e+00 - val_loss: 0.5856 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8903 - accuracy: 0.0000e+00 - val_loss: 0.5851 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8896 - accuracy: 0.0000e+00 - val_loss: 0.5842 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8885 - accuracy: 0.0000e+00 - val_loss: 0.5842 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8877 - accuracy: 0.0000e+00 - val_loss: 0.5843 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8868 - accuracy: 0.0000e+00 - val_loss: 0.5836 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.5837 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8851 - accuracy: 0.0000e+00 - val_loss: 0.5837 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.5841 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8836 - accuracy: 0.0000e+00 - val_loss: 0.5834 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8828 - accuracy: 0.0000e+00 - val_loss: 0.5837 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8819 - accuracy: 0.0000e+00 - val_loss: 0.5831 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8813 - accuracy: 0.0000e+00 - val_loss: 0.5816 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8805 - accuracy: 0.0000e+00 - val_loss: 0.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.5823 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8792 - accuracy: 0.0000e+00 - val_loss: 0.5825 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8783 - accuracy: 0.0000e+00 - val_loss: 0.5826 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8776 - accuracy: 0.0000e+00 - val_loss: 0.5823 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8770 - accuracy: 0.0000e+00 - val_loss: 0.5827 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8763 - accuracy: 0.0000e+00 - val_loss: 0.5817 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8757 - accuracy: 0.0000e+00 - val_loss: 0.5813 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.5812 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8743 - accuracy: 0.0000e+00 - val_loss: 0.5822 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.5819 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8730 - accuracy: 0.0000e+00 - val_loss: 0.5809 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8724 - accuracy: 0.0000e+00 - val_loss: 0.5812 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8717 - accuracy: 0.0000e+00 - val_loss: 0.5815 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.5808 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8704 - accuracy: 0.0000e+00 - val_loss: 0.5807 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.5808 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8694 - accuracy: 0.0000e+00 - val_loss: 0.5805 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8688 - accuracy: 0.0000e+00 - val_loss: 0.5809 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8682 - accuracy: 0.0000e+00 - val_loss: 0.5803 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8677 - accuracy: 0.0000e+00 - val_loss: 0.5802 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8672 - accuracy: 0.0000e+00 - val_loss: 0.5801 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8667 - accuracy: 0.0000e+00 - val_loss: 0.5795 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8662 - accuracy: 0.0000e+00 - val_loss: 0.5799 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8656 - accuracy: 0.0000e+00 - val_loss: 0.5794 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8652 - accuracy: 0.0000e+00 - val_loss: 0.5801 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.5798 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8640 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8636 - accuracy: 0.0000e+00 - val_loss: 0.5795 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8630 - accuracy: 0.0000e+00 - val_loss: 0.5787 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8626 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8620 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8617 - accuracy: 0.0000e+00 - val_loss: 0.5789 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8613 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8607 - accuracy: 0.0000e+00 - val_loss: 0.5778 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8602 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.5777 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8592 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8588 - accuracy: 0.0000e+00 - val_loss: 0.5780 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8583 - accuracy: 0.0000e+00 - val_loss: 0.5780 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8580 - accuracy: 0.0000e+00 - val_loss: 0.5775 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8575 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8569 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8566 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8561 - accuracy: 0.0000e+00 - val_loss: 0.5770 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8556 - accuracy: 0.0000e+00 - val_loss: 0.5765 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.5754 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8548 - accuracy: 0.0000e+00 - val_loss: 0.5752 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8543 - accuracy: 0.0000e+00 - val_loss: 0.5757 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8541 - accuracy: 0.0000e+00 - val_loss: 0.5751 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.5750 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8531 - accuracy: 0.0000e+00 - val_loss: 0.5744 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.5746 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8522 - accuracy: 0.0000e+00 - val_loss: 0.5743 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8518 - accuracy: 0.0000e+00 - val_loss: 0.5739 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8513 - accuracy: 0.0000e+00 - val_loss: 0.5735 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8510 - accuracy: 0.0000e+00 - val_loss: 0.5732 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - val_loss: 0.5734 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8502 - accuracy: 0.0000e+00 - val_loss: 0.5727 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8497 - accuracy: 0.0000e+00 - val_loss: 0.5721 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8494 - accuracy: 0.0000e+00 - val_loss: 0.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.5714 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8486 - accuracy: 0.0000e+00 - val_loss: 0.5713 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8482 - accuracy: 0.0000e+00 - val_loss: 0.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8478 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8473 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8470 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8466 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8461 - accuracy: 0.0000e+00 - val_loss: 0.5688 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8457 - accuracy: 0.0000e+00 - val_loss: 0.5690 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8455 - accuracy: 0.0000e+00 - val_loss: 0.5685 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8451 - accuracy: 0.0000e+00 - val_loss: 0.5682 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8448 - accuracy: 0.0000e+00 - val_loss: 0.5670 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8442 - accuracy: 0.0000e+00 - val_loss: 0.5669 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8438 - accuracy: 0.0000e+00 - val_loss: 0.5665 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8433 - accuracy: 0.0000e+00 - val_loss: 0.5663 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8430 - accuracy: 0.0000e+00 - val_loss: 0.5659 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8426 - accuracy: 0.0000e+00 - val_loss: 0.5662 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8423 - accuracy: 0.0000e+00 - val_loss: 0.5656 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8419 - accuracy: 0.0000e+00 - val_loss: 0.5654 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8415 - accuracy: 0.0000e+00 - val_loss: 0.5655 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8410 - accuracy: 0.0000e+00 - val_loss: 0.5650 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8406 - accuracy: 0.0000e+00 - val_loss: 0.5650 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8402 - accuracy: 0.0000e+00 - val_loss: 0.5648 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8398 - accuracy: 0.0000e+00 - val_loss: 0.5647 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8395 - accuracy: 0.0000e+00 - val_loss: 0.5640 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8390 - accuracy: 0.0000e+00 - val_loss: 0.5638 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8386 - accuracy: 0.0000e+00 - val_loss: 0.5637 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8383 - accuracy: 0.0000e+00 - val_loss: 0.5639 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8379 - accuracy: 0.0000e+00 - val_loss: 0.5637 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8375 - accuracy: 0.0000e+00 - val_loss: 0.5635 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8372 - accuracy: 0.0000e+00 - val_loss: 0.5631 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8369 - accuracy: 0.0000e+00 - val_loss: 0.5624 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8364 - accuracy: 0.0000e+00 - val_loss: 0.5628 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8360 - accuracy: 0.0000e+00 - val_loss: 0.5623 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8357 - accuracy: 0.0000e+00 - val_loss: 0.5620 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8353 - accuracy: 0.0000e+00 - val_loss: 0.5616 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8349 - accuracy: 0.0000e+00 - val_loss: 0.5618 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.5619 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8343 - accuracy: 0.0000e+00 - val_loss: 0.5618 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8341 - accuracy: 0.0000e+00 - val_loss: 0.5610 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8334 - accuracy: 0.0000e+00 - val_loss: 0.5610 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.5604 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8327 - accuracy: 0.0000e+00 - val_loss: 0.5604 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8324 - accuracy: 0.0000e+00 - val_loss: 0.5604 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8319 - accuracy: 0.0000e+00 - val_loss: 0.5602 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8316 - accuracy: 0.0000e+00 - val_loss: 0.5603 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.5603 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8308 - accuracy: 0.0000e+00 - val_loss: 0.5601 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8302 - accuracy: 0.0000e+00 - val_loss: 0.5610 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8298 - accuracy: 0.0000e+00 - val_loss: 0.5608 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8295 - accuracy: 0.0000e+00 - val_loss: 0.5604 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8291 - accuracy: 0.0000e+00 - val_loss: 0.5598 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8290 - accuracy: 0.0000e+00 - val_loss: 0.5599 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8285 - accuracy: 0.0000e+00 - val_loss: 0.5601 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8281 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8278 - accuracy: 0.0000e+00 - val_loss: 0.5604 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8275 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8273 - accuracy: 0.0000e+00 - val_loss: 0.5603 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8270 - accuracy: 0.0000e+00 - val_loss: 0.5600 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8267 - accuracy: 0.0000e+00 - val_loss: 0.5604 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.5603 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8261 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8259 - accuracy: 0.0000e+00 - val_loss: 0.5608 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8256 - accuracy: 0.0000e+00 - val_loss: 0.5608 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8253 - accuracy: 0.0000e+00 - val_loss: 0.5607 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8251 - accuracy: 0.0000e+00 - val_loss: 0.5608 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8249 - accuracy: 0.0000e+00 - val_loss: 0.5611 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8247 - accuracy: 0.0000e+00 - val_loss: 0.5612 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8243 - accuracy: 0.0000e+00 - val_loss: 0.5609 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8240 - accuracy: 0.0000e+00 - val_loss: 0.5613 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8238 - accuracy: 0.0000e+00 - val_loss: 0.5612 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8236 - accuracy: 0.0000e+00 - val_loss: 0.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8233 - accuracy: 0.0000e+00 - val_loss: 0.5611 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8231 - accuracy: 0.0000e+00 - val_loss: 0.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8229 - accuracy: 0.0000e+00 - val_loss: 0.5617 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8226 - accuracy: 0.0000e+00 - val_loss: 0.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 0.5611 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8223 - accuracy: 0.0000e+00 - val_loss: 0.5612 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8220 - accuracy: 0.0000e+00 - val_loss: 0.5611 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8218 - accuracy: 0.0000e+00 - val_loss: 0.5616 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8215 - accuracy: 0.0000e+00 - val_loss: 0.5620 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8213 - accuracy: 0.0000e+00 - val_loss: 0.5618 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8211 - accuracy: 0.0000e+00 - val_loss: 0.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8209 - accuracy: 0.0000e+00 - val_loss: 0.5622 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8207 - accuracy: 0.0000e+00 - val_loss: 0.5625 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8207 - accuracy: 0.0000e+00 - val_loss: 0.5621 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8203 - accuracy: 0.0000e+00 - val_loss: 0.5623 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8201 - accuracy: 0.0000e+00 - val_loss: 0.5623 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8199 - accuracy: 0.0000e+00 - val_loss: 0.5627 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8197 - accuracy: 0.0000e+00 - val_loss: 0.5626 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8195 - accuracy: 0.0000e+00 - val_loss: 0.5627 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/KO_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5627 - accuracy: 0.0000e+00\n",
      "[0.5626604557037354, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_26 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2281 - accuracy: 0.0000e+00 - val_loss: 0.4757 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2032 - accuracy: 0.0000e+00 - val_loss: 0.4827 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1872 - accuracy: 0.0000e+00 - val_loss: 0.4885 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1643 - accuracy: 0.0000e+00 - val_loss: 0.4939 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1498 - accuracy: 0.0000e+00 - val_loss: 0.4991 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1314 - accuracy: 0.0000e+00 - val_loss: 0.5042 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1167 - accuracy: 0.0000e+00 - val_loss: 0.5085 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1034 - accuracy: 0.0000e+00 - val_loss: 0.5127 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0908 - accuracy: 0.0000e+00 - val_loss: 0.5167 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0805 - accuracy: 0.0000e+00 - val_loss: 0.5214 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0678 - accuracy: 0.0000e+00 - val_loss: 0.5252 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.0592 - accuracy: 0.0000e+00 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0510 - accuracy: 0.0000e+00 - val_loss: 0.5328 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0433 - accuracy: 0.0000e+00 - val_loss: 0.5369 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0341 - accuracy: 0.0000e+00 - val_loss: 0.5403 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0288 - accuracy: 0.0000e+00 - val_loss: 0.5436 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0220 - accuracy: 0.0000e+00 - val_loss: 0.5468 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0173 - accuracy: 0.0000e+00 - val_loss: 0.5501 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0123 - accuracy: 0.0000e+00 - val_loss: 0.5532 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0075 - accuracy: 0.0000e+00 - val_loss: 0.5565 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0039 - accuracy: 0.0000e+00 - val_loss: 0.5597 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9998 - accuracy: 0.0000e+00 - val_loss: 0.5627 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9965 - accuracy: 0.0000e+00 - val_loss: 0.5656 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9932 - accuracy: 0.0000e+00 - val_loss: 0.5678 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9905 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9877 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9854 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9825 - accuracy: 0.0000e+00 - val_loss: 0.5778 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9806 - accuracy: 0.0000e+00 - val_loss: 0.5802 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9785 - accuracy: 0.0000e+00 - val_loss: 0.5826 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9767 - accuracy: 0.0000e+00 - val_loss: 0.5851 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9750 - accuracy: 0.0000e+00 - val_loss: 0.5874 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9730 - accuracy: 0.0000e+00 - val_loss: 0.5891 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9716 - accuracy: 0.0000e+00 - val_loss: 0.5908 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9700 - accuracy: 0.0000e+00 - val_loss: 0.5926 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9692 - accuracy: 0.0000e+00 - val_loss: 0.5947 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9674 - accuracy: 0.0000e+00 - val_loss: 0.5964 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9664 - accuracy: 0.0000e+00 - val_loss: 0.5983 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9651 - accuracy: 0.0000e+00 - val_loss: 0.6000 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9640 - accuracy: 0.0000e+00 - val_loss: 0.6020 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9632 - accuracy: 0.0000e+00 - val_loss: 0.6037 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9619 - accuracy: 0.0000e+00 - val_loss: 0.6053 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9608 - accuracy: 0.0000e+00 - val_loss: 0.6069 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9598 - accuracy: 0.0000e+00 - val_loss: 0.6084 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9590 - accuracy: 0.0000e+00 - val_loss: 0.6100 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9582 - accuracy: 0.0000e+00 - val_loss: 0.6117 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9571 - accuracy: 0.0000e+00 - val_loss: 0.6128 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9563 - accuracy: 0.0000e+00 - val_loss: 0.6136 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9554 - accuracy: 0.0000e+00 - val_loss: 0.6149 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9546 - accuracy: 0.0000e+00 - val_loss: 0.6160 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9540 - accuracy: 0.0000e+00 - val_loss: 0.6176 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9531 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9523 - accuracy: 0.0000e+00 - val_loss: 0.6198 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9515 - accuracy: 0.0000e+00 - val_loss: 0.6212 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9508 - accuracy: 0.0000e+00 - val_loss: 0.6222 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9499 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9493 - accuracy: 0.0000e+00 - val_loss: 0.6245 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9485 - accuracy: 0.0000e+00 - val_loss: 0.6258 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9479 - accuracy: 0.0000e+00 - val_loss: 0.6272 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9469 - accuracy: 0.0000e+00 - val_loss: 0.6279 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9461 - accuracy: 0.0000e+00 - val_loss: 0.6292 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9455 - accuracy: 0.0000e+00 - val_loss: 0.6301 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9448 - accuracy: 0.0000e+00 - val_loss: 0.6317 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9440 - accuracy: 0.0000e+00 - val_loss: 0.6330 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9434 - accuracy: 0.0000e+00 - val_loss: 0.6340 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9428 - accuracy: 0.0000e+00 - val_loss: 0.6348 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9422 - accuracy: 0.0000e+00 - val_loss: 0.6360 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9414 - accuracy: 0.0000e+00 - val_loss: 0.6372 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9409 - accuracy: 0.0000e+00 - val_loss: 0.6379 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.6389 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9395 - accuracy: 0.0000e+00 - val_loss: 0.6400 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9389 - accuracy: 0.0000e+00 - val_loss: 0.6412 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9384 - accuracy: 0.0000e+00 - val_loss: 0.6422 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9377 - accuracy: 0.0000e+00 - val_loss: 0.6429 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9370 - accuracy: 0.0000e+00 - val_loss: 0.6439 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9364 - accuracy: 0.0000e+00 - val_loss: 0.6448 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9358 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9352 - accuracy: 0.0000e+00 - val_loss: 0.6463 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9345 - accuracy: 0.0000e+00 - val_loss: 0.6468 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9338 - accuracy: 0.0000e+00 - val_loss: 0.6476 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9334 - accuracy: 0.0000e+00 - val_loss: 0.6484 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9326 - accuracy: 0.0000e+00 - val_loss: 0.6492 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9320 - accuracy: 0.0000e+00 - val_loss: 0.6501 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9313 - accuracy: 0.0000e+00 - val_loss: 0.6506 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9308 - accuracy: 0.0000e+00 - val_loss: 0.6515 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9301 - accuracy: 0.0000e+00 - val_loss: 0.6520 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9295 - accuracy: 0.0000e+00 - val_loss: 0.6529 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9291 - accuracy: 0.0000e+00 - val_loss: 0.6537 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9284 - accuracy: 0.0000e+00 - val_loss: 0.6542 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.6547 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9272 - accuracy: 0.0000e+00 - val_loss: 0.6557 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9267 - accuracy: 0.0000e+00 - val_loss: 0.6563 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9260 - accuracy: 0.0000e+00 - val_loss: 0.6568 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9255 - accuracy: 0.0000e+00 - val_loss: 0.6574 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9250 - accuracy: 0.0000e+00 - val_loss: 0.6577 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.6584 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9239 - accuracy: 0.0000e+00 - val_loss: 0.6592 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9231 - accuracy: 0.0000e+00 - val_loss: 0.6597 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9228 - accuracy: 0.0000e+00 - val_loss: 0.6599 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9221 - accuracy: 0.0000e+00 - val_loss: 0.6604 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9218 - accuracy: 0.0000e+00 - val_loss: 0.6608 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9212 - accuracy: 0.0000e+00 - val_loss: 0.6612 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9206 - accuracy: 0.0000e+00 - val_loss: 0.6619 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9201 - accuracy: 0.0000e+00 - val_loss: 0.6626 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9197 - accuracy: 0.0000e+00 - val_loss: 0.6630 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9189 - accuracy: 0.0000e+00 - val_loss: 0.6632 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9186 - accuracy: 0.0000e+00 - val_loss: 0.6640 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9179 - accuracy: 0.0000e+00 - val_loss: 0.6643 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9174 - accuracy: 0.0000e+00 - val_loss: 0.6648 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9170 - accuracy: 0.0000e+00 - val_loss: 0.6651 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9164 - accuracy: 0.0000e+00 - val_loss: 0.6654 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9159 - accuracy: 0.0000e+00 - val_loss: 0.6659 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9153 - accuracy: 0.0000e+00 - val_loss: 0.6663 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9149 - accuracy: 0.0000e+00 - val_loss: 0.6666 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9143 - accuracy: 0.0000e+00 - val_loss: 0.6669 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9140 - accuracy: 0.0000e+00 - val_loss: 0.6668 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9134 - accuracy: 0.0000e+00 - val_loss: 0.6675 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9129 - accuracy: 0.0000e+00 - val_loss: 0.6679 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9127 - accuracy: 0.0000e+00 - val_loss: 0.6682 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9120 - accuracy: 0.0000e+00 - val_loss: 0.6685 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9115 - accuracy: 0.0000e+00 - val_loss: 0.6690 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9110 - accuracy: 0.0000e+00 - val_loss: 0.6694 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9109 - accuracy: 0.0000e+00 - val_loss: 0.6693 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9101 - accuracy: 0.0000e+00 - val_loss: 0.6697 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9097 - accuracy: 0.0000e+00 - val_loss: 0.6700 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9093 - accuracy: 0.0000e+00 - val_loss: 0.6704 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9088 - accuracy: 0.0000e+00 - val_loss: 0.6708 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9084 - accuracy: 0.0000e+00 - val_loss: 0.6710 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9079 - accuracy: 0.0000e+00 - val_loss: 0.6714 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9074 - accuracy: 0.0000e+00 - val_loss: 0.6716 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9071 - accuracy: 0.0000e+00 - val_loss: 0.6719 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9067 - accuracy: 0.0000e+00 - val_loss: 0.6722 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9061 - accuracy: 0.0000e+00 - val_loss: 0.6725 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9056 - accuracy: 0.0000e+00 - val_loss: 0.6725 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9051 - accuracy: 0.0000e+00 - val_loss: 0.6729 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9049 - accuracy: 0.0000e+00 - val_loss: 0.6731 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9044 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9039 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9036 - accuracy: 0.0000e+00 - val_loss: 0.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9030 - accuracy: 0.0000e+00 - val_loss: 0.6740 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9027 - accuracy: 0.0000e+00 - val_loss: 0.6743 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9024 - accuracy: 0.0000e+00 - val_loss: 0.6742 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.6744 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9014 - accuracy: 0.0000e+00 - val_loss: 0.6744 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9011 - accuracy: 0.0000e+00 - val_loss: 0.6746 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.6747 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9001 - accuracy: 0.0000e+00 - val_loss: 0.6751 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8998 - accuracy: 0.0000e+00 - val_loss: 0.6754 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8994 - accuracy: 0.0000e+00 - val_loss: 0.6754 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8989 - accuracy: 0.0000e+00 - val_loss: 0.6756 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8985 - accuracy: 0.0000e+00 - val_loss: 0.6756 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.6756 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8977 - accuracy: 0.0000e+00 - val_loss: 0.6758 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8972 - accuracy: 0.0000e+00 - val_loss: 0.6761 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8968 - accuracy: 0.0000e+00 - val_loss: 0.6760 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8965 - accuracy: 0.0000e+00 - val_loss: 0.6761 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8960 - accuracy: 0.0000e+00 - val_loss: 0.6763 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8957 - accuracy: 0.0000e+00 - val_loss: 0.6763 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8952 - accuracy: 0.0000e+00 - val_loss: 0.6764 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8947 - accuracy: 0.0000e+00 - val_loss: 0.6767 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8944 - accuracy: 0.0000e+00 - val_loss: 0.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8941 - accuracy: 0.0000e+00 - val_loss: 0.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8937 - accuracy: 0.0000e+00 - val_loss: 0.6769 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8933 - accuracy: 0.0000e+00 - val_loss: 0.6771 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8929 - accuracy: 0.0000e+00 - val_loss: 0.6770 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8927 - accuracy: 0.0000e+00 - val_loss: 0.6770 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8921 - accuracy: 0.0000e+00 - val_loss: 0.6772 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8919 - accuracy: 0.0000e+00 - val_loss: 0.6771 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8915 - accuracy: 0.0000e+00 - val_loss: 0.6769 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.6771 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.6771 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8903 - accuracy: 0.0000e+00 - val_loss: 0.6769 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8900 - accuracy: 0.0000e+00 - val_loss: 0.6771 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8897 - accuracy: 0.0000e+00 - val_loss: 0.6771 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8894 - accuracy: 0.0000e+00 - val_loss: 0.6764 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8889 - accuracy: 0.0000e+00 - val_loss: 0.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8885 - accuracy: 0.0000e+00 - val_loss: 0.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8883 - accuracy: 0.0000e+00 - val_loss: 0.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.6766 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8875 - accuracy: 0.0000e+00 - val_loss: 0.6765 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8871 - accuracy: 0.0000e+00 - val_loss: 0.6764 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8866 - accuracy: 0.0000e+00 - val_loss: 0.6763 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8863 - accuracy: 0.0000e+00 - val_loss: 0.6763 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.6765 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.6760 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.6760 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.6759 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8845 - accuracy: 0.0000e+00 - val_loss: 0.6761 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8842 - accuracy: 0.0000e+00 - val_loss: 0.6763 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8840 - accuracy: 0.0000e+00 - val_loss: 0.6758 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8837 - accuracy: 0.0000e+00 - val_loss: 0.6758 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8831 - accuracy: 0.0000e+00 - val_loss: 0.6756 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8827 - accuracy: 0.0000e+00 - val_loss: 0.6753 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8823 - accuracy: 0.0000e+00 - val_loss: 0.6754 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8820 - accuracy: 0.0000e+00 - val_loss: 0.6750 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.6752 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - val_loss: 0.6751 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8809 - accuracy: 0.0000e+00 - val_loss: 0.6751 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8805 - accuracy: 0.0000e+00 - val_loss: 0.6748 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8803 - accuracy: 0.0000e+00 - val_loss: 0.6749 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8798 - accuracy: 0.0000e+00 - val_loss: 0.6750 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8795 - accuracy: 0.0000e+00 - val_loss: 0.6747 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8791 - accuracy: 0.0000e+00 - val_loss: 0.6746 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.6747 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8784 - accuracy: 0.0000e+00 - val_loss: 0.6746 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8784 - accuracy: 0.0000e+00 - val_loss: 0.6749 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.6744 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8775 - accuracy: 0.0000e+00 - val_loss: 0.6742 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8773 - accuracy: 0.0000e+00 - val_loss: 0.6738 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8763 - accuracy: 0.0000e+00 - val_loss: 0.6742 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8760 - accuracy: 0.0000e+00 - val_loss: 0.6741 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8756 - accuracy: 0.0000e+00 - val_loss: 0.6738 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8753 - accuracy: 0.0000e+00 - val_loss: 0.6738 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8751 - accuracy: 0.0000e+00 - val_loss: 0.6738 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.6743 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8738 - accuracy: 0.0000e+00 - val_loss: 0.6738 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8735 - accuracy: 0.0000e+00 - val_loss: 0.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8732 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8727 - accuracy: 0.0000e+00 - val_loss: 0.6735 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.6737 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8722 - accuracy: 0.0000e+00 - val_loss: 0.6737 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8719 - accuracy: 0.0000e+00 - val_loss: 0.6733 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8716 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.6735 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.6735 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8704 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.6737 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8699 - accuracy: 0.0000e+00 - val_loss: 0.6729 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8695 - accuracy: 0.0000e+00 - val_loss: 0.6732 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8692 - accuracy: 0.0000e+00 - val_loss: 0.6738 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8689 - accuracy: 0.0000e+00 - val_loss: 0.6737 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8686 - accuracy: 0.0000e+00 - val_loss: 0.6734 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8683 - accuracy: 0.0000e+00 - val_loss: 0.6731 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8682 - accuracy: 0.0000e+00 - val_loss: 0.6738 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8677 - accuracy: 0.0000e+00 - val_loss: 0.6737 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8675 - accuracy: 0.0000e+00 - val_loss: 0.6733 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8672 - accuracy: 0.0000e+00 - val_loss: 0.6737 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8669 - accuracy: 0.0000e+00 - val_loss: 0.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8669 - accuracy: 0.0000e+00 - val_loss: 0.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8665 - accuracy: 0.0000e+00 - val_loss: 0.6737 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8664 - accuracy: 0.0000e+00 - val_loss: 0.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8660 - accuracy: 0.0000e+00 - val_loss: 0.6735 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8660 - accuracy: 0.0000e+00 - val_loss: 0.6742 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8657 - accuracy: 0.0000e+00 - val_loss: 0.6741 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8650 - accuracy: 0.0000e+00 - val_loss: 0.6739 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8649 - accuracy: 0.0000e+00 - val_loss: 0.6741 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8651 - accuracy: 0.0000e+00 - val_loss: 0.6737 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8646 - accuracy: 0.0000e+00 - val_loss: 0.6744 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.6741 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.6742 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8637 - accuracy: 0.0000e+00 - val_loss: 0.6746 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/TMO_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6746 - accuracy: 0.0000e+00\n",
      "[0.6745617389678955, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_28 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/MCD_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 24ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.2786 - accuracy: 0.0000e+00 - val_loss: 0.3984 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2532 - accuracy: 0.0000e+00 - val_loss: 0.3989 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2295 - accuracy: 0.0000e+00 - val_loss: 0.3995 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2091 - accuracy: 0.0000e+00 - val_loss: 0.4006 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1919 - accuracy: 0.0000e+00 - val_loss: 0.4018 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1718 - accuracy: 0.0000e+00 - val_loss: 0.4034 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1555 - accuracy: 0.0000e+00 - val_loss: 0.4048 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1407 - accuracy: 0.0000e+00 - val_loss: 0.4064 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1280 - accuracy: 0.0000e+00 - val_loss: 0.4080 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1168 - accuracy: 0.0000e+00 - val_loss: 0.4101 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1052 - accuracy: 0.0000e+00 - val_loss: 0.4127 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0948 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0862 - accuracy: 0.0000e+00 - val_loss: 0.4171 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0768 - accuracy: 0.0000e+00 - val_loss: 0.4195 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0699 - accuracy: 0.0000e+00 - val_loss: 0.4218 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0622 - accuracy: 0.0000e+00 - val_loss: 0.4243 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0559 - accuracy: 0.0000e+00 - val_loss: 0.4264 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0508 - accuracy: 0.0000e+00 - val_loss: 0.4290 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0446 - accuracy: 0.0000e+00 - val_loss: 0.4312 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0390 - accuracy: 0.0000e+00 - val_loss: 0.4333 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0348 - accuracy: 0.0000e+00 - val_loss: 0.4357 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0312 - accuracy: 0.0000e+00 - val_loss: 0.4382 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0255 - accuracy: 0.0000e+00 - val_loss: 0.4406 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0216 - accuracy: 0.0000e+00 - val_loss: 0.4426 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0177 - accuracy: 0.0000e+00 - val_loss: 0.4450 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0135 - accuracy: 0.0000e+00 - val_loss: 0.4472 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0101 - accuracy: 0.0000e+00 - val_loss: 0.4496 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0067 - accuracy: 0.0000e+00 - val_loss: 0.4518 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0032 - accuracy: 0.0000e+00 - val_loss: 0.4540 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0000 - accuracy: 0.0000e+00 - val_loss: 0.4562 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9972 - accuracy: 0.0000e+00 - val_loss: 0.4582 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9940 - accuracy: 0.0000e+00 - val_loss: 0.4604 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9914 - accuracy: 0.0000e+00 - val_loss: 0.4627 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9885 - accuracy: 0.0000e+00 - val_loss: 0.4649 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9864 - accuracy: 0.0000e+00 - val_loss: 0.4671 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9834 - accuracy: 0.0000e+00 - val_loss: 0.4688 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9813 - accuracy: 0.0000e+00 - val_loss: 0.4709 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9792 - accuracy: 0.0000e+00 - val_loss: 0.4727 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9769 - accuracy: 0.0000e+00 - val_loss: 0.4748 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9746 - accuracy: 0.0000e+00 - val_loss: 0.4766 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9726 - accuracy: 0.0000e+00 - val_loss: 0.4785 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9711 - accuracy: 0.0000e+00 - val_loss: 0.4804 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9689 - accuracy: 0.0000e+00 - val_loss: 0.4821 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9673 - accuracy: 0.0000e+00 - val_loss: 0.4839 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9656 - accuracy: 0.0000e+00 - val_loss: 0.4853 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9637 - accuracy: 0.0000e+00 - val_loss: 0.4867 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9622 - accuracy: 0.0000e+00 - val_loss: 0.4878 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9607 - accuracy: 0.0000e+00 - val_loss: 0.4894 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9593 - accuracy: 0.0000e+00 - val_loss: 0.4911 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9578 - accuracy: 0.0000e+00 - val_loss: 0.4920 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9568 - accuracy: 0.0000e+00 - val_loss: 0.4934 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9554 - accuracy: 0.0000e+00 - val_loss: 0.4950 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9541 - accuracy: 0.0000e+00 - val_loss: 0.4964 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9532 - accuracy: 0.0000e+00 - val_loss: 0.4978 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9521 - accuracy: 0.0000e+00 - val_loss: 0.4997 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9509 - accuracy: 0.0000e+00 - val_loss: 0.5005 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9498 - accuracy: 0.0000e+00 - val_loss: 0.5012 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9488 - accuracy: 0.0000e+00 - val_loss: 0.5022 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9479 - accuracy: 0.0000e+00 - val_loss: 0.5031 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9469 - accuracy: 0.0000e+00 - val_loss: 0.5043 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9462 - accuracy: 0.0000e+00 - val_loss: 0.5052 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9453 - accuracy: 0.0000e+00 - val_loss: 0.5060 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9446 - accuracy: 0.0000e+00 - val_loss: 0.5066 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9436 - accuracy: 0.0000e+00 - val_loss: 0.5075 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9430 - accuracy: 0.0000e+00 - val_loss: 0.5084 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9421 - accuracy: 0.0000e+00 - val_loss: 0.5088 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9416 - accuracy: 0.0000e+00 - val_loss: 0.5095 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9407 - accuracy: 0.0000e+00 - val_loss: 0.5101 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9402 - accuracy: 0.0000e+00 - val_loss: 0.5109 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9396 - accuracy: 0.0000e+00 - val_loss: 0.5116 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9389 - accuracy: 0.0000e+00 - val_loss: 0.5121 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9383 - accuracy: 0.0000e+00 - val_loss: 0.5127 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9375 - accuracy: 0.0000e+00 - val_loss: 0.5129 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9371 - accuracy: 0.0000e+00 - val_loss: 0.5131 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9364 - accuracy: 0.0000e+00 - val_loss: 0.5136 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9358 - accuracy: 0.0000e+00 - val_loss: 0.5139 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9352 - accuracy: 0.0000e+00 - val_loss: 0.5137 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9346 - accuracy: 0.0000e+00 - val_loss: 0.5142 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9340 - accuracy: 0.0000e+00 - val_loss: 0.5144 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9336 - accuracy: 0.0000e+00 - val_loss: 0.5145 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9331 - accuracy: 0.0000e+00 - val_loss: 0.5145 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9324 - accuracy: 0.0000e+00 - val_loss: 0.5143 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9319 - accuracy: 0.0000e+00 - val_loss: 0.5148 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9313 - accuracy: 0.0000e+00 - val_loss: 0.5152 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9308 - accuracy: 0.0000e+00 - val_loss: 0.5147 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9301 - accuracy: 0.0000e+00 - val_loss: 0.5149 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9298 - accuracy: 0.0000e+00 - val_loss: 0.5145 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.5148 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9286 - accuracy: 0.0000e+00 - val_loss: 0.5146 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9280 - accuracy: 0.0000e+00 - val_loss: 0.5144 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.5141 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9269 - accuracy: 0.0000e+00 - val_loss: 0.5141 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9263 - accuracy: 0.0000e+00 - val_loss: 0.5140 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9259 - accuracy: 0.0000e+00 - val_loss: 0.5137 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9254 - accuracy: 0.0000e+00 - val_loss: 0.5137 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9247 - accuracy: 0.0000e+00 - val_loss: 0.5139 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.5139 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9238 - accuracy: 0.0000e+00 - val_loss: 0.5141 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9234 - accuracy: 0.0000e+00 - val_loss: 0.5139 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9229 - accuracy: 0.0000e+00 - val_loss: 0.5139 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9222 - accuracy: 0.0000e+00 - val_loss: 0.5142 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9218 - accuracy: 0.0000e+00 - val_loss: 0.5145 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9214 - accuracy: 0.0000e+00 - val_loss: 0.5144 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9210 - accuracy: 0.0000e+00 - val_loss: 0.5140 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9205 - accuracy: 0.0000e+00 - val_loss: 0.5137 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9199 - accuracy: 0.0000e+00 - val_loss: 0.5138 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9195 - accuracy: 0.0000e+00 - val_loss: 0.5135 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9191 - accuracy: 0.0000e+00 - val_loss: 0.5133 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9187 - accuracy: 0.0000e+00 - val_loss: 0.5136 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9183 - accuracy: 0.0000e+00 - val_loss: 0.5128 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9177 - accuracy: 0.0000e+00 - val_loss: 0.5128 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9174 - accuracy: 0.0000e+00 - val_loss: 0.5120 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9168 - accuracy: 0.0000e+00 - val_loss: 0.5116 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9163 - accuracy: 0.0000e+00 - val_loss: 0.5118 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9160 - accuracy: 0.0000e+00 - val_loss: 0.5111 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9155 - accuracy: 0.0000e+00 - val_loss: 0.5113 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9150 - accuracy: 0.0000e+00 - val_loss: 0.5109 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9146 - accuracy: 0.0000e+00 - val_loss: 0.5108 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9140 - accuracy: 0.0000e+00 - val_loss: 0.5111 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9137 - accuracy: 0.0000e+00 - val_loss: 0.5107 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9133 - accuracy: 0.0000e+00 - val_loss: 0.5103 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9127 - accuracy: 0.0000e+00 - val_loss: 0.5101 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9125 - accuracy: 0.0000e+00 - val_loss: 0.5090 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9118 - accuracy: 0.0000e+00 - val_loss: 0.5093 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9114 - accuracy: 0.0000e+00 - val_loss: 0.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9110 - accuracy: 0.0000e+00 - val_loss: 0.5088 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9106 - accuracy: 0.0000e+00 - val_loss: 0.5077 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9101 - accuracy: 0.0000e+00 - val_loss: 0.5074 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9096 - accuracy: 0.0000e+00 - val_loss: 0.5070 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9093 - accuracy: 0.0000e+00 - val_loss: 0.5060 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9088 - accuracy: 0.0000e+00 - val_loss: 0.5059 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9083 - accuracy: 0.0000e+00 - val_loss: 0.5055 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9080 - accuracy: 0.0000e+00 - val_loss: 0.5050 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9076 - accuracy: 0.0000e+00 - val_loss: 0.5040 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9071 - accuracy: 0.0000e+00 - val_loss: 0.5034 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9067 - accuracy: 0.0000e+00 - val_loss: 0.5037 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9065 - accuracy: 0.0000e+00 - val_loss: 0.5024 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9059 - accuracy: 0.0000e+00 - val_loss: 0.5020 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9055 - accuracy: 0.0000e+00 - val_loss: 0.5025 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9052 - accuracy: 0.0000e+00 - val_loss: 0.5020 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9047 - accuracy: 0.0000e+00 - val_loss: 0.5017 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9044 - accuracy: 0.0000e+00 - val_loss: 0.5008 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9040 - accuracy: 0.0000e+00 - val_loss: 0.5002 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9037 - accuracy: 0.0000e+00 - val_loss: 0.4994 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9032 - accuracy: 0.0000e+00 - val_loss: 0.4989 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.4984 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9023 - accuracy: 0.0000e+00 - val_loss: 0.4983 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9020 - accuracy: 0.0000e+00 - val_loss: 0.4977 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9017 - accuracy: 0.0000e+00 - val_loss: 0.4973 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9012 - accuracy: 0.0000e+00 - val_loss: 0.4966 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9009 - accuracy: 0.0000e+00 - val_loss: 0.4968 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.4956 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.4944 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8999 - accuracy: 0.0000e+00 - val_loss: 0.4934 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8994 - accuracy: 0.0000e+00 - val_loss: 0.4935 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8993 - accuracy: 0.0000e+00 - val_loss: 0.4925 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8987 - accuracy: 0.0000e+00 - val_loss: 0.4921 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8984 - accuracy: 0.0000e+00 - val_loss: 0.4927 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.4913 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8976 - accuracy: 0.0000e+00 - val_loss: 0.4908 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8975 - accuracy: 0.0000e+00 - val_loss: 0.4902 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8970 - accuracy: 0.0000e+00 - val_loss: 0.4898 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8967 - accuracy: 0.0000e+00 - val_loss: 0.4892 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8963 - accuracy: 0.0000e+00 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8960 - accuracy: 0.0000e+00 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8956 - accuracy: 0.0000e+00 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8954 - accuracy: 0.0000e+00 - val_loss: 0.4872 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8950 - accuracy: 0.0000e+00 - val_loss: 0.4862 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8946 - accuracy: 0.0000e+00 - val_loss: 0.4861 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8943 - accuracy: 0.0000e+00 - val_loss: 0.4859 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8940 - accuracy: 0.0000e+00 - val_loss: 0.4851 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8936 - accuracy: 0.0000e+00 - val_loss: 0.4844 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8933 - accuracy: 0.0000e+00 - val_loss: 0.4841 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8930 - accuracy: 0.0000e+00 - val_loss: 0.4846 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8926 - accuracy: 0.0000e+00 - val_loss: 0.4842 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.4831 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8919 - accuracy: 0.0000e+00 - val_loss: 0.4827 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8918 - accuracy: 0.0000e+00 - val_loss: 0.4825 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8912 - accuracy: 0.0000e+00 - val_loss: 0.4814 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8909 - accuracy: 0.0000e+00 - val_loss: 0.4810 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.4796 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8902 - accuracy: 0.0000e+00 - val_loss: 0.4790 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8898 - accuracy: 0.0000e+00 - val_loss: 0.4791 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8897 - accuracy: 0.0000e+00 - val_loss: 0.4784 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8892 - accuracy: 0.0000e+00 - val_loss: 0.4777 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8888 - accuracy: 0.0000e+00 - val_loss: 0.4770 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8884 - accuracy: 0.0000e+00 - val_loss: 0.4761 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8881 - accuracy: 0.0000e+00 - val_loss: 0.4760 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.4755 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8874 - accuracy: 0.0000e+00 - val_loss: 0.4755 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8870 - accuracy: 0.0000e+00 - val_loss: 0.4756 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 0.4743 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8864 - accuracy: 0.0000e+00 - val_loss: 0.4749 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.4737 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8856 - accuracy: 0.0000e+00 - val_loss: 0.4734 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8853 - accuracy: 0.0000e+00 - val_loss: 0.4731 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.4727 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8846 - accuracy: 0.0000e+00 - val_loss: 0.4725 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8842 - accuracy: 0.0000e+00 - val_loss: 0.4724 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8839 - accuracy: 0.0000e+00 - val_loss: 0.4718 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8834 - accuracy: 0.0000e+00 - val_loss: 0.4717 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8832 - accuracy: 0.0000e+00 - val_loss: 0.4707 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8828 - accuracy: 0.0000e+00 - val_loss: 0.4704 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8824 - accuracy: 0.0000e+00 - val_loss: 0.4701 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8822 - accuracy: 0.0000e+00 - val_loss: 0.4695 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8819 - accuracy: 0.0000e+00 - val_loss: 0.4703 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.4698 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8811 - accuracy: 0.0000e+00 - val_loss: 0.4700 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8808 - accuracy: 0.0000e+00 - val_loss: 0.4704 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8805 - accuracy: 0.0000e+00 - val_loss: 0.4712 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8805 - accuracy: 0.0000e+00 - val_loss: 0.4705 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.4708 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8796 - accuracy: 0.0000e+00 - val_loss: 0.4703 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8794 - accuracy: 0.0000e+00 - val_loss: 0.4711 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8791 - accuracy: 0.0000e+00 - val_loss: 0.4703 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8787 - accuracy: 0.0000e+00 - val_loss: 0.4706 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8784 - accuracy: 0.0000e+00 - val_loss: 0.4709 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.4709 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.4708 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8775 - accuracy: 0.0000e+00 - val_loss: 0.4707 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8772 - accuracy: 0.0000e+00 - val_loss: 0.4710 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8769 - accuracy: 0.0000e+00 - val_loss: 0.4715 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.4707 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8763 - accuracy: 0.0000e+00 - val_loss: 0.4717 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8760 - accuracy: 0.0000e+00 - val_loss: 0.4722 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8758 - accuracy: 0.0000e+00 - val_loss: 0.4729 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.4724 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8752 - accuracy: 0.0000e+00 - val_loss: 0.4726 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.4723 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.4717 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8744 - accuracy: 0.0000e+00 - val_loss: 0.4718 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.4722 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8739 - accuracy: 0.0000e+00 - val_loss: 0.4730 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.4730 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.4736 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.4730 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8729 - accuracy: 0.0000e+00 - val_loss: 0.4736 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8726 - accuracy: 0.0000e+00 - val_loss: 0.4741 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8724 - accuracy: 0.0000e+00 - val_loss: 0.4736 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8721 - accuracy: 0.0000e+00 - val_loss: 0.4738 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8719 - accuracy: 0.0000e+00 - val_loss: 0.4740 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8717 - accuracy: 0.0000e+00 - val_loss: 0.4739 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.4754 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.4745 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.4746 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8708 - accuracy: 0.0000e+00 - val_loss: 0.4755 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.4754 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8703 - accuracy: 0.0000e+00 - val_loss: 0.4750 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.4758 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8700 - accuracy: 0.0000e+00 - val_loss: 0.4746 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8697 - accuracy: 0.0000e+00 - val_loss: 0.4743 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8694 - accuracy: 0.0000e+00 - val_loss: 0.4759 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8692 - accuracy: 0.0000e+00 - val_loss: 0.4760 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8690 - accuracy: 0.0000e+00 - val_loss: 0.4761 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8687 - accuracy: 0.0000e+00 - val_loss: 0.4773 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8685 - accuracy: 0.0000e+00 - val_loss: 0.4772 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/DIS_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4772 - accuracy: 0.0000e+00\n",
      "[0.4771793782711029, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 18ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/WFC_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 22ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_34 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.3927 - accuracy: 0.0000e+00 - val_loss: 0.4048 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3618 - accuracy: 0.0000e+00 - val_loss: 0.4003 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3375 - accuracy: 0.0000e+00 - val_loss: 0.3964 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3146 - accuracy: 0.0000e+00 - val_loss: 0.3937 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2912 - accuracy: 0.0000e+00 - val_loss: 0.3918 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.2717 - accuracy: 0.0000e+00 - val_loss: 0.3922 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2525 - accuracy: 0.0000e+00 - val_loss: 0.3925 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.2365 - accuracy: 0.0000e+00 - val_loss: 0.3961 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2201 - accuracy: 0.0000e+00 - val_loss: 0.3997 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2062 - accuracy: 0.0000e+00 - val_loss: 0.4039 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1912 - accuracy: 0.0000e+00 - val_loss: 0.4114 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1796 - accuracy: 0.0000e+00 - val_loss: 0.4189 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1670 - accuracy: 0.0000e+00 - val_loss: 0.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1558 - accuracy: 0.0000e+00 - val_loss: 0.4353 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1454 - accuracy: 0.0000e+00 - val_loss: 0.4439 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1355 - accuracy: 0.0000e+00 - val_loss: 0.4524 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1282 - accuracy: 0.0000e+00 - val_loss: 0.4626 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1198 - accuracy: 0.0000e+00 - val_loss: 0.4722 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1099 - accuracy: 0.0000e+00 - val_loss: 0.4816 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1030 - accuracy: 0.0000e+00 - val_loss: 0.4912 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0942 - accuracy: 0.0000e+00 - val_loss: 0.4997 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0880 - accuracy: 0.0000e+00 - val_loss: 0.5097 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0786 - accuracy: 0.0000e+00 - val_loss: 0.5199 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0660 - accuracy: 0.0000e+00 - val_loss: 0.5284 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0561 - accuracy: 0.0000e+00 - val_loss: 0.5369 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0469 - accuracy: 0.0000e+00 - val_loss: 0.5457 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0342 - accuracy: 0.0000e+00 - val_loss: 0.5522 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0278 - accuracy: 0.0000e+00 - val_loss: 0.5583 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0203 - accuracy: 0.0000e+00 - val_loss: 0.5628 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0164 - accuracy: 0.0000e+00 - val_loss: 0.5672 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0121 - accuracy: 0.0000e+00 - val_loss: 0.5712 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0087 - accuracy: 0.0000e+00 - val_loss: 0.5742 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0063 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0039 - accuracy: 0.0000e+00 - val_loss: 0.5799 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0020 - accuracy: 0.0000e+00 - val_loss: 0.5830 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0003 - accuracy: 0.0000e+00 - val_loss: 0.5858 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9983 - accuracy: 0.0000e+00 - val_loss: 0.5878 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9967 - accuracy: 0.0000e+00 - val_loss: 0.5899 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9952 - accuracy: 0.0000e+00 - val_loss: 0.5919 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9938 - accuracy: 0.0000e+00 - val_loss: 0.5936 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9924 - accuracy: 0.0000e+00 - val_loss: 0.5956 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9912 - accuracy: 0.0000e+00 - val_loss: 0.5975 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9901 - accuracy: 0.0000e+00 - val_loss: 0.5993 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9888 - accuracy: 0.0000e+00 - val_loss: 0.6003 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9877 - accuracy: 0.0000e+00 - val_loss: 0.6019 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9867 - accuracy: 0.0000e+00 - val_loss: 0.6037 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9855 - accuracy: 0.0000e+00 - val_loss: 0.6048 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9845 - accuracy: 0.0000e+00 - val_loss: 0.6060 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9837 - accuracy: 0.0000e+00 - val_loss: 0.6075 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9826 - accuracy: 0.0000e+00 - val_loss: 0.6089 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9816 - accuracy: 0.0000e+00 - val_loss: 0.6099 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9808 - accuracy: 0.0000e+00 - val_loss: 0.6109 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9798 - accuracy: 0.0000e+00 - val_loss: 0.6121 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9790 - accuracy: 0.0000e+00 - val_loss: 0.6131 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9784 - accuracy: 0.0000e+00 - val_loss: 0.6143 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9773 - accuracy: 0.0000e+00 - val_loss: 0.6151 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9764 - accuracy: 0.0000e+00 - val_loss: 0.6160 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9757 - accuracy: 0.0000e+00 - val_loss: 0.6171 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9747 - accuracy: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9742 - accuracy: 0.0000e+00 - val_loss: 0.6183 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9730 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9722 - accuracy: 0.0000e+00 - val_loss: 0.6199 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9714 - accuracy: 0.0000e+00 - val_loss: 0.6206 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9705 - accuracy: 0.0000e+00 - val_loss: 0.6211 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9697 - accuracy: 0.0000e+00 - val_loss: 0.6220 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9689 - accuracy: 0.0000e+00 - val_loss: 0.6226 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9682 - accuracy: 0.0000e+00 - val_loss: 0.6226 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9674 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9666 - accuracy: 0.0000e+00 - val_loss: 0.6241 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9660 - accuracy: 0.0000e+00 - val_loss: 0.6241 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9650 - accuracy: 0.0000e+00 - val_loss: 0.6245 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9645 - accuracy: 0.0000e+00 - val_loss: 0.6254 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9634 - accuracy: 0.0000e+00 - val_loss: 0.6258 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9625 - accuracy: 0.0000e+00 - val_loss: 0.6261 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9616 - accuracy: 0.0000e+00 - val_loss: 0.6263 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9607 - accuracy: 0.0000e+00 - val_loss: 0.6265 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9602 - accuracy: 0.0000e+00 - val_loss: 0.6269 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9592 - accuracy: 0.0000e+00 - val_loss: 0.6270 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9584 - accuracy: 0.0000e+00 - val_loss: 0.6272 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9576 - accuracy: 0.0000e+00 - val_loss: 0.6272 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9565 - accuracy: 0.0000e+00 - val_loss: 0.6277 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9558 - accuracy: 0.0000e+00 - val_loss: 0.6278 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9551 - accuracy: 0.0000e+00 - val_loss: 0.6279 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9539 - accuracy: 0.0000e+00 - val_loss: 0.6282 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9535 - accuracy: 0.0000e+00 - val_loss: 0.6280 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9522 - accuracy: 0.0000e+00 - val_loss: 0.6283 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9513 - accuracy: 0.0000e+00 - val_loss: 0.6284 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9504 - accuracy: 0.0000e+00 - val_loss: 0.6283 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9494 - accuracy: 0.0000e+00 - val_loss: 0.6284 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9486 - accuracy: 0.0000e+00 - val_loss: 0.6286 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9476 - accuracy: 0.0000e+00 - val_loss: 0.6287 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9468 - accuracy: 0.0000e+00 - val_loss: 0.6291 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9458 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9452 - accuracy: 0.0000e+00 - val_loss: 0.6288 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9440 - accuracy: 0.0000e+00 - val_loss: 0.6288 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9429 - accuracy: 0.0000e+00 - val_loss: 0.6289 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9423 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9411 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9401 - accuracy: 0.0000e+00 - val_loss: 0.6291 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9395 - accuracy: 0.0000e+00 - val_loss: 0.6297 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9386 - accuracy: 0.0000e+00 - val_loss: 0.6294 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9377 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9366 - accuracy: 0.0000e+00 - val_loss: 0.6292 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9355 - accuracy: 0.0000e+00 - val_loss: 0.6295 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9346 - accuracy: 0.0000e+00 - val_loss: 0.6291 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9335 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9325 - accuracy: 0.0000e+00 - val_loss: 0.6288 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9317 - accuracy: 0.0000e+00 - val_loss: 0.6291 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9308 - accuracy: 0.0000e+00 - val_loss: 0.6289 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9300 - accuracy: 0.0000e+00 - val_loss: 0.6285 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9285 - accuracy: 0.0000e+00 - val_loss: 0.6287 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9279 - accuracy: 0.0000e+00 - val_loss: 0.6287 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9273 - accuracy: 0.0000e+00 - val_loss: 0.6283 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9261 - accuracy: 0.0000e+00 - val_loss: 0.6283 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9252 - accuracy: 0.0000e+00 - val_loss: 0.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9239 - accuracy: 0.0000e+00 - val_loss: 0.6282 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9234 - accuracy: 0.0000e+00 - val_loss: 0.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9223 - accuracy: 0.0000e+00 - val_loss: 0.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9214 - accuracy: 0.0000e+00 - val_loss: 0.6283 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9203 - accuracy: 0.0000e+00 - val_loss: 0.6284 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9195 - accuracy: 0.0000e+00 - val_loss: 0.6284 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9190 - accuracy: 0.0000e+00 - val_loss: 0.6282 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9176 - accuracy: 0.0000e+00 - val_loss: 0.6282 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9169 - accuracy: 0.0000e+00 - val_loss: 0.6287 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9160 - accuracy: 0.0000e+00 - val_loss: 0.6286 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9149 - accuracy: 0.0000e+00 - val_loss: 0.6284 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9147 - accuracy: 0.0000e+00 - val_loss: 0.6289 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9133 - accuracy: 0.0000e+00 - val_loss: 0.6287 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9126 - accuracy: 0.0000e+00 - val_loss: 0.6286 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9116 - accuracy: 0.0000e+00 - val_loss: 0.6286 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9107 - accuracy: 0.0000e+00 - val_loss: 0.6289 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9099 - accuracy: 0.0000e+00 - val_loss: 0.6291 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9095 - accuracy: 0.0000e+00 - val_loss: 0.6286 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9082 - accuracy: 0.0000e+00 - val_loss: 0.6286 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9078 - accuracy: 0.0000e+00 - val_loss: 0.6284 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9066 - accuracy: 0.0000e+00 - val_loss: 0.6289 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9058 - accuracy: 0.0000e+00 - val_loss: 0.6294 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9053 - accuracy: 0.0000e+00 - val_loss: 0.6292 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9043 - accuracy: 0.0000e+00 - val_loss: 0.6296 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9034 - accuracy: 0.0000e+00 - val_loss: 0.6296 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9027 - accuracy: 0.0000e+00 - val_loss: 0.6295 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.6298 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9013 - accuracy: 0.0000e+00 - val_loss: 0.6299 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.6300 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8999 - accuracy: 0.0000e+00 - val_loss: 0.6305 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8990 - accuracy: 0.0000e+00 - val_loss: 0.6305 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8982 - accuracy: 0.0000e+00 - val_loss: 0.6308 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8975 - accuracy: 0.0000e+00 - val_loss: 0.6310 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8967 - accuracy: 0.0000e+00 - val_loss: 0.6310 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8961 - accuracy: 0.0000e+00 - val_loss: 0.6310 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8955 - accuracy: 0.0000e+00 - val_loss: 0.6316 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8946 - accuracy: 0.0000e+00 - val_loss: 0.6313 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8941 - accuracy: 0.0000e+00 - val_loss: 0.6311 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8933 - accuracy: 0.0000e+00 - val_loss: 0.6313 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8927 - accuracy: 0.0000e+00 - val_loss: 0.6317 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8920 - accuracy: 0.0000e+00 - val_loss: 0.6320 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.6319 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8905 - accuracy: 0.0000e+00 - val_loss: 0.6319 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8900 - accuracy: 0.0000e+00 - val_loss: 0.6320 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8893 - accuracy: 0.0000e+00 - val_loss: 0.6321 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.6323 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8879 - accuracy: 0.0000e+00 - val_loss: 0.6327 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8874 - accuracy: 0.0000e+00 - val_loss: 0.6328 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 0.6329 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8864 - accuracy: 0.0000e+00 - val_loss: 0.6334 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8856 - accuracy: 0.0000e+00 - val_loss: 0.6333 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.6336 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8842 - accuracy: 0.0000e+00 - val_loss: 0.6335 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8837 - accuracy: 0.0000e+00 - val_loss: 0.6338 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8831 - accuracy: 0.0000e+00 - val_loss: 0.6340 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8826 - accuracy: 0.0000e+00 - val_loss: 0.6344 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8819 - accuracy: 0.0000e+00 - val_loss: 0.6346 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8812 - accuracy: 0.0000e+00 - val_loss: 0.6346 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8808 - accuracy: 0.0000e+00 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8801 - accuracy: 0.0000e+00 - val_loss: 0.6350 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8796 - accuracy: 0.0000e+00 - val_loss: 0.6352 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8790 - accuracy: 0.0000e+00 - val_loss: 0.6355 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8788 - accuracy: 0.0000e+00 - val_loss: 0.6353 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.6358 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8774 - accuracy: 0.0000e+00 - val_loss: 0.6361 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.6365 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8764 - accuracy: 0.0000e+00 - val_loss: 0.6367 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8758 - accuracy: 0.0000e+00 - val_loss: 0.6371 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.6371 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8748 - accuracy: 0.0000e+00 - val_loss: 0.6375 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.6378 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8738 - accuracy: 0.0000e+00 - val_loss: 0.6378 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.6381 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8726 - accuracy: 0.0000e+00 - val_loss: 0.6385 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8721 - accuracy: 0.0000e+00 - val_loss: 0.6388 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8716 - accuracy: 0.0000e+00 - val_loss: 0.6388 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8711 - accuracy: 0.0000e+00 - val_loss: 0.6391 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8708 - accuracy: 0.0000e+00 - val_loss: 0.6392 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.6393 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.6396 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8692 - accuracy: 0.0000e+00 - val_loss: 0.6400 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8686 - accuracy: 0.0000e+00 - val_loss: 0.6400 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8681 - accuracy: 0.0000e+00 - val_loss: 0.6402 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8677 - accuracy: 0.0000e+00 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8672 - accuracy: 0.0000e+00 - val_loss: 0.6409 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8668 - accuracy: 0.0000e+00 - val_loss: 0.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8662 - accuracy: 0.0000e+00 - val_loss: 0.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8658 - accuracy: 0.0000e+00 - val_loss: 0.6414 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.6417 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8649 - accuracy: 0.0000e+00 - val_loss: 0.6418 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8646 - accuracy: 0.0000e+00 - val_loss: 0.6420 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.6421 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8636 - accuracy: 0.0000e+00 - val_loss: 0.6427 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8631 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8627 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8623 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8619 - accuracy: 0.0000e+00 - val_loss: 0.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8615 - accuracy: 0.0000e+00 - val_loss: 0.6434 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8611 - accuracy: 0.0000e+00 - val_loss: 0.6434 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8606 - accuracy: 0.0000e+00 - val_loss: 0.6436 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8603 - accuracy: 0.0000e+00 - val_loss: 0.6438 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8598 - accuracy: 0.0000e+00 - val_loss: 0.6438 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8594 - accuracy: 0.0000e+00 - val_loss: 0.6438 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8591 - accuracy: 0.0000e+00 - val_loss: 0.6440 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8586 - accuracy: 0.0000e+00 - val_loss: 0.6440 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8582 - accuracy: 0.0000e+00 - val_loss: 0.6444 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8579 - accuracy: 0.0000e+00 - val_loss: 0.6447 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8574 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8572 - accuracy: 0.0000e+00 - val_loss: 0.6447 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8568 - accuracy: 0.0000e+00 - val_loss: 0.6446 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8564 - accuracy: 0.0000e+00 - val_loss: 0.6447 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8561 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8556 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8554 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8549 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8546 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8543 - accuracy: 0.0000e+00 - val_loss: 0.6457 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.6459 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8532 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8529 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8523 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8519 - accuracy: 0.0000e+00 - val_loss: 0.6454 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8516 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8512 - accuracy: 0.0000e+00 - val_loss: 0.6456 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8509 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8503 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8500 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8496 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8494 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8487 - accuracy: 0.0000e+00 - val_loss: 0.6452 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8484 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8482 - accuracy: 0.0000e+00 - val_loss: 0.6450 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8478 - accuracy: 0.0000e+00 - val_loss: 0.6447 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8475 - accuracy: 0.0000e+00 - val_loss: 0.6446 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8472 - accuracy: 0.0000e+00 - val_loss: 0.6445 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8469 - accuracy: 0.0000e+00 - val_loss: 0.6447 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8467 - accuracy: 0.0000e+00 - val_loss: 0.6449 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/ABT_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6449 - accuracy: 0.0000e+00\n",
      "[0.6449006795883179, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2010 - accuracy: 0.0000e+00 - val_loss: 0.4358 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1770 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1604 - accuracy: 0.0000e+00 - val_loss: 0.4284 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1466 - accuracy: 0.0000e+00 - val_loss: 0.4242 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1293 - accuracy: 0.0000e+00 - val_loss: 0.4204 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1188 - accuracy: 0.0000e+00 - val_loss: 0.4158 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1088 - accuracy: 0.0000e+00 - val_loss: 0.4110 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0982 - accuracy: 0.0000e+00 - val_loss: 0.4061 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0891 - accuracy: 0.0000e+00 - val_loss: 0.4014 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0813 - accuracy: 0.0000e+00 - val_loss: 0.3965 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0732 - accuracy: 0.0000e+00 - val_loss: 0.3929 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0663 - accuracy: 0.0000e+00 - val_loss: 0.3889 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0596 - accuracy: 0.0000e+00 - val_loss: 0.3858 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0534 - accuracy: 0.0000e+00 - val_loss: 0.3830 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0477 - accuracy: 0.0000e+00 - val_loss: 0.3805 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0425 - accuracy: 0.0000e+00 - val_loss: 0.3787 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0361 - accuracy: 0.0000e+00 - val_loss: 0.3784 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0308 - accuracy: 0.0000e+00 - val_loss: 0.3902 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0242 - accuracy: 0.0000e+00 - val_loss: 0.4934 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0184 - accuracy: 0.0000e+00 - val_loss: 0.5586 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0137 - accuracy: 0.0000e+00 - val_loss: 0.5691 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0092 - accuracy: 0.0000e+00 - val_loss: 0.5707 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0031 - accuracy: 0.0000e+00 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9995 - accuracy: 0.0000e+00 - val_loss: 0.5742 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9941 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9905 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9858 - accuracy: 0.0000e+00 - val_loss: 0.5787 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9830 - accuracy: 0.0000e+00 - val_loss: 0.5803 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9793 - accuracy: 0.0000e+00 - val_loss: 0.5818 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9764 - accuracy: 0.0000e+00 - val_loss: 0.5835 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9731 - accuracy: 0.0000e+00 - val_loss: 0.5849 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9711 - accuracy: 0.0000e+00 - val_loss: 0.5860 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9683 - accuracy: 0.0000e+00 - val_loss: 0.5872 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9654 - accuracy: 0.0000e+00 - val_loss: 0.5882 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9633 - accuracy: 0.0000e+00 - val_loss: 0.5890 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9616 - accuracy: 0.0000e+00 - val_loss: 0.5901 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9593 - accuracy: 0.0000e+00 - val_loss: 0.5911 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9574 - accuracy: 0.0000e+00 - val_loss: 0.5916 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9554 - accuracy: 0.0000e+00 - val_loss: 0.5925 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9536 - accuracy: 0.0000e+00 - val_loss: 0.5933 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9519 - accuracy: 0.0000e+00 - val_loss: 0.5940 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9505 - accuracy: 0.0000e+00 - val_loss: 0.5947 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9487 - accuracy: 0.0000e+00 - val_loss: 0.5950 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9473 - accuracy: 0.0000e+00 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9460 - accuracy: 0.0000e+00 - val_loss: 0.5960 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9447 - accuracy: 0.0000e+00 - val_loss: 0.5966 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9433 - accuracy: 0.0000e+00 - val_loss: 0.5969 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9420 - accuracy: 0.0000e+00 - val_loss: 0.5969 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9407 - accuracy: 0.0000e+00 - val_loss: 0.5975 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9394 - accuracy: 0.0000e+00 - val_loss: 0.5978 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9380 - accuracy: 0.0000e+00 - val_loss: 0.5981 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9370 - accuracy: 0.0000e+00 - val_loss: 0.5984 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9358 - accuracy: 0.0000e+00 - val_loss: 0.5985 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9347 - accuracy: 0.0000e+00 - val_loss: 0.5987 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9333 - accuracy: 0.0000e+00 - val_loss: 0.5990 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9324 - accuracy: 0.0000e+00 - val_loss: 0.5992 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9312 - accuracy: 0.0000e+00 - val_loss: 0.5996 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9300 - accuracy: 0.0000e+00 - val_loss: 0.5997 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9291 - accuracy: 0.0000e+00 - val_loss: 0.6002 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9278 - accuracy: 0.0000e+00 - val_loss: 0.6005 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9268 - accuracy: 0.0000e+00 - val_loss: 0.6004 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9258 - accuracy: 0.0000e+00 - val_loss: 0.6007 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9248 - accuracy: 0.0000e+00 - val_loss: 0.6010 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9237 - accuracy: 0.0000e+00 - val_loss: 0.6012 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9227 - accuracy: 0.0000e+00 - val_loss: 0.6015 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9217 - accuracy: 0.0000e+00 - val_loss: 0.6019 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9207 - accuracy: 0.0000e+00 - val_loss: 0.6022 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9198 - accuracy: 0.0000e+00 - val_loss: 0.6024 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9188 - accuracy: 0.0000e+00 - val_loss: 0.6026 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9178 - accuracy: 0.0000e+00 - val_loss: 0.6029 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9174 - accuracy: 0.0000e+00 - val_loss: 0.6032 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9159 - accuracy: 0.0000e+00 - val_loss: 0.6035 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9148 - accuracy: 0.0000e+00 - val_loss: 0.6039 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9138 - accuracy: 0.0000e+00 - val_loss: 0.6041 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9127 - accuracy: 0.0000e+00 - val_loss: 0.6042 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9117 - accuracy: 0.0000e+00 - val_loss: 0.6045 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9108 - accuracy: 0.0000e+00 - val_loss: 0.6047 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9098 - accuracy: 0.0000e+00 - val_loss: 0.6050 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9088 - accuracy: 0.0000e+00 - val_loss: 0.6052 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9080 - accuracy: 0.0000e+00 - val_loss: 0.6054 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9070 - accuracy: 0.0000e+00 - val_loss: 0.6057 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9060 - accuracy: 0.0000e+00 - val_loss: 0.6058 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9050 - accuracy: 0.0000e+00 - val_loss: 0.6061 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9040 - accuracy: 0.0000e+00 - val_loss: 0.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9031 - accuracy: 0.0000e+00 - val_loss: 0.6069 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9021 - accuracy: 0.0000e+00 - val_loss: 0.6072 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9011 - accuracy: 0.0000e+00 - val_loss: 0.6074 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.6078 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8990 - accuracy: 0.0000e+00 - val_loss: 0.6080 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8980 - accuracy: 0.0000e+00 - val_loss: 0.6086 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8972 - accuracy: 0.0000e+00 - val_loss: 0.6090 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8962 - accuracy: 0.0000e+00 - val_loss: 0.6093 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8951 - accuracy: 0.0000e+00 - val_loss: 0.6097 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8941 - accuracy: 0.0000e+00 - val_loss: 0.6101 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8932 - accuracy: 0.0000e+00 - val_loss: 0.6106 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.6109 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.6113 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8904 - accuracy: 0.0000e+00 - val_loss: 0.6116 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8897 - accuracy: 0.0000e+00 - val_loss: 0.6123 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.6128 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8877 - accuracy: 0.0000e+00 - val_loss: 0.6131 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8869 - accuracy: 0.0000e+00 - val_loss: 0.6134 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.6138 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8851 - accuracy: 0.0000e+00 - val_loss: 0.6143 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8842 - accuracy: 0.0000e+00 - val_loss: 0.6148 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8834 - accuracy: 0.0000e+00 - val_loss: 0.6152 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8825 - accuracy: 0.0000e+00 - val_loss: 0.6157 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.6161 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8808 - accuracy: 0.0000e+00 - val_loss: 0.6166 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8800 - accuracy: 0.0000e+00 - val_loss: 0.6170 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8793 - accuracy: 0.0000e+00 - val_loss: 0.6173 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8783 - accuracy: 0.0000e+00 - val_loss: 0.6178 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8775 - accuracy: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8760 - accuracy: 0.0000e+00 - val_loss: 0.6191 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8753 - accuracy: 0.0000e+00 - val_loss: 0.6196 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.6201 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8738 - accuracy: 0.0000e+00 - val_loss: 0.6205 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8729 - accuracy: 0.0000e+00 - val_loss: 0.6209 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8723 - accuracy: 0.0000e+00 - val_loss: 0.6213 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.6217 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8711 - accuracy: 0.0000e+00 - val_loss: 0.6221 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.6226 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8695 - accuracy: 0.0000e+00 - val_loss: 0.6230 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8689 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8682 - accuracy: 0.0000e+00 - val_loss: 0.6239 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8675 - accuracy: 0.0000e+00 - val_loss: 0.6243 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8669 - accuracy: 0.0000e+00 - val_loss: 0.6247 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8663 - accuracy: 0.0000e+00 - val_loss: 0.6251 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8658 - accuracy: 0.0000e+00 - val_loss: 0.6254 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8650 - accuracy: 0.0000e+00 - val_loss: 0.6257 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.6261 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8639 - accuracy: 0.0000e+00 - val_loss: 0.6265 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8634 - accuracy: 0.0000e+00 - val_loss: 0.6268 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8629 - accuracy: 0.0000e+00 - val_loss: 0.6271 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8621 - accuracy: 0.0000e+00 - val_loss: 0.6273 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8616 - accuracy: 0.0000e+00 - val_loss: 0.6277 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8612 - accuracy: 0.0000e+00 - val_loss: 0.6282 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8605 - accuracy: 0.0000e+00 - val_loss: 0.6285 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8602 - accuracy: 0.0000e+00 - val_loss: 0.6288 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.6290 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8590 - accuracy: 0.0000e+00 - val_loss: 0.6293 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8584 - accuracy: 0.0000e+00 - val_loss: 0.6295 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8580 - accuracy: 0.0000e+00 - val_loss: 0.6299 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8575 - accuracy: 0.0000e+00 - val_loss: 0.6301 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8570 - accuracy: 0.0000e+00 - val_loss: 0.6303 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8566 - accuracy: 0.0000e+00 - val_loss: 0.6308 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8560 - accuracy: 0.0000e+00 - val_loss: 0.6310 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8557 - accuracy: 0.0000e+00 - val_loss: 0.6313 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.6315 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8547 - accuracy: 0.0000e+00 - val_loss: 0.6318 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8544 - accuracy: 0.0000e+00 - val_loss: 0.6321 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.6324 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.6325 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8530 - accuracy: 0.0000e+00 - val_loss: 0.6327 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.6330 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8522 - accuracy: 0.0000e+00 - val_loss: 0.6332 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8519 - accuracy: 0.0000e+00 - val_loss: 0.6334 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8516 - accuracy: 0.0000e+00 - val_loss: 0.6337 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8511 - accuracy: 0.0000e+00 - val_loss: 0.6340 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8507 - accuracy: 0.0000e+00 - val_loss: 0.6341 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8503 - accuracy: 0.0000e+00 - val_loss: 0.6343 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8500 - accuracy: 0.0000e+00 - val_loss: 0.6346 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8497 - accuracy: 0.0000e+00 - val_loss: 0.6348 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8492 - accuracy: 0.0000e+00 - val_loss: 0.6350 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8489 - accuracy: 0.0000e+00 - val_loss: 0.6352 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8486 - accuracy: 0.0000e+00 - val_loss: 0.6354 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8482 - accuracy: 0.0000e+00 - val_loss: 0.6356 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8479 - accuracy: 0.0000e+00 - val_loss: 0.6358 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8476 - accuracy: 0.0000e+00 - val_loss: 0.6360 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8472 - accuracy: 0.0000e+00 - val_loss: 0.6362 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8468 - accuracy: 0.0000e+00 - val_loss: 0.6364 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8466 - accuracy: 0.0000e+00 - val_loss: 0.6366 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8462 - accuracy: 0.0000e+00 - val_loss: 0.6367 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8459 - accuracy: 0.0000e+00 - val_loss: 0.6368 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8457 - accuracy: 0.0000e+00 - val_loss: 0.6370 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8454 - accuracy: 0.0000e+00 - val_loss: 0.6372 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8450 - accuracy: 0.0000e+00 - val_loss: 0.6374 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8448 - accuracy: 0.0000e+00 - val_loss: 0.6376 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8445 - accuracy: 0.0000e+00 - val_loss: 0.6377 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8441 - accuracy: 0.0000e+00 - val_loss: 0.6378 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8439 - accuracy: 0.0000e+00 - val_loss: 0.6380 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8437 - accuracy: 0.0000e+00 - val_loss: 0.6381 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8434 - accuracy: 0.0000e+00 - val_loss: 0.6383 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8431 - accuracy: 0.0000e+00 - val_loss: 0.6384 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8429 - accuracy: 0.0000e+00 - val_loss: 0.6385 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8426 - accuracy: 0.0000e+00 - val_loss: 0.6387 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.6388 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8421 - accuracy: 0.0000e+00 - val_loss: 0.6389 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8419 - accuracy: 0.0000e+00 - val_loss: 0.6390 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8417 - accuracy: 0.0000e+00 - val_loss: 0.6391 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8415 - accuracy: 0.0000e+00 - val_loss: 0.6392 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8413 - accuracy: 0.0000e+00 - val_loss: 0.6394 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8410 - accuracy: 0.0000e+00 - val_loss: 0.6395 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8408 - accuracy: 0.0000e+00 - val_loss: 0.6396 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8405 - accuracy: 0.0000e+00 - val_loss: 0.6397 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8403 - accuracy: 0.0000e+00 - val_loss: 0.6398 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8402 - accuracy: 0.0000e+00 - val_loss: 0.6399 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8399 - accuracy: 0.0000e+00 - val_loss: 0.6400 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8397 - accuracy: 0.0000e+00 - val_loss: 0.6401 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8395 - accuracy: 0.0000e+00 - val_loss: 0.6402 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8393 - accuracy: 0.0000e+00 - val_loss: 0.6403 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8391 - accuracy: 0.0000e+00 - val_loss: 0.6404 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8389 - accuracy: 0.0000e+00 - val_loss: 0.6405 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8387 - accuracy: 0.0000e+00 - val_loss: 0.6406 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8385 - accuracy: 0.0000e+00 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8384 - accuracy: 0.0000e+00 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8381 - accuracy: 0.0000e+00 - val_loss: 0.6408 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8379 - accuracy: 0.0000e+00 - val_loss: 0.6409 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8378 - accuracy: 0.0000e+00 - val_loss: 0.6410 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8376 - accuracy: 0.0000e+00 - val_loss: 0.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8374 - accuracy: 0.0000e+00 - val_loss: 0.6412 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.6412 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8371 - accuracy: 0.0000e+00 - val_loss: 0.6413 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8370 - accuracy: 0.0000e+00 - val_loss: 0.6415 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8368 - accuracy: 0.0000e+00 - val_loss: 0.6416 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8367 - accuracy: 0.0000e+00 - val_loss: 0.6416 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8365 - accuracy: 0.0000e+00 - val_loss: 0.6416 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8364 - accuracy: 0.0000e+00 - val_loss: 0.6417 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8362 - accuracy: 0.0000e+00 - val_loss: 0.6417 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8360 - accuracy: 0.0000e+00 - val_loss: 0.6418 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8358 - accuracy: 0.0000e+00 - val_loss: 0.6418 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8357 - accuracy: 0.0000e+00 - val_loss: 0.6419 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8356 - accuracy: 0.0000e+00 - val_loss: 0.6420 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8354 - accuracy: 0.0000e+00 - val_loss: 0.6421 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8353 - accuracy: 0.0000e+00 - val_loss: 0.6421 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8352 - accuracy: 0.0000e+00 - val_loss: 0.6422 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8350 - accuracy: 0.0000e+00 - val_loss: 0.6422 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8349 - accuracy: 0.0000e+00 - val_loss: 0.6422 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8347 - accuracy: 0.0000e+00 - val_loss: 0.6423 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8345 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8344 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8342 - accuracy: 0.0000e+00 - val_loss: 0.6425 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8341 - accuracy: 0.0000e+00 - val_loss: 0.6425 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8340 - accuracy: 0.0000e+00 - val_loss: 0.6426 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8339 - accuracy: 0.0000e+00 - val_loss: 0.6426 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8337 - accuracy: 0.0000e+00 - val_loss: 0.6427 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8336 - accuracy: 0.0000e+00 - val_loss: 0.6427 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8335 - accuracy: 0.0000e+00 - val_loss: 0.6428 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8333 - accuracy: 0.0000e+00 - val_loss: 0.6428 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8332 - accuracy: 0.0000e+00 - val_loss: 0.6429 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.6429 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8330 - accuracy: 0.0000e+00 - val_loss: 0.6429 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8329 - accuracy: 0.0000e+00 - val_loss: 0.6430 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8328 - accuracy: 0.0000e+00 - val_loss: 0.6430 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8327 - accuracy: 0.0000e+00 - val_loss: 0.6430 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8326 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8324 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8323 - accuracy: 0.0000e+00 - val_loss: 0.6431 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8322 - accuracy: 0.0000e+00 - val_loss: 0.6432 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8321 - accuracy: 0.0000e+00 - val_loss: 0.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8319 - accuracy: 0.0000e+00 - val_loss: 0.6432 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8319 - accuracy: 0.0000e+00 - val_loss: 0.6432 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8318 - accuracy: 0.0000e+00 - val_loss: 0.6433 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/DHR_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6433 - accuracy: 0.0000e+00\n",
      "[0.6433020234107971, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_38 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2867 - accuracy: 0.0000e+00 - val_loss: 0.3567 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2637 - accuracy: 0.0000e+00 - val_loss: 0.3593 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2414 - accuracy: 0.0000e+00 - val_loss: 0.3637 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2227 - accuracy: 0.0000e+00 - val_loss: 0.3695 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2030 - accuracy: 0.0000e+00 - val_loss: 0.3774 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1843 - accuracy: 0.0000e+00 - val_loss: 0.3875 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1710 - accuracy: 0.0000e+00 - val_loss: 0.3993 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1530 - accuracy: 0.0000e+00 - val_loss: 0.4098 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1399 - accuracy: 0.0000e+00 - val_loss: 0.4209 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1288 - accuracy: 0.0000e+00 - val_loss: 0.4314 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1170 - accuracy: 0.0000e+00 - val_loss: 0.4405 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1063 - accuracy: 0.0000e+00 - val_loss: 0.4487 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0943 - accuracy: 0.0000e+00 - val_loss: 0.4543 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0841 - accuracy: 0.0000e+00 - val_loss: 0.4598 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0773 - accuracy: 0.0000e+00 - val_loss: 0.4647 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0687 - accuracy: 0.0000e+00 - val_loss: 0.4689 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0615 - accuracy: 0.0000e+00 - val_loss: 0.4722 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0545 - accuracy: 0.0000e+00 - val_loss: 0.4754 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0467 - accuracy: 0.0000e+00 - val_loss: 0.4782 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0430 - accuracy: 0.0000e+00 - val_loss: 0.4804 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0361 - accuracy: 0.0000e+00 - val_loss: 0.4825 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0316 - accuracy: 0.0000e+00 - val_loss: 0.4850 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0267 - accuracy: 0.0000e+00 - val_loss: 0.4869 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0228 - accuracy: 0.0000e+00 - val_loss: 0.4889 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0183 - accuracy: 0.0000e+00 - val_loss: 0.4903 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0148 - accuracy: 0.0000e+00 - val_loss: 0.4912 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0112 - accuracy: 0.0000e+00 - val_loss: 0.4925 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0076 - accuracy: 0.0000e+00 - val_loss: 0.4936 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0052 - accuracy: 0.0000e+00 - val_loss: 0.4949 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0025 - accuracy: 0.0000e+00 - val_loss: 0.4959 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9991 - accuracy: 0.0000e+00 - val_loss: 0.4966 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9969 - accuracy: 0.0000e+00 - val_loss: 0.4979 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9946 - accuracy: 0.0000e+00 - val_loss: 0.4983 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9923 - accuracy: 0.0000e+00 - val_loss: 0.4991 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9905 - accuracy: 0.0000e+00 - val_loss: 0.5002 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9881 - accuracy: 0.0000e+00 - val_loss: 0.5007 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9866 - accuracy: 0.0000e+00 - val_loss: 0.5012 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9852 - accuracy: 0.0000e+00 - val_loss: 0.5020 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9829 - accuracy: 0.0000e+00 - val_loss: 0.5024 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9813 - accuracy: 0.0000e+00 - val_loss: 0.5033 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9802 - accuracy: 0.0000e+00 - val_loss: 0.5035 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9786 - accuracy: 0.0000e+00 - val_loss: 0.5037 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9773 - accuracy: 0.0000e+00 - val_loss: 0.5045 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9758 - accuracy: 0.0000e+00 - val_loss: 0.5049 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9744 - accuracy: 0.0000e+00 - val_loss: 0.5053 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9732 - accuracy: 0.0000e+00 - val_loss: 0.5058 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9722 - accuracy: 0.0000e+00 - val_loss: 0.5066 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9709 - accuracy: 0.0000e+00 - val_loss: 0.5071 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9703 - accuracy: 0.0000e+00 - val_loss: 0.5077 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9688 - accuracy: 0.0000e+00 - val_loss: 0.5081 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9678 - accuracy: 0.0000e+00 - val_loss: 0.5084 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9668 - accuracy: 0.0000e+00 - val_loss: 0.5086 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9660 - accuracy: 0.0000e+00 - val_loss: 0.5095 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9647 - accuracy: 0.0000e+00 - val_loss: 0.5102 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9638 - accuracy: 0.0000e+00 - val_loss: 0.5105 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9628 - accuracy: 0.0000e+00 - val_loss: 0.5112 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9622 - accuracy: 0.0000e+00 - val_loss: 0.5111 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9613 - accuracy: 0.0000e+00 - val_loss: 0.5119 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9603 - accuracy: 0.0000e+00 - val_loss: 0.5122 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9596 - accuracy: 0.0000e+00 - val_loss: 0.5125 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9588 - accuracy: 0.0000e+00 - val_loss: 0.5126 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9579 - accuracy: 0.0000e+00 - val_loss: 0.5132 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9570 - accuracy: 0.0000e+00 - val_loss: 0.5140 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9564 - accuracy: 0.0000e+00 - val_loss: 0.5147 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9555 - accuracy: 0.0000e+00 - val_loss: 0.5149 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9547 - accuracy: 0.0000e+00 - val_loss: 0.5155 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9542 - accuracy: 0.0000e+00 - val_loss: 0.5153 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9534 - accuracy: 0.0000e+00 - val_loss: 0.5159 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9525 - accuracy: 0.0000e+00 - val_loss: 0.5160 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9521 - accuracy: 0.0000e+00 - val_loss: 0.5166 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9512 - accuracy: 0.0000e+00 - val_loss: 0.5175 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9505 - accuracy: 0.0000e+00 - val_loss: 0.5180 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9498 - accuracy: 0.0000e+00 - val_loss: 0.5187 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9493 - accuracy: 0.0000e+00 - val_loss: 0.5194 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9484 - accuracy: 0.0000e+00 - val_loss: 0.5195 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9478 - accuracy: 0.0000e+00 - val_loss: 0.5199 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9471 - accuracy: 0.0000e+00 - val_loss: 0.5203 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9465 - accuracy: 0.0000e+00 - val_loss: 0.5210 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9456 - accuracy: 0.0000e+00 - val_loss: 0.5210 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9449 - accuracy: 0.0000e+00 - val_loss: 0.5216 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9442 - accuracy: 0.0000e+00 - val_loss: 0.5220 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9436 - accuracy: 0.0000e+00 - val_loss: 0.5226 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9427 - accuracy: 0.0000e+00 - val_loss: 0.5235 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9421 - accuracy: 0.0000e+00 - val_loss: 0.5237 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9414 - accuracy: 0.0000e+00 - val_loss: 0.5243 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9407 - accuracy: 0.0000e+00 - val_loss: 0.5242 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.5258 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9395 - accuracy: 0.0000e+00 - val_loss: 0.5261 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9386 - accuracy: 0.0000e+00 - val_loss: 0.5256 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9379 - accuracy: 0.0000e+00 - val_loss: 0.5260 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9373 - accuracy: 0.0000e+00 - val_loss: 0.5255 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9366 - accuracy: 0.0000e+00 - val_loss: 0.5264 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9359 - accuracy: 0.0000e+00 - val_loss: 0.5264 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9354 - accuracy: 0.0000e+00 - val_loss: 0.5267 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9349 - accuracy: 0.0000e+00 - val_loss: 0.5262 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9343 - accuracy: 0.0000e+00 - val_loss: 0.5266 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9338 - accuracy: 0.0000e+00 - val_loss: 0.5259 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9332 - accuracy: 0.0000e+00 - val_loss: 0.5262 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.5267 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9321 - accuracy: 0.0000e+00 - val_loss: 0.5275 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9317 - accuracy: 0.0000e+00 - val_loss: 0.5268 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9310 - accuracy: 0.0000e+00 - val_loss: 0.5270 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9304 - accuracy: 0.0000e+00 - val_loss: 0.5269 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9298 - accuracy: 0.0000e+00 - val_loss: 0.5271 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.5275 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9289 - accuracy: 0.0000e+00 - val_loss: 0.5273 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9282 - accuracy: 0.0000e+00 - val_loss: 0.5279 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9278 - accuracy: 0.0000e+00 - val_loss: 0.5286 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9272 - accuracy: 0.0000e+00 - val_loss: 0.5289 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9265 - accuracy: 0.0000e+00 - val_loss: 0.5286 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9259 - accuracy: 0.0000e+00 - val_loss: 0.5290 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9254 - accuracy: 0.0000e+00 - val_loss: 0.5289 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9249 - accuracy: 0.0000e+00 - val_loss: 0.5287 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9242 - accuracy: 0.0000e+00 - val_loss: 0.5289 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9232 - accuracy: 0.0000e+00 - val_loss: 0.5292 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9227 - accuracy: 0.0000e+00 - val_loss: 0.5298 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9220 - accuracy: 0.0000e+00 - val_loss: 0.5299 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9215 - accuracy: 0.0000e+00 - val_loss: 0.5302 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9209 - accuracy: 0.0000e+00 - val_loss: 0.5297 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9204 - accuracy: 0.0000e+00 - val_loss: 0.5304 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9196 - accuracy: 0.0000e+00 - val_loss: 0.5303 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9193 - accuracy: 0.0000e+00 - val_loss: 0.5312 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9185 - accuracy: 0.0000e+00 - val_loss: 0.5307 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9179 - accuracy: 0.0000e+00 - val_loss: 0.5305 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9173 - accuracy: 0.0000e+00 - val_loss: 0.5310 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9168 - accuracy: 0.0000e+00 - val_loss: 0.5315 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9162 - accuracy: 0.0000e+00 - val_loss: 0.5312 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9155 - accuracy: 0.0000e+00 - val_loss: 0.5305 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9148 - accuracy: 0.0000e+00 - val_loss: 0.5306 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9141 - accuracy: 0.0000e+00 - val_loss: 0.5308 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9138 - accuracy: 0.0000e+00 - val_loss: 0.5309 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9132 - accuracy: 0.0000e+00 - val_loss: 0.5306 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9125 - accuracy: 0.0000e+00 - val_loss: 0.5312 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9118 - accuracy: 0.0000e+00 - val_loss: 0.5311 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9111 - accuracy: 0.0000e+00 - val_loss: 0.5314 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9105 - accuracy: 0.0000e+00 - val_loss: 0.5323 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9100 - accuracy: 0.0000e+00 - val_loss: 0.5323 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9091 - accuracy: 0.0000e+00 - val_loss: 0.5315 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9085 - accuracy: 0.0000e+00 - val_loss: 0.5309 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9079 - accuracy: 0.0000e+00 - val_loss: 0.5320 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9074 - accuracy: 0.0000e+00 - val_loss: 0.5331 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9067 - accuracy: 0.0000e+00 - val_loss: 0.5320 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9061 - accuracy: 0.0000e+00 - val_loss: 0.5317 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9056 - accuracy: 0.0000e+00 - val_loss: 0.5320 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9048 - accuracy: 0.0000e+00 - val_loss: 0.5324 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9042 - accuracy: 0.0000e+00 - val_loss: 0.5327 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9034 - accuracy: 0.0000e+00 - val_loss: 0.5326 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9030 - accuracy: 0.0000e+00 - val_loss: 0.5334 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9024 - accuracy: 0.0000e+00 - val_loss: 0.5334 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9016 - accuracy: 0.0000e+00 - val_loss: 0.5336 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9011 - accuracy: 0.0000e+00 - val_loss: 0.5331 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9004 - accuracy: 0.0000e+00 - val_loss: 0.5333 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8999 - accuracy: 0.0000e+00 - val_loss: 0.5342 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8992 - accuracy: 0.0000e+00 - val_loss: 0.5344 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8988 - accuracy: 0.0000e+00 - val_loss: 0.5340 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8984 - accuracy: 0.0000e+00 - val_loss: 0.5344 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8976 - accuracy: 0.0000e+00 - val_loss: 0.5344 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8969 - accuracy: 0.0000e+00 - val_loss: 0.5336 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8964 - accuracy: 0.0000e+00 - val_loss: 0.5339 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8958 - accuracy: 0.0000e+00 - val_loss: 0.5335 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8953 - accuracy: 0.0000e+00 - val_loss: 0.5333 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8946 - accuracy: 0.0000e+00 - val_loss: 0.5341 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8942 - accuracy: 0.0000e+00 - val_loss: 0.5340 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8934 - accuracy: 0.0000e+00 - val_loss: 0.5332 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8930 - accuracy: 0.0000e+00 - val_loss: 0.5348 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8925 - accuracy: 0.0000e+00 - val_loss: 0.5346 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8919 - accuracy: 0.0000e+00 - val_loss: 0.5331 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.5340 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8909 - accuracy: 0.0000e+00 - val_loss: 0.5348 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8905 - accuracy: 0.0000e+00 - val_loss: 0.5345 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8899 - accuracy: 0.0000e+00 - val_loss: 0.5340 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8892 - accuracy: 0.0000e+00 - val_loss: 0.5350 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8890 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.5334 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.5334 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8874 - accuracy: 0.0000e+00 - val_loss: 0.5335 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8870 - accuracy: 0.0000e+00 - val_loss: 0.5331 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8864 - accuracy: 0.0000e+00 - val_loss: 0.5344 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.5341 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8855 - accuracy: 0.0000e+00 - val_loss: 0.5342 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.5343 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8844 - accuracy: 0.0000e+00 - val_loss: 0.5343 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.5337 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8837 - accuracy: 0.0000e+00 - val_loss: 0.5332 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8833 - accuracy: 0.0000e+00 - val_loss: 0.5331 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8828 - accuracy: 0.0000e+00 - val_loss: 0.5343 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8824 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8819 - accuracy: 0.0000e+00 - val_loss: 0.5327 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8815 - accuracy: 0.0000e+00 - val_loss: 0.5336 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8811 - accuracy: 0.0000e+00 - val_loss: 0.5327 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8806 - accuracy: 0.0000e+00 - val_loss: 0.5322 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8802 - accuracy: 0.0000e+00 - val_loss: 0.5323 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8798 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8795 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8790 - accuracy: 0.0000e+00 - val_loss: 0.5323 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8787 - accuracy: 0.0000e+00 - val_loss: 0.5326 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8782 - accuracy: 0.0000e+00 - val_loss: 0.5327 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8779 - accuracy: 0.0000e+00 - val_loss: 0.5328 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8774 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8770 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8762 - accuracy: 0.0000e+00 - val_loss: 0.5327 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8759 - accuracy: 0.0000e+00 - val_loss: 0.5321 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8756 - accuracy: 0.0000e+00 - val_loss: 0.5319 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8752 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8749 - accuracy: 0.0000e+00 - val_loss: 0.5325 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8746 - accuracy: 0.0000e+00 - val_loss: 0.5314 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8741 - accuracy: 0.0000e+00 - val_loss: 0.5314 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8739 - accuracy: 0.0000e+00 - val_loss: 0.5309 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.5311 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.5314 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8730 - accuracy: 0.0000e+00 - val_loss: 0.5311 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.5305 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8722 - accuracy: 0.0000e+00 - val_loss: 0.5301 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8718 - accuracy: 0.0000e+00 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.5297 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.5308 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8708 - accuracy: 0.0000e+00 - val_loss: 0.5298 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.5298 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8702 - accuracy: 0.0000e+00 - val_loss: 0.5295 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8699 - accuracy: 0.0000e+00 - val_loss: 0.5286 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.5296 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8693 - accuracy: 0.0000e+00 - val_loss: 0.5297 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8690 - accuracy: 0.0000e+00 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8687 - accuracy: 0.0000e+00 - val_loss: 0.5281 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8685 - accuracy: 0.0000e+00 - val_loss: 0.5294 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8681 - accuracy: 0.0000e+00 - val_loss: 0.5295 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8678 - accuracy: 0.0000e+00 - val_loss: 0.5288 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8676 - accuracy: 0.0000e+00 - val_loss: 0.5286 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8673 - accuracy: 0.0000e+00 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8670 - accuracy: 0.0000e+00 - val_loss: 0.5292 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8667 - accuracy: 0.0000e+00 - val_loss: 0.5285 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8664 - accuracy: 0.0000e+00 - val_loss: 0.5279 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8662 - accuracy: 0.0000e+00 - val_loss: 0.5272 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8659 - accuracy: 0.0000e+00 - val_loss: 0.5292 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8657 - accuracy: 0.0000e+00 - val_loss: 0.5289 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.5281 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8651 - accuracy: 0.0000e+00 - val_loss: 0.5266 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8649 - accuracy: 0.0000e+00 - val_loss: 0.5259 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8646 - accuracy: 0.0000e+00 - val_loss: 0.5268 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8644 - accuracy: 0.0000e+00 - val_loss: 0.5277 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.5278 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8639 - accuracy: 0.0000e+00 - val_loss: 0.5277 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8637 - accuracy: 0.0000e+00 - val_loss: 0.5279 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8634 - accuracy: 0.0000e+00 - val_loss: 0.5267 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8632 - accuracy: 0.0000e+00 - val_loss: 0.5262 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8630 - accuracy: 0.0000e+00 - val_loss: 0.5264 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8627 - accuracy: 0.0000e+00 - val_loss: 0.5269 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8625 - accuracy: 0.0000e+00 - val_loss: 0.5267 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8623 - accuracy: 0.0000e+00 - val_loss: 0.5263 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8621 - accuracy: 0.0000e+00 - val_loss: 0.5259 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.5260 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8617 - accuracy: 0.0000e+00 - val_loss: 0.5266 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8614 - accuracy: 0.0000e+00 - val_loss: 0.5254 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8612 - accuracy: 0.0000e+00 - val_loss: 0.5252 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/BMY_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5252 - accuracy: 0.0000e+00\n",
      "[0.5251549482345581, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_40 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.2410 - accuracy: 0.0000e+00 - val_loss: 0.4997 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2104 - accuracy: 0.0000e+00 - val_loss: 0.5039 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1889 - accuracy: 0.0000e+00 - val_loss: 0.5078 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1728 - accuracy: 0.0000e+00 - val_loss: 0.5117 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1517 - accuracy: 0.0000e+00 - val_loss: 0.5161 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1358 - accuracy: 0.0000e+00 - val_loss: 0.5195 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1185 - accuracy: 0.0000e+00 - val_loss: 0.5231 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1033 - accuracy: 0.0000e+00 - val_loss: 0.5267 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0902 - accuracy: 0.0000e+00 - val_loss: 0.5300 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0777 - accuracy: 0.0000e+00 - val_loss: 0.5333 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0684 - accuracy: 0.0000e+00 - val_loss: 0.5359 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0595 - accuracy: 0.0000e+00 - val_loss: 0.5389 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0501 - accuracy: 0.0000e+00 - val_loss: 0.5412 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0421 - accuracy: 0.0000e+00 - val_loss: 0.5431 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0356 - accuracy: 0.0000e+00 - val_loss: 0.5451 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0285 - accuracy: 0.0000e+00 - val_loss: 0.5465 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0234 - accuracy: 0.0000e+00 - val_loss: 0.5483 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0182 - accuracy: 0.0000e+00 - val_loss: 0.5498 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0129 - accuracy: 0.0000e+00 - val_loss: 0.5511 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0099 - accuracy: 0.0000e+00 - val_loss: 0.5522 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0055 - accuracy: 0.0000e+00 - val_loss: 0.5533 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0018 - accuracy: 0.0000e+00 - val_loss: 0.5545 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9985 - accuracy: 0.0000e+00 - val_loss: 0.5552 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9955 - accuracy: 0.0000e+00 - val_loss: 0.5561 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9928 - accuracy: 0.0000e+00 - val_loss: 0.5565 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9896 - accuracy: 0.0000e+00 - val_loss: 0.5576 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9872 - accuracy: 0.0000e+00 - val_loss: 0.5581 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9841 - accuracy: 0.0000e+00 - val_loss: 0.5585 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9819 - accuracy: 0.0000e+00 - val_loss: 0.5589 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9797 - accuracy: 0.0000e+00 - val_loss: 0.5593 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9779 - accuracy: 0.0000e+00 - val_loss: 0.5598 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9758 - accuracy: 0.0000e+00 - val_loss: 0.5602 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9739 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9721 - accuracy: 0.0000e+00 - val_loss: 0.5607 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9706 - accuracy: 0.0000e+00 - val_loss: 0.5609 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9687 - accuracy: 0.0000e+00 - val_loss: 0.5609 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9674 - accuracy: 0.0000e+00 - val_loss: 0.5607 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9662 - accuracy: 0.0000e+00 - val_loss: 0.5611 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9652 - accuracy: 0.0000e+00 - val_loss: 0.5605 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9631 - accuracy: 0.0000e+00 - val_loss: 0.5608 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9620 - accuracy: 0.0000e+00 - val_loss: 0.5612 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9608 - accuracy: 0.0000e+00 - val_loss: 0.5609 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9597 - accuracy: 0.0000e+00 - val_loss: 0.5608 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9585 - accuracy: 0.0000e+00 - val_loss: 0.5610 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9574 - accuracy: 0.0000e+00 - val_loss: 0.5611 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9563 - accuracy: 0.0000e+00 - val_loss: 0.5614 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9553 - accuracy: 0.0000e+00 - val_loss: 0.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9542 - accuracy: 0.0000e+00 - val_loss: 0.5616 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9532 - accuracy: 0.0000e+00 - val_loss: 0.5613 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9522 - accuracy: 0.0000e+00 - val_loss: 0.5614 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9512 - accuracy: 0.0000e+00 - val_loss: 0.5616 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9503 - accuracy: 0.0000e+00 - val_loss: 0.5617 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9494 - accuracy: 0.0000e+00 - val_loss: 0.5620 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9484 - accuracy: 0.0000e+00 - val_loss: 0.5617 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9476 - accuracy: 0.0000e+00 - val_loss: 0.5617 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9466 - accuracy: 0.0000e+00 - val_loss: 0.5618 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9457 - accuracy: 0.0000e+00 - val_loss: 0.5619 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.5620 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9440 - accuracy: 0.0000e+00 - val_loss: 0.5618 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9432 - accuracy: 0.0000e+00 - val_loss: 0.5618 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9427 - accuracy: 0.0000e+00 - val_loss: 0.5627 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9416 - accuracy: 0.0000e+00 - val_loss: 0.5623 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9409 - accuracy: 0.0000e+00 - val_loss: 0.5630 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.5632 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9391 - accuracy: 0.0000e+00 - val_loss: 0.5638 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9382 - accuracy: 0.0000e+00 - val_loss: 0.5640 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9374 - accuracy: 0.0000e+00 - val_loss: 0.5643 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9367 - accuracy: 0.0000e+00 - val_loss: 0.5638 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9361 - accuracy: 0.0000e+00 - val_loss: 0.5652 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9351 - accuracy: 0.0000e+00 - val_loss: 0.5651 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9343 - accuracy: 0.0000e+00 - val_loss: 0.5658 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9337 - accuracy: 0.0000e+00 - val_loss: 0.5658 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9329 - accuracy: 0.0000e+00 - val_loss: 0.5662 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9321 - accuracy: 0.0000e+00 - val_loss: 0.5667 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9315 - accuracy: 0.0000e+00 - val_loss: 0.5673 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9309 - accuracy: 0.0000e+00 - val_loss: 0.5686 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9301 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9285 - accuracy: 0.0000e+00 - val_loss: 0.5696 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9280 - accuracy: 0.0000e+00 - val_loss: 0.5704 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9271 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9265 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9259 - accuracy: 0.0000e+00 - val_loss: 0.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9251 - accuracy: 0.0000e+00 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9246 - accuracy: 0.0000e+00 - val_loss: 0.5724 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9241 - accuracy: 0.0000e+00 - val_loss: 0.5736 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9234 - accuracy: 0.0000e+00 - val_loss: 0.5737 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9225 - accuracy: 0.0000e+00 - val_loss: 0.5739 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9219 - accuracy: 0.0000e+00 - val_loss: 0.5743 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9215 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9208 - accuracy: 0.0000e+00 - val_loss: 0.5761 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9200 - accuracy: 0.0000e+00 - val_loss: 0.5761 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9195 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9187 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9181 - accuracy: 0.0000e+00 - val_loss: 0.5786 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9176 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9171 - accuracy: 0.0000e+00 - val_loss: 0.5795 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9165 - accuracy: 0.0000e+00 - val_loss: 0.5807 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9157 - accuracy: 0.0000e+00 - val_loss: 0.5813 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9152 - accuracy: 0.0000e+00 - val_loss: 0.5807 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9146 - accuracy: 0.0000e+00 - val_loss: 0.5809 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9139 - accuracy: 0.0000e+00 - val_loss: 0.5815 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9133 - accuracy: 0.0000e+00 - val_loss: 0.5820 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9127 - accuracy: 0.0000e+00 - val_loss: 0.5828 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9122 - accuracy: 0.0000e+00 - val_loss: 0.5838 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9118 - accuracy: 0.0000e+00 - val_loss: 0.5832 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9111 - accuracy: 0.0000e+00 - val_loss: 0.5850 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9107 - accuracy: 0.0000e+00 - val_loss: 0.5861 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9099 - accuracy: 0.0000e+00 - val_loss: 0.5865 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9093 - accuracy: 0.0000e+00 - val_loss: 0.5863 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9088 - accuracy: 0.0000e+00 - val_loss: 0.5863 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9084 - accuracy: 0.0000e+00 - val_loss: 0.5865 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9078 - accuracy: 0.0000e+00 - val_loss: 0.5868 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9072 - accuracy: 0.0000e+00 - val_loss: 0.5868 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9068 - accuracy: 0.0000e+00 - val_loss: 0.5884 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9064 - accuracy: 0.0000e+00 - val_loss: 0.5883 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9060 - accuracy: 0.0000e+00 - val_loss: 0.5899 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9054 - accuracy: 0.0000e+00 - val_loss: 0.5895 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9047 - accuracy: 0.0000e+00 - val_loss: 0.5901 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9044 - accuracy: 0.0000e+00 - val_loss: 0.5899 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9038 - accuracy: 0.0000e+00 - val_loss: 0.5906 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9034 - accuracy: 0.0000e+00 - val_loss: 0.5909 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.5909 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9024 - accuracy: 0.0000e+00 - val_loss: 0.5921 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9019 - accuracy: 0.0000e+00 - val_loss: 0.5920 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9017 - accuracy: 0.0000e+00 - val_loss: 0.5920 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9014 - accuracy: 0.0000e+00 - val_loss: 0.5935 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.5940 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9003 - accuracy: 0.0000e+00 - val_loss: 0.5935 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8997 - accuracy: 0.0000e+00 - val_loss: 0.5937 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8993 - accuracy: 0.0000e+00 - val_loss: 0.5937 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8992 - accuracy: 0.0000e+00 - val_loss: 0.5952 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8986 - accuracy: 0.0000e+00 - val_loss: 0.5948 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8980 - accuracy: 0.0000e+00 - val_loss: 0.5949 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8977 - accuracy: 0.0000e+00 - val_loss: 0.5957 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8973 - accuracy: 0.0000e+00 - val_loss: 0.5954 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8969 - accuracy: 0.0000e+00 - val_loss: 0.5954 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8967 - accuracy: 0.0000e+00 - val_loss: 0.5964 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8960 - accuracy: 0.0000e+00 - val_loss: 0.5960 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8956 - accuracy: 0.0000e+00 - val_loss: 0.5964 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8950 - accuracy: 0.0000e+00 - val_loss: 0.5963 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8947 - accuracy: 0.0000e+00 - val_loss: 0.5963 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8943 - accuracy: 0.0000e+00 - val_loss: 0.5958 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8940 - accuracy: 0.0000e+00 - val_loss: 0.5968 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8936 - accuracy: 0.0000e+00 - val_loss: 0.5971 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8931 - accuracy: 0.0000e+00 - val_loss: 0.5970 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8928 - accuracy: 0.0000e+00 - val_loss: 0.5973 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8924 - accuracy: 0.0000e+00 - val_loss: 0.5972 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8924 - accuracy: 0.0000e+00 - val_loss: 0.5977 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8919 - accuracy: 0.0000e+00 - val_loss: 0.5970 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8914 - accuracy: 0.0000e+00 - val_loss: 0.5972 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8912 - accuracy: 0.0000e+00 - val_loss: 0.5969 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8909 - accuracy: 0.0000e+00 - val_loss: 0.5976 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8904 - accuracy: 0.0000e+00 - val_loss: 0.5977 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8901 - accuracy: 0.0000e+00 - val_loss: 0.5966 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8897 - accuracy: 0.0000e+00 - val_loss: 0.5964 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8894 - accuracy: 0.0000e+00 - val_loss: 0.5962 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8891 - accuracy: 0.0000e+00 - val_loss: 0.5966 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.5965 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8884 - accuracy: 0.0000e+00 - val_loss: 0.5967 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.5969 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.5968 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8877 - accuracy: 0.0000e+00 - val_loss: 0.5963 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8872 - accuracy: 0.0000e+00 - val_loss: 0.5970 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8869 - accuracy: 0.0000e+00 - val_loss: 0.5971 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 0.5969 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8865 - accuracy: 0.0000e+00 - val_loss: 0.5971 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.5968 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.5972 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.5975 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8854 - accuracy: 0.0000e+00 - val_loss: 0.5974 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.5978 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.5979 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8847 - accuracy: 0.0000e+00 - val_loss: 0.5981 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8845 - accuracy: 0.0000e+00 - val_loss: 0.5981 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8842 - accuracy: 0.0000e+00 - val_loss: 0.5982 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8839 - accuracy: 0.0000e+00 - val_loss: 0.5982 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8838 - accuracy: 0.0000e+00 - val_loss: 0.5981 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8836 - accuracy: 0.0000e+00 - val_loss: 0.5986 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8833 - accuracy: 0.0000e+00 - val_loss: 0.5988 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8831 - accuracy: 0.0000e+00 - val_loss: 0.5988 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8829 - accuracy: 0.0000e+00 - val_loss: 0.5988 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8827 - accuracy: 0.0000e+00 - val_loss: 0.5991 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8825 - accuracy: 0.0000e+00 - val_loss: 0.5994 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8823 - accuracy: 0.0000e+00 - val_loss: 0.5994 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8822 - accuracy: 0.0000e+00 - val_loss: 0.5997 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8821 - accuracy: 0.0000e+00 - val_loss: 0.5996 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8817 - accuracy: 0.0000e+00 - val_loss: 0.5996 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8815 - accuracy: 0.0000e+00 - val_loss: 0.5997 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8813 - accuracy: 0.0000e+00 - val_loss: 0.6002 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8812 - accuracy: 0.0000e+00 - val_loss: 0.6003 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8810 - accuracy: 0.0000e+00 - val_loss: 0.6006 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8808 - accuracy: 0.0000e+00 - val_loss: 0.6009 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8807 - accuracy: 0.0000e+00 - val_loss: 0.6007 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8805 - accuracy: 0.0000e+00 - val_loss: 0.6011 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8803 - accuracy: 0.0000e+00 - val_loss: 0.6014 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8801 - accuracy: 0.0000e+00 - val_loss: 0.6013 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8800 - accuracy: 0.0000e+00 - val_loss: 0.6013 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.6014 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8796 - accuracy: 0.0000e+00 - val_loss: 0.6014 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8795 - accuracy: 0.0000e+00 - val_loss: 0.6017 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8793 - accuracy: 0.0000e+00 - val_loss: 0.6016 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8792 - accuracy: 0.0000e+00 - val_loss: 0.6018 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8790 - accuracy: 0.0000e+00 - val_loss: 0.6020 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.6021 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8787 - accuracy: 0.0000e+00 - val_loss: 0.6019 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8785 - accuracy: 0.0000e+00 - val_loss: 0.6019 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8784 - accuracy: 0.0000e+00 - val_loss: 0.6020 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8783 - accuracy: 0.0000e+00 - val_loss: 0.6021 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8782 - accuracy: 0.0000e+00 - val_loss: 0.6021 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.6023 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8779 - accuracy: 0.0000e+00 - val_loss: 0.6020 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8778 - accuracy: 0.0000e+00 - val_loss: 0.6023 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.6022 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8775 - accuracy: 0.0000e+00 - val_loss: 0.6024 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8774 - accuracy: 0.0000e+00 - val_loss: 0.6028 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8773 - accuracy: 0.0000e+00 - val_loss: 0.6026 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8770 - accuracy: 0.0000e+00 - val_loss: 0.6026 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8770 - accuracy: 0.0000e+00 - val_loss: 0.6029 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8769 - accuracy: 0.0000e+00 - val_loss: 0.6027 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.6027 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.6030 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8764 - accuracy: 0.0000e+00 - val_loss: 0.6030 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8763 - accuracy: 0.0000e+00 - val_loss: 0.6029 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8762 - accuracy: 0.0000e+00 - val_loss: 0.6031 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8761 - accuracy: 0.0000e+00 - val_loss: 0.6029 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8759 - accuracy: 0.0000e+00 - val_loss: 0.6027 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8759 - accuracy: 0.0000e+00 - val_loss: 0.6030 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8757 - accuracy: 0.0000e+00 - val_loss: 0.6031 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8756 - accuracy: 0.0000e+00 - val_loss: 0.6032 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8755 - accuracy: 0.0000e+00 - val_loss: 0.6034 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.6034 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8752 - accuracy: 0.0000e+00 - val_loss: 0.6035 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8752 - accuracy: 0.0000e+00 - val_loss: 0.6035 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8751 - accuracy: 0.0000e+00 - val_loss: 0.6038 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.6039 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8748 - accuracy: 0.0000e+00 - val_loss: 0.6038 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8748 - accuracy: 0.0000e+00 - val_loss: 0.6037 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.6038 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.6039 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.6040 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8744 - accuracy: 0.0000e+00 - val_loss: 0.6040 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8743 - accuracy: 0.0000e+00 - val_loss: 0.6041 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.6042 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8741 - accuracy: 0.0000e+00 - val_loss: 0.6041 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8740 - accuracy: 0.0000e+00 - val_loss: 0.6043 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8738 - accuracy: 0.0000e+00 - val_loss: 0.6043 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.6044 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.6044 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.6044 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8735 - accuracy: 0.0000e+00 - val_loss: 0.6047 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.6047 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.6047 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8732 - accuracy: 0.0000e+00 - val_loss: 0.6048 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.6047 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.6047 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/VZ_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6047 - accuracy: 0.0000e+00\n",
      "[0.6046835780143738, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.3127 - accuracy: 0.0000e+00 - val_loss: 0.4708 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2829 - accuracy: 0.0000e+00 - val_loss: 0.4736 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2529 - accuracy: 0.0000e+00 - val_loss: 0.4754 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2223 - accuracy: 0.0000e+00 - val_loss: 0.4773 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2018 - accuracy: 0.0000e+00 - val_loss: 0.4802 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1747 - accuracy: 0.0000e+00 - val_loss: 0.4826 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1519 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1311 - accuracy: 0.0000e+00 - val_loss: 0.4881 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1119 - accuracy: 0.0000e+00 - val_loss: 0.4905 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0948 - accuracy: 0.0000e+00 - val_loss: 0.4929 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0827 - accuracy: 0.0000e+00 - val_loss: 0.4960 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0662 - accuracy: 0.0000e+00 - val_loss: 0.4982 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0546 - accuracy: 0.0000e+00 - val_loss: 0.4999 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0448 - accuracy: 0.0000e+00 - val_loss: 0.5023 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0373 - accuracy: 0.0000e+00 - val_loss: 0.5046 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0268 - accuracy: 0.0000e+00 - val_loss: 0.5066 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0211 - accuracy: 0.0000e+00 - val_loss: 0.5090 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0137 - accuracy: 0.0000e+00 - val_loss: 0.5109 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0077 - accuracy: 0.0000e+00 - val_loss: 0.5123 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0020 - accuracy: 0.0000e+00 - val_loss: 0.5147 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9963 - accuracy: 0.0000e+00 - val_loss: 0.5166 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9914 - accuracy: 0.0000e+00 - val_loss: 0.5179 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9870 - accuracy: 0.0000e+00 - val_loss: 0.5196 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9815 - accuracy: 0.0000e+00 - val_loss: 0.5209 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9777 - accuracy: 0.0000e+00 - val_loss: 0.5223 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9730 - accuracy: 0.0000e+00 - val_loss: 0.5236 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9692 - accuracy: 0.0000e+00 - val_loss: 0.5251 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9655 - accuracy: 0.0000e+00 - val_loss: 0.5266 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9619 - accuracy: 0.0000e+00 - val_loss: 0.5285 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9584 - accuracy: 0.0000e+00 - val_loss: 0.5300 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9555 - accuracy: 0.0000e+00 - val_loss: 0.5314 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9527 - accuracy: 0.0000e+00 - val_loss: 0.5328 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9498 - accuracy: 0.0000e+00 - val_loss: 0.5345 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9475 - accuracy: 0.0000e+00 - val_loss: 0.5361 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9451 - accuracy: 0.0000e+00 - val_loss: 0.5375 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9429 - accuracy: 0.0000e+00 - val_loss: 0.5387 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9407 - accuracy: 0.0000e+00 - val_loss: 0.5398 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9383 - accuracy: 0.0000e+00 - val_loss: 0.5411 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9364 - accuracy: 0.0000e+00 - val_loss: 0.5426 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9349 - accuracy: 0.0000e+00 - val_loss: 0.5434 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9328 - accuracy: 0.0000e+00 - val_loss: 0.5450 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9310 - accuracy: 0.0000e+00 - val_loss: 0.5469 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9295 - accuracy: 0.0000e+00 - val_loss: 0.5492 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.5505 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9259 - accuracy: 0.0000e+00 - val_loss: 0.5518 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.5528 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9231 - accuracy: 0.0000e+00 - val_loss: 0.5535 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9213 - accuracy: 0.0000e+00 - val_loss: 0.5553 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9200 - accuracy: 0.0000e+00 - val_loss: 0.5568 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9183 - accuracy: 0.0000e+00 - val_loss: 0.5579 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9171 - accuracy: 0.0000e+00 - val_loss: 0.5583 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9159 - accuracy: 0.0000e+00 - val_loss: 0.5589 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9145 - accuracy: 0.0000e+00 - val_loss: 0.5602 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9131 - accuracy: 0.0000e+00 - val_loss: 0.5619 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9119 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9105 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9095 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9078 - accuracy: 0.0000e+00 - val_loss: 0.5645 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9066 - accuracy: 0.0000e+00 - val_loss: 0.5662 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9054 - accuracy: 0.0000e+00 - val_loss: 0.5670 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9040 - accuracy: 0.0000e+00 - val_loss: 0.5678 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9030 - accuracy: 0.0000e+00 - val_loss: 0.5679 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9017 - accuracy: 0.0000e+00 - val_loss: 0.5685 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8993 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8983 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8970 - accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8957 - accuracy: 0.0000e+00 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8951 - accuracy: 0.0000e+00 - val_loss: 0.5716 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8937 - accuracy: 0.0000e+00 - val_loss: 0.5720 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8923 - accuracy: 0.0000e+00 - val_loss: 0.5733 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8914 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8900 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8889 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8879 - accuracy: 0.0000e+00 - val_loss: 0.5740 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8870 - accuracy: 0.0000e+00 - val_loss: 0.5737 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8856 - accuracy: 0.0000e+00 - val_loss: 0.5739 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8845 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8836 - accuracy: 0.0000e+00 - val_loss: 0.5760 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8823 - accuracy: 0.0000e+00 - val_loss: 0.5765 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - val_loss: 0.5756 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8800 - accuracy: 0.0000e+00 - val_loss: 0.5764 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8791 - accuracy: 0.0000e+00 - val_loss: 0.5762 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8783 - accuracy: 0.0000e+00 - val_loss: 0.5767 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8771 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8759 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8751 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8741 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.5795 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.5799 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8706 - accuracy: 0.0000e+00 - val_loss: 0.5798 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.5809 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8687 - accuracy: 0.0000e+00 - val_loss: 0.5810 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8678 - accuracy: 0.0000e+00 - val_loss: 0.5816 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8669 - accuracy: 0.0000e+00 - val_loss: 0.5834 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8659 - accuracy: 0.0000e+00 - val_loss: 0.5839 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8652 - accuracy: 0.0000e+00 - val_loss: 0.5848 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.5847 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8636 - accuracy: 0.0000e+00 - val_loss: 0.5847 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8628 - accuracy: 0.0000e+00 - val_loss: 0.5844 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.5856 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8611 - accuracy: 0.0000e+00 - val_loss: 0.5877 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8604 - accuracy: 0.0000e+00 - val_loss: 0.5875 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.5876 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8587 - accuracy: 0.0000e+00 - val_loss: 0.5882 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8579 - accuracy: 0.0000e+00 - val_loss: 0.5898 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8574 - accuracy: 0.0000e+00 - val_loss: 0.5894 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8567 - accuracy: 0.0000e+00 - val_loss: 0.5899 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8558 - accuracy: 0.0000e+00 - val_loss: 0.5917 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8552 - accuracy: 0.0000e+00 - val_loss: 0.5918 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8545 - accuracy: 0.0000e+00 - val_loss: 0.5932 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.5923 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.5915 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8525 - accuracy: 0.0000e+00 - val_loss: 0.5935 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8522 - accuracy: 0.0000e+00 - val_loss: 0.5960 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8512 - accuracy: 0.0000e+00 - val_loss: 0.5951 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - val_loss: 0.5953 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8500 - accuracy: 0.0000e+00 - val_loss: 0.5963 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8495 - accuracy: 0.0000e+00 - val_loss: 0.5968 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.5984 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8483 - accuracy: 0.0000e+00 - val_loss: 0.5985 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8479 - accuracy: 0.0000e+00 - val_loss: 0.5985 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8473 - accuracy: 0.0000e+00 - val_loss: 0.5992 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8467 - accuracy: 0.0000e+00 - val_loss: 0.5995 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8463 - accuracy: 0.0000e+00 - val_loss: 0.5997 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8458 - accuracy: 0.0000e+00 - val_loss: 0.6017 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8452 - accuracy: 0.0000e+00 - val_loss: 0.6016 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8446 - accuracy: 0.0000e+00 - val_loss: 0.6022 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8441 - accuracy: 0.0000e+00 - val_loss: 0.6029 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8437 - accuracy: 0.0000e+00 - val_loss: 0.6043 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8432 - accuracy: 0.0000e+00 - val_loss: 0.6054 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8427 - accuracy: 0.0000e+00 - val_loss: 0.6062 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8423 - accuracy: 0.0000e+00 - val_loss: 0.6064 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8418 - accuracy: 0.0000e+00 - val_loss: 0.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8414 - accuracy: 0.0000e+00 - val_loss: 0.6075 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8409 - accuracy: 0.0000e+00 - val_loss: 0.6073 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8406 - accuracy: 0.0000e+00 - val_loss: 0.6087 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8400 - accuracy: 0.0000e+00 - val_loss: 0.6085 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8397 - accuracy: 0.0000e+00 - val_loss: 0.6084 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8393 - accuracy: 0.0000e+00 - val_loss: 0.6084 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8389 - accuracy: 0.0000e+00 - val_loss: 0.6103 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8384 - accuracy: 0.0000e+00 - val_loss: 0.6107 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8379 - accuracy: 0.0000e+00 - val_loss: 0.6108 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8374 - accuracy: 0.0000e+00 - val_loss: 0.6118 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8371 - accuracy: 0.0000e+00 - val_loss: 0.6118 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8366 - accuracy: 0.0000e+00 - val_loss: 0.6129 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8363 - accuracy: 0.0000e+00 - val_loss: 0.6134 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8360 - accuracy: 0.0000e+00 - val_loss: 0.6135 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8356 - accuracy: 0.0000e+00 - val_loss: 0.6149 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8353 - accuracy: 0.0000e+00 - val_loss: 0.6166 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8348 - accuracy: 0.0000e+00 - val_loss: 0.6161 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8345 - accuracy: 0.0000e+00 - val_loss: 0.6154 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8342 - accuracy: 0.0000e+00 - val_loss: 0.6166 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8337 - accuracy: 0.0000e+00 - val_loss: 0.6173 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8334 - accuracy: 0.0000e+00 - val_loss: 0.6165 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8330 - accuracy: 0.0000e+00 - val_loss: 0.6171 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8327 - accuracy: 0.0000e+00 - val_loss: 0.6187 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8324 - accuracy: 0.0000e+00 - val_loss: 0.6198 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8318 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8315 - accuracy: 0.0000e+00 - val_loss: 0.6188 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8311 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8308 - accuracy: 0.0000e+00 - val_loss: 0.6190 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8304 - accuracy: 0.0000e+00 - val_loss: 0.6195 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8300 - accuracy: 0.0000e+00 - val_loss: 0.6205 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8297 - accuracy: 0.0000e+00 - val_loss: 0.6210 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8294 - accuracy: 0.0000e+00 - val_loss: 0.6211 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8290 - accuracy: 0.0000e+00 - val_loss: 0.6219 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8286 - accuracy: 0.0000e+00 - val_loss: 0.6219 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8284 - accuracy: 0.0000e+00 - val_loss: 0.6219 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8281 - accuracy: 0.0000e+00 - val_loss: 0.6222 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8277 - accuracy: 0.0000e+00 - val_loss: 0.6223 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8275 - accuracy: 0.0000e+00 - val_loss: 0.6243 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8270 - accuracy: 0.0000e+00 - val_loss: 0.6246 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8269 - accuracy: 0.0000e+00 - val_loss: 0.6242 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8265 - accuracy: 0.0000e+00 - val_loss: 0.6246 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.6243 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8259 - accuracy: 0.0000e+00 - val_loss: 0.6251 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8257 - accuracy: 0.0000e+00 - val_loss: 0.6248 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8256 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8250 - accuracy: 0.0000e+00 - val_loss: 0.6250 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8249 - accuracy: 0.0000e+00 - val_loss: 0.6267 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8245 - accuracy: 0.0000e+00 - val_loss: 0.6260 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8242 - accuracy: 0.0000e+00 - val_loss: 0.6262 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8239 - accuracy: 0.0000e+00 - val_loss: 0.6260 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8236 - accuracy: 0.0000e+00 - val_loss: 0.6265 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8234 - accuracy: 0.0000e+00 - val_loss: 0.6275 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8232 - accuracy: 0.0000e+00 - val_loss: 0.6277 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8228 - accuracy: 0.0000e+00 - val_loss: 0.6275 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.8226 - accuracy: 0.0000e+00 - val_loss: 0.6276 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 0.6274 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8221 - accuracy: 0.0000e+00 - val_loss: 0.6289 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8218 - accuracy: 0.0000e+00 - val_loss: 0.6285 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8215 - accuracy: 0.0000e+00 - val_loss: 0.6292 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8213 - accuracy: 0.0000e+00 - val_loss: 0.6302 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8211 - accuracy: 0.0000e+00 - val_loss: 0.6306 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8208 - accuracy: 0.0000e+00 - val_loss: 0.6304 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8205 - accuracy: 0.0000e+00 - val_loss: 0.6309 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8203 - accuracy: 0.0000e+00 - val_loss: 0.6309 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8201 - accuracy: 0.0000e+00 - val_loss: 0.6310 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8199 - accuracy: 0.0000e+00 - val_loss: 0.6311 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8197 - accuracy: 0.0000e+00 - val_loss: 0.6314 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8194 - accuracy: 0.0000e+00 - val_loss: 0.6318 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8193 - accuracy: 0.0000e+00 - val_loss: 0.6331 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 0.6329 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8188 - accuracy: 0.0000e+00 - val_loss: 0.6331 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8186 - accuracy: 0.0000e+00 - val_loss: 0.6329 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8184 - accuracy: 0.0000e+00 - val_loss: 0.6325 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8182 - accuracy: 0.0000e+00 - val_loss: 0.6331 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8180 - accuracy: 0.0000e+00 - val_loss: 0.6325 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8177 - accuracy: 0.0000e+00 - val_loss: 0.6338 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8175 - accuracy: 0.0000e+00 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8174 - accuracy: 0.0000e+00 - val_loss: 0.6345 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8171 - accuracy: 0.0000e+00 - val_loss: 0.6351 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8170 - accuracy: 0.0000e+00 - val_loss: 0.6359 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8168 - accuracy: 0.0000e+00 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8165 - accuracy: 0.0000e+00 - val_loss: 0.6347 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8164 - accuracy: 0.0000e+00 - val_loss: 0.6349 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8161 - accuracy: 0.0000e+00 - val_loss: 0.6357 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8160 - accuracy: 0.0000e+00 - val_loss: 0.6355 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8158 - accuracy: 0.0000e+00 - val_loss: 0.6357 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8156 - accuracy: 0.0000e+00 - val_loss: 0.6359 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8155 - accuracy: 0.0000e+00 - val_loss: 0.6361 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8152 - accuracy: 0.0000e+00 - val_loss: 0.6369 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8151 - accuracy: 0.0000e+00 - val_loss: 0.6370 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8149 - accuracy: 0.0000e+00 - val_loss: 0.6367 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8147 - accuracy: 0.0000e+00 - val_loss: 0.6374 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8146 - accuracy: 0.0000e+00 - val_loss: 0.6382 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8144 - accuracy: 0.0000e+00 - val_loss: 0.6378 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8143 - accuracy: 0.0000e+00 - val_loss: 0.6379 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8142 - accuracy: 0.0000e+00 - val_loss: 0.6376 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8140 - accuracy: 0.0000e+00 - val_loss: 0.6388 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8138 - accuracy: 0.0000e+00 - val_loss: 0.6399 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8136 - accuracy: 0.0000e+00 - val_loss: 0.6400 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8135 - accuracy: 0.0000e+00 - val_loss: 0.6388 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8134 - accuracy: 0.0000e+00 - val_loss: 0.6394 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8133 - accuracy: 0.0000e+00 - val_loss: 0.6390 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8131 - accuracy: 0.0000e+00 - val_loss: 0.6394 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8129 - accuracy: 0.0000e+00 - val_loss: 0.6396 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8128 - accuracy: 0.0000e+00 - val_loss: 0.6404 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8126 - accuracy: 0.0000e+00 - val_loss: 0.6400 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8125 - accuracy: 0.0000e+00 - val_loss: 0.6406 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8124 - accuracy: 0.0000e+00 - val_loss: 0.6409 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8123 - accuracy: 0.0000e+00 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8122 - accuracy: 0.0000e+00 - val_loss: 0.6417 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8120 - accuracy: 0.0000e+00 - val_loss: 0.6406 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8119 - accuracy: 0.0000e+00 - val_loss: 0.6406 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8117 - accuracy: 0.0000e+00 - val_loss: 0.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8116 - accuracy: 0.0000e+00 - val_loss: 0.6421 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8115 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8114 - accuracy: 0.0000e+00 - val_loss: 0.6426 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8112 - accuracy: 0.0000e+00 - val_loss: 0.6421 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8111 - accuracy: 0.0000e+00 - val_loss: 0.6420 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8110 - accuracy: 0.0000e+00 - val_loss: 0.6423 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8109 - accuracy: 0.0000e+00 - val_loss: 0.6426 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8108 - accuracy: 0.0000e+00 - val_loss: 0.6420 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/TXN_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6420 - accuracy: 0.0000e+00\n",
      "[0.6420014500617981, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_44 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.3095 - accuracy: 0.0000e+00 - val_loss: 0.4670 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2825 - accuracy: 0.0000e+00 - val_loss: 0.4570 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2556 - accuracy: 0.0000e+00 - val_loss: 0.4468 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2262 - accuracy: 0.0000e+00 - val_loss: 0.4364 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2053 - accuracy: 0.0000e+00 - val_loss: 0.4252 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1798 - accuracy: 0.0000e+00 - val_loss: 0.4154 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1580 - accuracy: 0.0000e+00 - val_loss: 0.4048 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1371 - accuracy: 0.0000e+00 - val_loss: 0.3939 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1193 - accuracy: 0.0000e+00 - val_loss: 0.3852 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1006 - accuracy: 0.0000e+00 - val_loss: 0.3782 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0826 - accuracy: 0.0000e+00 - val_loss: 0.3733 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0681 - accuracy: 0.0000e+00 - val_loss: 0.3700 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0526 - accuracy: 0.0000e+00 - val_loss: 0.3686 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0409 - accuracy: 0.0000e+00 - val_loss: 0.3690 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0278 - accuracy: 0.0000e+00 - val_loss: 0.3712 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0160 - accuracy: 0.0000e+00 - val_loss: 0.3745 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0067 - accuracy: 0.0000e+00 - val_loss: 0.3795 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9991 - accuracy: 0.0000e+00 - val_loss: 0.3853 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9900 - accuracy: 0.0000e+00 - val_loss: 0.3910 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9817 - accuracy: 0.0000e+00 - val_loss: 0.3970 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9761 - accuracy: 0.0000e+00 - val_loss: 0.4035 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9698 - accuracy: 0.0000e+00 - val_loss: 0.4091 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9645 - accuracy: 0.0000e+00 - val_loss: 0.4151 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9596 - accuracy: 0.0000e+00 - val_loss: 0.4209 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9559 - accuracy: 0.0000e+00 - val_loss: 0.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9506 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9470 - accuracy: 0.0000e+00 - val_loss: 0.4363 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9434 - accuracy: 0.0000e+00 - val_loss: 0.4408 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9411 - accuracy: 0.0000e+00 - val_loss: 0.4447 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9377 - accuracy: 0.0000e+00 - val_loss: 0.4487 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9348 - accuracy: 0.0000e+00 - val_loss: 0.4523 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9325 - accuracy: 0.0000e+00 - val_loss: 0.4562 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9297 - accuracy: 0.0000e+00 - val_loss: 0.4592 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9272 - accuracy: 0.0000e+00 - val_loss: 0.4620 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9254 - accuracy: 0.0000e+00 - val_loss: 0.4650 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9236 - accuracy: 0.0000e+00 - val_loss: 0.4679 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9210 - accuracy: 0.0000e+00 - val_loss: 0.4700 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9197 - accuracy: 0.0000e+00 - val_loss: 0.4723 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9180 - accuracy: 0.0000e+00 - val_loss: 0.4744 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9160 - accuracy: 0.0000e+00 - val_loss: 0.4763 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9148 - accuracy: 0.0000e+00 - val_loss: 0.4785 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9130 - accuracy: 0.0000e+00 - val_loss: 0.4802 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9124 - accuracy: 0.0000e+00 - val_loss: 0.4821 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9102 - accuracy: 0.0000e+00 - val_loss: 0.4838 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9090 - accuracy: 0.0000e+00 - val_loss: 0.4858 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9080 - accuracy: 0.0000e+00 - val_loss: 0.4872 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9067 - accuracy: 0.0000e+00 - val_loss: 0.4890 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9060 - accuracy: 0.0000e+00 - val_loss: 0.4906 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9044 - accuracy: 0.0000e+00 - val_loss: 0.4912 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9037 - accuracy: 0.0000e+00 - val_loss: 0.4922 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9026 - accuracy: 0.0000e+00 - val_loss: 0.4931 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9016 - accuracy: 0.0000e+00 - val_loss: 0.4946 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9010 - accuracy: 0.0000e+00 - val_loss: 0.4951 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8997 - accuracy: 0.0000e+00 - val_loss: 0.4962 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8989 - accuracy: 0.0000e+00 - val_loss: 0.4965 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.4972 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8973 - accuracy: 0.0000e+00 - val_loss: 0.4985 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8964 - accuracy: 0.0000e+00 - val_loss: 0.4990 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8956 - accuracy: 0.0000e+00 - val_loss: 0.4998 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8949 - accuracy: 0.0000e+00 - val_loss: 0.5002 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8942 - accuracy: 0.0000e+00 - val_loss: 0.5004 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8935 - accuracy: 0.0000e+00 - val_loss: 0.5009 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8929 - accuracy: 0.0000e+00 - val_loss: 0.5018 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.5023 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8915 - accuracy: 0.0000e+00 - val_loss: 0.5026 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8908 - accuracy: 0.0000e+00 - val_loss: 0.5031 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8902 - accuracy: 0.0000e+00 - val_loss: 0.5034 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8897 - accuracy: 0.0000e+00 - val_loss: 0.5040 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8891 - accuracy: 0.0000e+00 - val_loss: 0.5046 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8885 - accuracy: 0.0000e+00 - val_loss: 0.5044 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.5046 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8874 - accuracy: 0.0000e+00 - val_loss: 0.5054 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8868 - accuracy: 0.0000e+00 - val_loss: 0.5054 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8863 - accuracy: 0.0000e+00 - val_loss: 0.5059 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8858 - accuracy: 0.0000e+00 - val_loss: 0.5059 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8853 - accuracy: 0.0000e+00 - val_loss: 0.5058 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8848 - accuracy: 0.0000e+00 - val_loss: 0.5061 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.5059 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8839 - accuracy: 0.0000e+00 - val_loss: 0.5062 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8833 - accuracy: 0.0000e+00 - val_loss: 0.5061 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8829 - accuracy: 0.0000e+00 - val_loss: 0.5063 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8824 - accuracy: 0.0000e+00 - val_loss: 0.5067 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8819 - accuracy: 0.0000e+00 - val_loss: 0.5067 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8815 - accuracy: 0.0000e+00 - val_loss: 0.5070 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8811 - accuracy: 0.0000e+00 - val_loss: 0.5069 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8807 - accuracy: 0.0000e+00 - val_loss: 0.5068 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8802 - accuracy: 0.0000e+00 - val_loss: 0.5067 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8798 - accuracy: 0.0000e+00 - val_loss: 0.5066 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8793 - accuracy: 0.0000e+00 - val_loss: 0.5069 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8791 - accuracy: 0.0000e+00 - val_loss: 0.5074 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8785 - accuracy: 0.0000e+00 - val_loss: 0.5074 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.5074 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8776 - accuracy: 0.0000e+00 - val_loss: 0.5073 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8772 - accuracy: 0.0000e+00 - val_loss: 0.5075 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.5075 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8764 - accuracy: 0.0000e+00 - val_loss: 0.5077 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8761 - accuracy: 0.0000e+00 - val_loss: 0.5078 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8757 - accuracy: 0.0000e+00 - val_loss: 0.5080 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8753 - accuracy: 0.0000e+00 - val_loss: 0.5087 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8749 - accuracy: 0.0000e+00 - val_loss: 0.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8744 - accuracy: 0.0000e+00 - val_loss: 0.5091 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8741 - accuracy: 0.0000e+00 - val_loss: 0.5094 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.5096 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.5102 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8730 - accuracy: 0.0000e+00 - val_loss: 0.5102 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.5104 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8722 - accuracy: 0.0000e+00 - val_loss: 0.5101 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8717 - accuracy: 0.0000e+00 - val_loss: 0.5101 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8714 - accuracy: 0.0000e+00 - val_loss: 0.5108 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.5111 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8706 - accuracy: 0.0000e+00 - val_loss: 0.5114 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.5113 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8697 - accuracy: 0.0000e+00 - val_loss: 0.5114 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8694 - accuracy: 0.0000e+00 - val_loss: 0.5115 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8689 - accuracy: 0.0000e+00 - val_loss: 0.5119 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8685 - accuracy: 0.0000e+00 - val_loss: 0.5122 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8678 - accuracy: 0.0000e+00 - val_loss: 0.5122 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8673 - accuracy: 0.0000e+00 - val_loss: 0.5122 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8666 - accuracy: 0.0000e+00 - val_loss: 0.5119 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8660 - accuracy: 0.0000e+00 - val_loss: 0.5115 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8655 - accuracy: 0.0000e+00 - val_loss: 0.5110 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.5114 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8639 - accuracy: 0.0000e+00 - val_loss: 0.5117 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8636 - accuracy: 0.0000e+00 - val_loss: 0.5123 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8631 - accuracy: 0.0000e+00 - val_loss: 0.5122 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8628 - accuracy: 0.0000e+00 - val_loss: 0.5125 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8625 - accuracy: 0.0000e+00 - val_loss: 0.5128 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8623 - accuracy: 0.0000e+00 - val_loss: 0.5131 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8619 - accuracy: 0.0000e+00 - val_loss: 0.5136 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8615 - accuracy: 0.0000e+00 - val_loss: 0.5137 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8612 - accuracy: 0.0000e+00 - val_loss: 0.5137 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8609 - accuracy: 0.0000e+00 - val_loss: 0.5144 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8605 - accuracy: 0.0000e+00 - val_loss: 0.5147 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8602 - accuracy: 0.0000e+00 - val_loss: 0.5153 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8598 - accuracy: 0.0000e+00 - val_loss: 0.5155 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8595 - accuracy: 0.0000e+00 - val_loss: 0.5159 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8593 - accuracy: 0.0000e+00 - val_loss: 0.5161 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8590 - accuracy: 0.0000e+00 - val_loss: 0.5165 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8586 - accuracy: 0.0000e+00 - val_loss: 0.5167 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8584 - accuracy: 0.0000e+00 - val_loss: 0.5169 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8581 - accuracy: 0.0000e+00 - val_loss: 0.5172 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8578 - accuracy: 0.0000e+00 - val_loss: 0.5173 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8575 - accuracy: 0.0000e+00 - val_loss: 0.5182 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8573 - accuracy: 0.0000e+00 - val_loss: 0.5190 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8570 - accuracy: 0.0000e+00 - val_loss: 0.5193 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8567 - accuracy: 0.0000e+00 - val_loss: 0.5196 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8565 - accuracy: 0.0000e+00 - val_loss: 0.5195 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8561 - accuracy: 0.0000e+00 - val_loss: 0.5201 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8559 - accuracy: 0.0000e+00 - val_loss: 0.5202 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8556 - accuracy: 0.0000e+00 - val_loss: 0.5205 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.5207 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8552 - accuracy: 0.0000e+00 - val_loss: 0.5209 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8548 - accuracy: 0.0000e+00 - val_loss: 0.5215 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8546 - accuracy: 0.0000e+00 - val_loss: 0.5215 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8543 - accuracy: 0.0000e+00 - val_loss: 0.5217 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8541 - accuracy: 0.0000e+00 - val_loss: 0.5221 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.5225 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8536 - accuracy: 0.0000e+00 - val_loss: 0.5230 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8533 - accuracy: 0.0000e+00 - val_loss: 0.5232 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8531 - accuracy: 0.0000e+00 - val_loss: 0.5232 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8529 - accuracy: 0.0000e+00 - val_loss: 0.5235 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8527 - accuracy: 0.0000e+00 - val_loss: 0.5236 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8524 - accuracy: 0.0000e+00 - val_loss: 0.5242 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8522 - accuracy: 0.0000e+00 - val_loss: 0.5246 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8520 - accuracy: 0.0000e+00 - val_loss: 0.5253 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8518 - accuracy: 0.0000e+00 - val_loss: 0.5250 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8515 - accuracy: 0.0000e+00 - val_loss: 0.5254 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8513 - accuracy: 0.0000e+00 - val_loss: 0.5256 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8511 - accuracy: 0.0000e+00 - val_loss: 0.5260 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8509 - accuracy: 0.0000e+00 - val_loss: 0.5262 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - val_loss: 0.5267 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8504 - accuracy: 0.0000e+00 - val_loss: 0.5267 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8502 - accuracy: 0.0000e+00 - val_loss: 0.5272 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8499 - accuracy: 0.0000e+00 - val_loss: 0.5274 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8498 - accuracy: 0.0000e+00 - val_loss: 0.5279 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8496 - accuracy: 0.0000e+00 - val_loss: 0.5282 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8493 - accuracy: 0.0000e+00 - val_loss: 0.5280 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8492 - accuracy: 0.0000e+00 - val_loss: 0.5281 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8489 - accuracy: 0.0000e+00 - val_loss: 0.5279 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8487 - accuracy: 0.0000e+00 - val_loss: 0.5282 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8485 - accuracy: 0.0000e+00 - val_loss: 0.5284 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8483 - accuracy: 0.0000e+00 - val_loss: 0.5286 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8480 - accuracy: 0.0000e+00 - val_loss: 0.5289 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8479 - accuracy: 0.0000e+00 - val_loss: 0.5292 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8477 - accuracy: 0.0000e+00 - val_loss: 0.5290 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8475 - accuracy: 0.0000e+00 - val_loss: 0.5291 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8472 - accuracy: 0.0000e+00 - val_loss: 0.5296 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8471 - accuracy: 0.0000e+00 - val_loss: 0.5301 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8468 - accuracy: 0.0000e+00 - val_loss: 0.5302 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8466 - accuracy: 0.0000e+00 - val_loss: 0.5295 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8464 - accuracy: 0.0000e+00 - val_loss: 0.5298 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8462 - accuracy: 0.0000e+00 - val_loss: 0.5301 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8460 - accuracy: 0.0000e+00 - val_loss: 0.5303 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8458 - accuracy: 0.0000e+00 - val_loss: 0.5302 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8456 - accuracy: 0.0000e+00 - val_loss: 0.5302 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8454 - accuracy: 0.0000e+00 - val_loss: 0.5305 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8452 - accuracy: 0.0000e+00 - val_loss: 0.5306 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8450 - accuracy: 0.0000e+00 - val_loss: 0.5304 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8448 - accuracy: 0.0000e+00 - val_loss: 0.5306 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8446 - accuracy: 0.0000e+00 - val_loss: 0.5305 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8444 - accuracy: 0.0000e+00 - val_loss: 0.5313 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8442 - accuracy: 0.0000e+00 - val_loss: 0.5313 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8440 - accuracy: 0.0000e+00 - val_loss: 0.5313 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8438 - accuracy: 0.0000e+00 - val_loss: 0.5318 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8436 - accuracy: 0.0000e+00 - val_loss: 0.5322 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8434 - accuracy: 0.0000e+00 - val_loss: 0.5319 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8432 - accuracy: 0.0000e+00 - val_loss: 0.5322 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8430 - accuracy: 0.0000e+00 - val_loss: 0.5319 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8428 - accuracy: 0.0000e+00 - val_loss: 0.5316 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8426 - accuracy: 0.0000e+00 - val_loss: 0.5317 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.5327 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8422 - accuracy: 0.0000e+00 - val_loss: 0.5329 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8420 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8417 - accuracy: 0.0000e+00 - val_loss: 0.5329 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8415 - accuracy: 0.0000e+00 - val_loss: 0.5328 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8414 - accuracy: 0.0000e+00 - val_loss: 0.5328 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8411 - accuracy: 0.0000e+00 - val_loss: 0.5326 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8409 - accuracy: 0.0000e+00 - val_loss: 0.5331 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8407 - accuracy: 0.0000e+00 - val_loss: 0.5332 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8404 - accuracy: 0.0000e+00 - val_loss: 0.5333 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8402 - accuracy: 0.0000e+00 - val_loss: 0.5329 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8401 - accuracy: 0.0000e+00 - val_loss: 0.5326 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8398 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8395 - accuracy: 0.0000e+00 - val_loss: 0.5331 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8393 - accuracy: 0.0000e+00 - val_loss: 0.5328 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8390 - accuracy: 0.0000e+00 - val_loss: 0.5327 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8388 - accuracy: 0.0000e+00 - val_loss: 0.5326 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8386 - accuracy: 0.0000e+00 - val_loss: 0.5322 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8382 - accuracy: 0.0000e+00 - val_loss: 0.5320 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8380 - accuracy: 0.0000e+00 - val_loss: 0.5316 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8378 - accuracy: 0.0000e+00 - val_loss: 0.5310 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.5309 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8370 - accuracy: 0.0000e+00 - val_loss: 0.5308 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8366 - accuracy: 0.0000e+00 - val_loss: 0.5308 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8363 - accuracy: 0.0000e+00 - val_loss: 0.5301 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8358 - accuracy: 0.0000e+00 - val_loss: 0.5297 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8355 - accuracy: 0.0000e+00 - val_loss: 0.5297 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8350 - accuracy: 0.0000e+00 - val_loss: 0.5298 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8347 - accuracy: 0.0000e+00 - val_loss: 0.5293 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8341 - accuracy: 0.0000e+00 - val_loss: 0.5292 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8338 - accuracy: 0.0000e+00 - val_loss: 0.5282 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8329 - accuracy: 0.0000e+00 - val_loss: 0.5275 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8324 - accuracy: 0.0000e+00 - val_loss: 0.5269 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8320 - accuracy: 0.0000e+00 - val_loss: 0.5259 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8310 - accuracy: 0.0000e+00 - val_loss: 0.5263 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8304 - accuracy: 0.0000e+00 - val_loss: 0.5258 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8300 - accuracy: 0.0000e+00 - val_loss: 0.5245 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8285 - accuracy: 0.0000e+00 - val_loss: 0.5242 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8282 - accuracy: 0.0000e+00 - val_loss: 0.5231 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8270 - accuracy: 0.0000e+00 - val_loss: 0.5232 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8265 - accuracy: 0.0000e+00 - val_loss: 0.5231 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8255 - accuracy: 0.0000e+00 - val_loss: 0.5231 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8248 - accuracy: 0.0000e+00 - val_loss: 0.5226 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8245 - accuracy: 0.0000e+00 - val_loss: 0.5220 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8235 - accuracy: 0.0000e+00 - val_loss: 0.5213 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8232 - accuracy: 0.0000e+00 - val_loss: 0.5221 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/AMGN_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5221 - accuracy: 0.0000e+00\n",
      "[0.5221380591392517, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_46 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.3818 - accuracy: 0.0000e+00 - val_loss: 0.4861 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3502 - accuracy: 0.0000e+00 - val_loss: 0.4880 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3220 - accuracy: 0.0000e+00 - val_loss: 0.4904 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2959 - accuracy: 0.0000e+00 - val_loss: 0.4936 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2657 - accuracy: 0.0000e+00 - val_loss: 0.4971 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2459 - accuracy: 0.0000e+00 - val_loss: 0.5014 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2231 - accuracy: 0.0000e+00 - val_loss: 0.5059 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2038 - accuracy: 0.0000e+00 - val_loss: 0.5101 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1876 - accuracy: 0.0000e+00 - val_loss: 0.5150 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1702 - accuracy: 0.0000e+00 - val_loss: 0.5195 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1567 - accuracy: 0.0000e+00 - val_loss: 0.5238 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1425 - accuracy: 0.0000e+00 - val_loss: 0.5282 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1307 - accuracy: 0.0000e+00 - val_loss: 0.5330 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1199 - accuracy: 0.0000e+00 - val_loss: 0.5378 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1092 - accuracy: 0.0000e+00 - val_loss: 0.5423 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1016 - accuracy: 0.0000e+00 - val_loss: 0.5472 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0913 - accuracy: 0.0000e+00 - val_loss: 0.5516 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0846 - accuracy: 0.0000e+00 - val_loss: 0.5565 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0778 - accuracy: 0.0000e+00 - val_loss: 0.5613 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0702 - accuracy: 0.0000e+00 - val_loss: 0.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0638 - accuracy: 0.0000e+00 - val_loss: 0.5698 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0579 - accuracy: 0.0000e+00 - val_loss: 0.5733 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0529 - accuracy: 0.0000e+00 - val_loss: 0.5767 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0487 - accuracy: 0.0000e+00 - val_loss: 0.5810 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0431 - accuracy: 0.0000e+00 - val_loss: 0.5842 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0392 - accuracy: 0.0000e+00 - val_loss: 0.5873 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0353 - accuracy: 0.0000e+00 - val_loss: 0.5906 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0311 - accuracy: 0.0000e+00 - val_loss: 0.5939 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0285 - accuracy: 0.0000e+00 - val_loss: 0.5979 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0242 - accuracy: 0.0000e+00 - val_loss: 0.6009 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0212 - accuracy: 0.0000e+00 - val_loss: 0.6042 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0175 - accuracy: 0.0000e+00 - val_loss: 0.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0145 - accuracy: 0.0000e+00 - val_loss: 0.6093 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0121 - accuracy: 0.0000e+00 - val_loss: 0.6119 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0089 - accuracy: 0.0000e+00 - val_loss: 0.6138 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0068 - accuracy: 0.0000e+00 - val_loss: 0.6169 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0039 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0017 - accuracy: 0.0000e+00 - val_loss: 0.6213 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9996 - accuracy: 0.0000e+00 - val_loss: 0.6239 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9973 - accuracy: 0.0000e+00 - val_loss: 0.6253 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9957 - accuracy: 0.0000e+00 - val_loss: 0.6277 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9936 - accuracy: 0.0000e+00 - val_loss: 0.6302 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9915 - accuracy: 0.0000e+00 - val_loss: 0.6317 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9904 - accuracy: 0.0000e+00 - val_loss: 0.6332 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9890 - accuracy: 0.0000e+00 - val_loss: 0.6355 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9866 - accuracy: 0.0000e+00 - val_loss: 0.6373 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9850 - accuracy: 0.0000e+00 - val_loss: 0.6388 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9836 - accuracy: 0.0000e+00 - val_loss: 0.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9821 - accuracy: 0.0000e+00 - val_loss: 0.6424 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9808 - accuracy: 0.0000e+00 - val_loss: 0.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9796 - accuracy: 0.0000e+00 - val_loss: 0.6455 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9779 - accuracy: 0.0000e+00 - val_loss: 0.6463 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9765 - accuracy: 0.0000e+00 - val_loss: 0.6480 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9752 - accuracy: 0.0000e+00 - val_loss: 0.6494 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9740 - accuracy: 0.0000e+00 - val_loss: 0.6504 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9727 - accuracy: 0.0000e+00 - val_loss: 0.6512 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9715 - accuracy: 0.0000e+00 - val_loss: 0.6524 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9705 - accuracy: 0.0000e+00 - val_loss: 0.6544 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9691 - accuracy: 0.0000e+00 - val_loss: 0.6557 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9679 - accuracy: 0.0000e+00 - val_loss: 0.6566 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9666 - accuracy: 0.0000e+00 - val_loss: 0.6575 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9656 - accuracy: 0.0000e+00 - val_loss: 0.6587 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9646 - accuracy: 0.0000e+00 - val_loss: 0.6601 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9632 - accuracy: 0.0000e+00 - val_loss: 0.6610 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9624 - accuracy: 0.0000e+00 - val_loss: 0.6617 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9609 - accuracy: 0.0000e+00 - val_loss: 0.6629 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9598 - accuracy: 0.0000e+00 - val_loss: 0.6636 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9585 - accuracy: 0.0000e+00 - val_loss: 0.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9574 - accuracy: 0.0000e+00 - val_loss: 0.6658 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9566 - accuracy: 0.0000e+00 - val_loss: 0.6661 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9555 - accuracy: 0.0000e+00 - val_loss: 0.6672 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9546 - accuracy: 0.0000e+00 - val_loss: 0.6672 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9535 - accuracy: 0.0000e+00 - val_loss: 0.6687 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9523 - accuracy: 0.0000e+00 - val_loss: 0.6700 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9514 - accuracy: 0.0000e+00 - val_loss: 0.6704 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9503 - accuracy: 0.0000e+00 - val_loss: 0.6718 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9492 - accuracy: 0.0000e+00 - val_loss: 0.6724 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9483 - accuracy: 0.0000e+00 - val_loss: 0.6734 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9471 - accuracy: 0.0000e+00 - val_loss: 0.6743 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9460 - accuracy: 0.0000e+00 - val_loss: 0.6750 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9451 - accuracy: 0.0000e+00 - val_loss: 0.6754 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9440 - accuracy: 0.0000e+00 - val_loss: 0.6758 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9431 - accuracy: 0.0000e+00 - val_loss: 0.6760 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9419 - accuracy: 0.0000e+00 - val_loss: 0.6767 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9410 - accuracy: 0.0000e+00 - val_loss: 0.6774 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.6784 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9388 - accuracy: 0.0000e+00 - val_loss: 0.6789 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9378 - accuracy: 0.0000e+00 - val_loss: 0.6796 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9369 - accuracy: 0.0000e+00 - val_loss: 0.6803 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9358 - accuracy: 0.0000e+00 - val_loss: 0.6809 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9350 - accuracy: 0.0000e+00 - val_loss: 0.6810 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9338 - accuracy: 0.0000e+00 - val_loss: 0.6815 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.6825 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9320 - accuracy: 0.0000e+00 - val_loss: 0.6830 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9309 - accuracy: 0.0000e+00 - val_loss: 0.6832 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9299 - accuracy: 0.0000e+00 - val_loss: 0.6839 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.6842 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9280 - accuracy: 0.0000e+00 - val_loss: 0.6847 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9272 - accuracy: 0.0000e+00 - val_loss: 0.6849 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9260 - accuracy: 0.0000e+00 - val_loss: 0.6858 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9254 - accuracy: 0.0000e+00 - val_loss: 0.6857 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9243 - accuracy: 0.0000e+00 - val_loss: 0.6861 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9232 - accuracy: 0.0000e+00 - val_loss: 0.6873 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9224 - accuracy: 0.0000e+00 - val_loss: 0.6884 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9215 - accuracy: 0.0000e+00 - val_loss: 0.6887 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9206 - accuracy: 0.0000e+00 - val_loss: 0.6891 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9199 - accuracy: 0.0000e+00 - val_loss: 0.6890 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9191 - accuracy: 0.0000e+00 - val_loss: 0.6894 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9182 - accuracy: 0.0000e+00 - val_loss: 0.6896 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9173 - accuracy: 0.0000e+00 - val_loss: 0.6907 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9164 - accuracy: 0.0000e+00 - val_loss: 0.6905 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9153 - accuracy: 0.0000e+00 - val_loss: 0.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9147 - accuracy: 0.0000e+00 - val_loss: 0.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9136 - accuracy: 0.0000e+00 - val_loss: 0.6920 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9126 - accuracy: 0.0000e+00 - val_loss: 0.6923 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9121 - accuracy: 0.0000e+00 - val_loss: 0.6922 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9111 - accuracy: 0.0000e+00 - val_loss: 0.6924 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9104 - accuracy: 0.0000e+00 - val_loss: 0.6930 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9096 - accuracy: 0.0000e+00 - val_loss: 0.6925 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9084 - accuracy: 0.0000e+00 - val_loss: 0.6926 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9079 - accuracy: 0.0000e+00 - val_loss: 0.6943 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9067 - accuracy: 0.0000e+00 - val_loss: 0.6944 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9060 - accuracy: 0.0000e+00 - val_loss: 0.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9049 - accuracy: 0.0000e+00 - val_loss: 0.6946 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9042 - accuracy: 0.0000e+00 - val_loss: 0.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9033 - accuracy: 0.0000e+00 - val_loss: 0.6951 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9027 - accuracy: 0.0000e+00 - val_loss: 0.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9019 - accuracy: 0.0000e+00 - val_loss: 0.6956 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9011 - accuracy: 0.0000e+00 - val_loss: 0.6949 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9000 - accuracy: 0.0000e+00 - val_loss: 0.6946 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8990 - accuracy: 0.0000e+00 - val_loss: 0.6954 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8983 - accuracy: 0.0000e+00 - val_loss: 0.6963 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8979 - accuracy: 0.0000e+00 - val_loss: 0.6951 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8967 - accuracy: 0.0000e+00 - val_loss: 0.6957 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8961 - accuracy: 0.0000e+00 - val_loss: 0.6959 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8951 - accuracy: 0.0000e+00 - val_loss: 0.6960 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8944 - accuracy: 0.0000e+00 - val_loss: 0.6956 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8936 - accuracy: 0.0000e+00 - val_loss: 0.6958 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8928 - accuracy: 0.0000e+00 - val_loss: 0.6959 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.6966 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8915 - accuracy: 0.0000e+00 - val_loss: 0.6963 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.6970 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8899 - accuracy: 0.0000e+00 - val_loss: 0.6966 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8891 - accuracy: 0.0000e+00 - val_loss: 0.6965 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.6970 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8877 - accuracy: 0.0000e+00 - val_loss: 0.6971 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8870 - accuracy: 0.0000e+00 - val_loss: 0.6961 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8861 - accuracy: 0.0000e+00 - val_loss: 0.6967 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8855 - accuracy: 0.0000e+00 - val_loss: 0.6974 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8845 - accuracy: 0.0000e+00 - val_loss: 0.6969 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.6958 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8832 - accuracy: 0.0000e+00 - val_loss: 0.6958 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8826 - accuracy: 0.0000e+00 - val_loss: 0.6961 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8817 - accuracy: 0.0000e+00 - val_loss: 0.6963 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8810 - accuracy: 0.0000e+00 - val_loss: 0.6957 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8804 - accuracy: 0.0000e+00 - val_loss: 0.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8796 - accuracy: 0.0000e+00 - val_loss: 0.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8788 - accuracy: 0.0000e+00 - val_loss: 0.6949 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8780 - accuracy: 0.0000e+00 - val_loss: 0.6956 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8775 - accuracy: 0.0000e+00 - val_loss: 0.6952 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.6947 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8762 - accuracy: 0.0000e+00 - val_loss: 0.6944 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8753 - accuracy: 0.0000e+00 - val_loss: 0.6948 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8749 - accuracy: 0.0000e+00 - val_loss: 0.6956 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.6942 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.6943 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8729 - accuracy: 0.0000e+00 - val_loss: 0.6939 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8722 - accuracy: 0.0000e+00 - val_loss: 0.6944 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8714 - accuracy: 0.0000e+00 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8707 - accuracy: 0.0000e+00 - val_loss: 0.6935 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8700 - accuracy: 0.0000e+00 - val_loss: 0.6932 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8692 - accuracy: 0.0000e+00 - val_loss: 0.6930 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8689 - accuracy: 0.0000e+00 - val_loss: 0.6937 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8680 - accuracy: 0.0000e+00 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8674 - accuracy: 0.0000e+00 - val_loss: 0.6929 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8666 - accuracy: 0.0000e+00 - val_loss: 0.6930 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8661 - accuracy: 0.0000e+00 - val_loss: 0.6923 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8655 - accuracy: 0.0000e+00 - val_loss: 0.6925 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8648 - accuracy: 0.0000e+00 - val_loss: 0.6926 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8643 - accuracy: 0.0000e+00 - val_loss: 0.6917 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8637 - accuracy: 0.0000e+00 - val_loss: 0.6923 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8633 - accuracy: 0.0000e+00 - val_loss: 0.6907 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8625 - accuracy: 0.0000e+00 - val_loss: 0.6910 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.6916 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8610 - accuracy: 0.0000e+00 - val_loss: 0.6918 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8606 - accuracy: 0.0000e+00 - val_loss: 0.6915 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8599 - accuracy: 0.0000e+00 - val_loss: 0.6913 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8594 - accuracy: 0.0000e+00 - val_loss: 0.6911 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8588 - accuracy: 0.0000e+00 - val_loss: 0.6913 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8581 - accuracy: 0.0000e+00 - val_loss: 0.6908 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8576 - accuracy: 0.0000e+00 - val_loss: 0.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8570 - accuracy: 0.0000e+00 - val_loss: 0.6906 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8564 - accuracy: 0.0000e+00 - val_loss: 0.6907 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8558 - accuracy: 0.0000e+00 - val_loss: 0.6910 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8554 - accuracy: 0.0000e+00 - val_loss: 0.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8548 - accuracy: 0.0000e+00 - val_loss: 0.6912 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8545 - accuracy: 0.0000e+00 - val_loss: 0.6904 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.6909 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8533 - accuracy: 0.0000e+00 - val_loss: 0.6907 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8529 - accuracy: 0.0000e+00 - val_loss: 0.6914 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8524 - accuracy: 0.0000e+00 - val_loss: 0.6913 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8517 - accuracy: 0.0000e+00 - val_loss: 0.6916 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8512 - accuracy: 0.0000e+00 - val_loss: 0.6917 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8507 - accuracy: 0.0000e+00 - val_loss: 0.6917 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8503 - accuracy: 0.0000e+00 - val_loss: 0.6921 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8500 - accuracy: 0.0000e+00 - val_loss: 0.6912 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8493 - accuracy: 0.0000e+00 - val_loss: 0.6914 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8489 - accuracy: 0.0000e+00 - val_loss: 0.6921 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8484 - accuracy: 0.0000e+00 - val_loss: 0.6917 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8479 - accuracy: 0.0000e+00 - val_loss: 0.6920 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8474 - accuracy: 0.0000e+00 - val_loss: 0.6924 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8470 - accuracy: 0.0000e+00 - val_loss: 0.6925 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8465 - accuracy: 0.0000e+00 - val_loss: 0.6925 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8461 - accuracy: 0.0000e+00 - val_loss: 0.6925 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8457 - accuracy: 0.0000e+00 - val_loss: 0.6924 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8452 - accuracy: 0.0000e+00 - val_loss: 0.6923 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8448 - accuracy: 0.0000e+00 - val_loss: 0.6930 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8445 - accuracy: 0.0000e+00 - val_loss: 0.6924 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8440 - accuracy: 0.0000e+00 - val_loss: 0.6933 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8434 - accuracy: 0.0000e+00 - val_loss: 0.6932 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8431 - accuracy: 0.0000e+00 - val_loss: 0.6932 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8427 - accuracy: 0.0000e+00 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8422 - accuracy: 0.0000e+00 - val_loss: 0.6942 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8418 - accuracy: 0.0000e+00 - val_loss: 0.6942 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8415 - accuracy: 0.0000e+00 - val_loss: 0.6940 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8410 - accuracy: 0.0000e+00 - val_loss: 0.6941 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8406 - accuracy: 0.0000e+00 - val_loss: 0.6943 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8404 - accuracy: 0.0000e+00 - val_loss: 0.6943 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8400 - accuracy: 0.0000e+00 - val_loss: 0.6943 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8395 - accuracy: 0.0000e+00 - val_loss: 0.6951 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8392 - accuracy: 0.0000e+00 - val_loss: 0.6949 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8389 - accuracy: 0.0000e+00 - val_loss: 0.6951 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8386 - accuracy: 0.0000e+00 - val_loss: 0.6954 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8382 - accuracy: 0.0000e+00 - val_loss: 0.6957 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8379 - accuracy: 0.0000e+00 - val_loss: 0.6952 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8375 - accuracy: 0.0000e+00 - val_loss: 0.6956 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.6960 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8368 - accuracy: 0.0000e+00 - val_loss: 0.6967 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8366 - accuracy: 0.0000e+00 - val_loss: 0.6965 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8362 - accuracy: 0.0000e+00 - val_loss: 0.6966 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8359 - accuracy: 0.0000e+00 - val_loss: 0.6972 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8354 - accuracy: 0.0000e+00 - val_loss: 0.6974 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8353 - accuracy: 0.0000e+00 - val_loss: 0.6975 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8348 - accuracy: 0.0000e+00 - val_loss: 0.6977 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.6981 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8345 - accuracy: 0.0000e+00 - val_loss: 0.6991 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8341 - accuracy: 0.0000e+00 - val_loss: 0.6984 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8337 - accuracy: 0.0000e+00 - val_loss: 0.6990 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8335 - accuracy: 0.0000e+00 - val_loss: 0.6988 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.6991 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8328 - accuracy: 0.0000e+00 - val_loss: 0.6998 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.6999 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8322 - accuracy: 0.0000e+00 - val_loss: 0.7002 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8318 - accuracy: 0.0000e+00 - val_loss: 0.7004 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8316 - accuracy: 0.0000e+00 - val_loss: 0.7006 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8313 - accuracy: 0.0000e+00 - val_loss: 0.7008 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/HON_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7008 - accuracy: 0.0000e+00\n",
      "[0.7007513642311096, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2227 - accuracy: 0.0000e+00 - val_loss: 0.2893 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1992 - accuracy: 0.0000e+00 - val_loss: 0.2960 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1817 - accuracy: 0.0000e+00 - val_loss: 0.3010 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1639 - accuracy: 0.0000e+00 - val_loss: 0.3052 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1458 - accuracy: 0.0000e+00 - val_loss: 0.3089 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1290 - accuracy: 0.0000e+00 - val_loss: 0.3131 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1162 - accuracy: 0.0000e+00 - val_loss: 0.3173 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.1030 - accuracy: 0.0000e+00 - val_loss: 0.3215 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0902 - accuracy: 0.0000e+00 - val_loss: 0.3252 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0811 - accuracy: 0.0000e+00 - val_loss: 0.3287 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0694 - accuracy: 0.0000e+00 - val_loss: 0.3321 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0623 - accuracy: 0.0000e+00 - val_loss: 0.3352 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0524 - accuracy: 0.0000e+00 - val_loss: 0.3379 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0470 - accuracy: 0.0000e+00 - val_loss: 0.3409 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0394 - accuracy: 0.0000e+00 - val_loss: 0.3436 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0320 - accuracy: 0.0000e+00 - val_loss: 0.3462 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0256 - accuracy: 0.0000e+00 - val_loss: 0.3484 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0204 - accuracy: 0.0000e+00 - val_loss: 0.3505 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0158 - accuracy: 0.0000e+00 - val_loss: 0.3523 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0125 - accuracy: 0.0000e+00 - val_loss: 0.3544 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0074 - accuracy: 0.0000e+00 - val_loss: 0.3556 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0049 - accuracy: 0.0000e+00 - val_loss: 0.3577 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0008 - accuracy: 0.0000e+00 - val_loss: 0.3594 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9973 - accuracy: 0.0000e+00 - val_loss: 0.3606 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9949 - accuracy: 0.0000e+00 - val_loss: 0.3619 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9929 - accuracy: 0.0000e+00 - val_loss: 0.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9899 - accuracy: 0.0000e+00 - val_loss: 0.3650 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9876 - accuracy: 0.0000e+00 - val_loss: 0.3659 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9855 - accuracy: 0.0000e+00 - val_loss: 0.3672 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9833 - accuracy: 0.0000e+00 - val_loss: 0.3680 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9815 - accuracy: 0.0000e+00 - val_loss: 0.3685 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9797 - accuracy: 0.0000e+00 - val_loss: 0.3698 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9778 - accuracy: 0.0000e+00 - val_loss: 0.3705 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9762 - accuracy: 0.0000e+00 - val_loss: 0.3711 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9743 - accuracy: 0.0000e+00 - val_loss: 0.3721 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9729 - accuracy: 0.0000e+00 - val_loss: 0.3722 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9708 - accuracy: 0.0000e+00 - val_loss: 0.3732 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9695 - accuracy: 0.0000e+00 - val_loss: 0.3741 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9678 - accuracy: 0.0000e+00 - val_loss: 0.3753 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9662 - accuracy: 0.0000e+00 - val_loss: 0.3758 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9642 - accuracy: 0.0000e+00 - val_loss: 0.3765 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9628 - accuracy: 0.0000e+00 - val_loss: 0.3768 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9612 - accuracy: 0.0000e+00 - val_loss: 0.3772 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9592 - accuracy: 0.0000e+00 - val_loss: 0.3778 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9578 - accuracy: 0.0000e+00 - val_loss: 0.3786 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9562 - accuracy: 0.0000e+00 - val_loss: 0.3790 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9546 - accuracy: 0.0000e+00 - val_loss: 0.3804 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9527 - accuracy: 0.0000e+00 - val_loss: 0.3809 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9513 - accuracy: 0.0000e+00 - val_loss: 0.3814 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9498 - accuracy: 0.0000e+00 - val_loss: 0.3816 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9481 - accuracy: 0.0000e+00 - val_loss: 0.3821 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9471 - accuracy: 0.0000e+00 - val_loss: 0.3829 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9456 - accuracy: 0.0000e+00 - val_loss: 0.3830 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9442 - accuracy: 0.0000e+00 - val_loss: 0.3835 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9429 - accuracy: 0.0000e+00 - val_loss: 0.3836 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9415 - accuracy: 0.0000e+00 - val_loss: 0.3838 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9405 - accuracy: 0.0000e+00 - val_loss: 0.3839 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9394 - accuracy: 0.0000e+00 - val_loss: 0.3841 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9382 - accuracy: 0.0000e+00 - val_loss: 0.3842 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9371 - accuracy: 0.0000e+00 - val_loss: 0.3846 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9362 - accuracy: 0.0000e+00 - val_loss: 0.3855 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9353 - accuracy: 0.0000e+00 - val_loss: 0.3853 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9340 - accuracy: 0.0000e+00 - val_loss: 0.3859 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9330 - accuracy: 0.0000e+00 - val_loss: 0.3864 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9321 - accuracy: 0.0000e+00 - val_loss: 0.3870 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9312 - accuracy: 0.0000e+00 - val_loss: 0.3869 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9302 - accuracy: 0.0000e+00 - val_loss: 0.3872 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.3883 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9283 - accuracy: 0.0000e+00 - val_loss: 0.3884 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9275 - accuracy: 0.0000e+00 - val_loss: 0.3881 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9264 - accuracy: 0.0000e+00 - val_loss: 0.3889 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9253 - accuracy: 0.0000e+00 - val_loss: 0.3889 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9245 - accuracy: 0.0000e+00 - val_loss: 0.3892 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9236 - accuracy: 0.0000e+00 - val_loss: 0.3893 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9227 - accuracy: 0.0000e+00 - val_loss: 0.3901 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9218 - accuracy: 0.0000e+00 - val_loss: 0.3899 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9209 - accuracy: 0.0000e+00 - val_loss: 0.3897 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9200 - accuracy: 0.0000e+00 - val_loss: 0.3905 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9193 - accuracy: 0.0000e+00 - val_loss: 0.3906 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9186 - accuracy: 0.0000e+00 - val_loss: 0.3901 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9177 - accuracy: 0.0000e+00 - val_loss: 0.3907 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9169 - accuracy: 0.0000e+00 - val_loss: 0.3915 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9161 - accuracy: 0.0000e+00 - val_loss: 0.3912 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9152 - accuracy: 0.0000e+00 - val_loss: 0.3910 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9144 - accuracy: 0.0000e+00 - val_loss: 0.3909 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9136 - accuracy: 0.0000e+00 - val_loss: 0.3905 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9129 - accuracy: 0.0000e+00 - val_loss: 0.3905 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9121 - accuracy: 0.0000e+00 - val_loss: 0.3905 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9114 - accuracy: 0.0000e+00 - val_loss: 0.3905 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9104 - accuracy: 0.0000e+00 - val_loss: 0.3913 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9097 - accuracy: 0.0000e+00 - val_loss: 0.3913 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9091 - accuracy: 0.0000e+00 - val_loss: 0.3909 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9082 - accuracy: 0.0000e+00 - val_loss: 0.3911 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9076 - accuracy: 0.0000e+00 - val_loss: 0.3916 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9070 - accuracy: 0.0000e+00 - val_loss: 0.3911 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9063 - accuracy: 0.0000e+00 - val_loss: 0.3916 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9058 - accuracy: 0.0000e+00 - val_loss: 0.3922 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9051 - accuracy: 0.0000e+00 - val_loss: 0.3921 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9046 - accuracy: 0.0000e+00 - val_loss: 0.3924 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9040 - accuracy: 0.0000e+00 - val_loss: 0.3931 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9034 - accuracy: 0.0000e+00 - val_loss: 0.3933 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.3936 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9023 - accuracy: 0.0000e+00 - val_loss: 0.3933 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.3936 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9012 - accuracy: 0.0000e+00 - val_loss: 0.3944 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9008 - accuracy: 0.0000e+00 - val_loss: 0.3939 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9004 - accuracy: 0.0000e+00 - val_loss: 0.3946 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8997 - accuracy: 0.0000e+00 - val_loss: 0.3948 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8993 - accuracy: 0.0000e+00 - val_loss: 0.3947 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8988 - accuracy: 0.0000e+00 - val_loss: 0.3948 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8983 - accuracy: 0.0000e+00 - val_loss: 0.3950 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8978 - accuracy: 0.0000e+00 - val_loss: 0.3950 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8974 - accuracy: 0.0000e+00 - val_loss: 0.3952 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8969 - accuracy: 0.0000e+00 - val_loss: 0.3957 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8965 - accuracy: 0.0000e+00 - val_loss: 0.3962 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8960 - accuracy: 0.0000e+00 - val_loss: 0.3961 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8956 - accuracy: 0.0000e+00 - val_loss: 0.3966 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8952 - accuracy: 0.0000e+00 - val_loss: 0.3964 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8948 - accuracy: 0.0000e+00 - val_loss: 0.3965 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8944 - accuracy: 0.0000e+00 - val_loss: 0.3970 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8939 - accuracy: 0.0000e+00 - val_loss: 0.3970 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8935 - accuracy: 0.0000e+00 - val_loss: 0.3970 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8932 - accuracy: 0.0000e+00 - val_loss: 0.3973 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8928 - accuracy: 0.0000e+00 - val_loss: 0.3974 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8924 - accuracy: 0.0000e+00 - val_loss: 0.3979 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8921 - accuracy: 0.0000e+00 - val_loss: 0.3973 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8916 - accuracy: 0.0000e+00 - val_loss: 0.3978 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.3982 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.3993 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.3992 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8902 - accuracy: 0.0000e+00 - val_loss: 0.3998 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8899 - accuracy: 0.0000e+00 - val_loss: 0.3991 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8896 - accuracy: 0.0000e+00 - val_loss: 0.3990 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8891 - accuracy: 0.0000e+00 - val_loss: 0.3993 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8888 - accuracy: 0.0000e+00 - val_loss: 0.4001 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8886 - accuracy: 0.0000e+00 - val_loss: 0.3999 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8883 - accuracy: 0.0000e+00 - val_loss: 0.3998 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.4005 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8875 - accuracy: 0.0000e+00 - val_loss: 0.4009 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8871 - accuracy: 0.0000e+00 - val_loss: 0.4011 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8869 - accuracy: 0.0000e+00 - val_loss: 0.4016 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8865 - accuracy: 0.0000e+00 - val_loss: 0.4018 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8862 - accuracy: 0.0000e+00 - val_loss: 0.4018 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.4026 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8856 - accuracy: 0.0000e+00 - val_loss: 0.4030 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8855 - accuracy: 0.0000e+00 - val_loss: 0.4025 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.4027 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8848 - accuracy: 0.0000e+00 - val_loss: 0.4033 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8844 - accuracy: 0.0000e+00 - val_loss: 0.4035 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8841 - accuracy: 0.0000e+00 - val_loss: 0.4043 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8838 - accuracy: 0.0000e+00 - val_loss: 0.4048 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8835 - accuracy: 0.0000e+00 - val_loss: 0.4052 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8832 - accuracy: 0.0000e+00 - val_loss: 0.4049 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8830 - accuracy: 0.0000e+00 - val_loss: 0.4053 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8827 - accuracy: 0.0000e+00 - val_loss: 0.4061 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8824 - accuracy: 0.0000e+00 - val_loss: 0.4057 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8821 - accuracy: 0.0000e+00 - val_loss: 0.4059 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.8819 - accuracy: 0.0000e+00 - val_loss: 0.4060 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.4066 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - val_loss: 0.4070 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8812 - accuracy: 0.0000e+00 - val_loss: 0.4068 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8808 - accuracy: 0.0000e+00 - val_loss: 0.4073 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8805 - accuracy: 0.0000e+00 - val_loss: 0.4084 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8804 - accuracy: 0.0000e+00 - val_loss: 0.4093 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8802 - accuracy: 0.0000e+00 - val_loss: 0.4088 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.4079 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8796 - accuracy: 0.0000e+00 - val_loss: 0.4082 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8793 - accuracy: 0.0000e+00 - val_loss: 0.4089 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8791 - accuracy: 0.0000e+00 - val_loss: 0.4097 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.4099 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8786 - accuracy: 0.0000e+00 - val_loss: 0.4096 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8784 - accuracy: 0.0000e+00 - val_loss: 0.4098 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8782 - accuracy: 0.0000e+00 - val_loss: 0.4105 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8779 - accuracy: 0.0000e+00 - val_loss: 0.4106 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.4111 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8775 - accuracy: 0.0000e+00 - val_loss: 0.4117 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8773 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8771 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8769 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.4125 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8764 - accuracy: 0.0000e+00 - val_loss: 0.4131 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8762 - accuracy: 0.0000e+00 - val_loss: 0.4134 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8760 - accuracy: 0.0000e+00 - val_loss: 0.4135 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8758 - accuracy: 0.0000e+00 - val_loss: 0.4134 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8756 - accuracy: 0.0000e+00 - val_loss: 0.4142 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.4146 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8752 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8749 - accuracy: 0.0000e+00 - val_loss: 0.4149 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8748 - accuracy: 0.0000e+00 - val_loss: 0.4151 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8746 - accuracy: 0.0000e+00 - val_loss: 0.4156 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.4168 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8743 - accuracy: 0.0000e+00 - val_loss: 0.4160 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8740 - accuracy: 0.0000e+00 - val_loss: 0.4166 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8739 - accuracy: 0.0000e+00 - val_loss: 0.4164 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.4172 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.4171 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.4176 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8729 - accuracy: 0.0000e+00 - val_loss: 0.4184 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8728 - accuracy: 0.0000e+00 - val_loss: 0.4184 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8726 - accuracy: 0.0000e+00 - val_loss: 0.4189 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8724 - accuracy: 0.0000e+00 - val_loss: 0.4194 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8721 - accuracy: 0.0000e+00 - val_loss: 0.4193 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8720 - accuracy: 0.0000e+00 - val_loss: 0.4195 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8718 - accuracy: 0.0000e+00 - val_loss: 0.4202 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8716 - accuracy: 0.0000e+00 - val_loss: 0.4199 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8714 - accuracy: 0.0000e+00 - val_loss: 0.4202 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8710 - accuracy: 0.0000e+00 - val_loss: 0.4209 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.4211 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8706 - accuracy: 0.0000e+00 - val_loss: 0.4215 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.4217 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8703 - accuracy: 0.0000e+00 - val_loss: 0.4220 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.4219 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.4225 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8697 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8695 - accuracy: 0.0000e+00 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8693 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8691 - accuracy: 0.0000e+00 - val_loss: 0.4230 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8690 - accuracy: 0.0000e+00 - val_loss: 0.4236 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8689 - accuracy: 0.0000e+00 - val_loss: 0.4234 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8688 - accuracy: 0.0000e+00 - val_loss: 0.4240 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8686 - accuracy: 0.0000e+00 - val_loss: 0.4241 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8684 - accuracy: 0.0000e+00 - val_loss: 0.4240 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8683 - accuracy: 0.0000e+00 - val_loss: 0.4238 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8681 - accuracy: 0.0000e+00 - val_loss: 0.4242 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8679 - accuracy: 0.0000e+00 - val_loss: 0.4246 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8677 - accuracy: 0.0000e+00 - val_loss: 0.4251 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8677 - accuracy: 0.0000e+00 - val_loss: 0.4252 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8675 - accuracy: 0.0000e+00 - val_loss: 0.4248 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8673 - accuracy: 0.0000e+00 - val_loss: 0.4249 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8673 - accuracy: 0.0000e+00 - val_loss: 0.4255 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8670 - accuracy: 0.0000e+00 - val_loss: 0.4253 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8670 - accuracy: 0.0000e+00 - val_loss: 0.4252 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8668 - accuracy: 0.0000e+00 - val_loss: 0.4250 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8667 - accuracy: 0.0000e+00 - val_loss: 0.4254 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8665 - accuracy: 0.0000e+00 - val_loss: 0.4261 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8665 - accuracy: 0.0000e+00 - val_loss: 0.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8664 - accuracy: 0.0000e+00 - val_loss: 0.4255 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8663 - accuracy: 0.0000e+00 - val_loss: 0.4252 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8661 - accuracy: 0.0000e+00 - val_loss: 0.4258 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8659 - accuracy: 0.0000e+00 - val_loss: 0.4259 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8658 - accuracy: 0.0000e+00 - val_loss: 0.4267 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8657 - accuracy: 0.0000e+00 - val_loss: 0.4272 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8655 - accuracy: 0.0000e+00 - val_loss: 0.4271 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8655 - accuracy: 0.0000e+00 - val_loss: 0.4268 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8653 - accuracy: 0.0000e+00 - val_loss: 0.4269 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8652 - accuracy: 0.0000e+00 - val_loss: 0.4270 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8650 - accuracy: 0.0000e+00 - val_loss: 0.4271 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8650 - accuracy: 0.0000e+00 - val_loss: 0.4268 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8648 - accuracy: 0.0000e+00 - val_loss: 0.4272 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8647 - accuracy: 0.0000e+00 - val_loss: 0.4274 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8646 - accuracy: 0.0000e+00 - val_loss: 0.4273 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8644 - accuracy: 0.0000e+00 - val_loss: 0.4275 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8643 - accuracy: 0.0000e+00 - val_loss: 0.4276 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8642 - accuracy: 0.0000e+00 - val_loss: 0.4279 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/QCOM_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4279 - accuracy: 0.0000e+00\n",
      "[0.4278649091720581, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_50 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.2521 - accuracy: 0.0000e+00 - val_loss: 0.2761 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2292 - accuracy: 0.0000e+00 - val_loss: 0.2791 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2128 - accuracy: 0.0000e+00 - val_loss: 0.2824 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1985 - accuracy: 0.0000e+00 - val_loss: 0.2869 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1839 - accuracy: 0.0000e+00 - val_loss: 0.2913 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1682 - accuracy: 0.0000e+00 - val_loss: 0.2962 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1570 - accuracy: 0.0000e+00 - val_loss: 0.3016 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1442 - accuracy: 0.0000e+00 - val_loss: 0.3064 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1288 - accuracy: 0.0000e+00 - val_loss: 0.3107 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1208 - accuracy: 0.0000e+00 - val_loss: 0.3154 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1079 - accuracy: 0.0000e+00 - val_loss: 0.3194 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1015 - accuracy: 0.0000e+00 - val_loss: 0.3233 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0900 - accuracy: 0.0000e+00 - val_loss: 0.3275 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0805 - accuracy: 0.0000e+00 - val_loss: 0.3314 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0721 - accuracy: 0.0000e+00 - val_loss: 0.3351 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0649 - accuracy: 0.0000e+00 - val_loss: 0.3388 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0589 - accuracy: 0.0000e+00 - val_loss: 0.3421 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0528 - accuracy: 0.0000e+00 - val_loss: 0.3451 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0469 - accuracy: 0.0000e+00 - val_loss: 0.3481 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0403 - accuracy: 0.0000e+00 - val_loss: 0.3512 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0361 - accuracy: 0.0000e+00 - val_loss: 0.3543 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0306 - accuracy: 0.0000e+00 - val_loss: 0.3568 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0255 - accuracy: 0.0000e+00 - val_loss: 0.3597 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0207 - accuracy: 0.0000e+00 - val_loss: 0.3623 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0164 - accuracy: 0.0000e+00 - val_loss: 0.3647 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0130 - accuracy: 0.0000e+00 - val_loss: 0.3671 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0082 - accuracy: 0.0000e+00 - val_loss: 0.3693 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0046 - accuracy: 0.0000e+00 - val_loss: 0.3714 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0011 - accuracy: 0.0000e+00 - val_loss: 0.3733 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9979 - accuracy: 0.0000e+00 - val_loss: 0.3754 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9943 - accuracy: 0.0000e+00 - val_loss: 0.3770 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9913 - accuracy: 0.0000e+00 - val_loss: 0.3785 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9882 - accuracy: 0.0000e+00 - val_loss: 0.3799 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9857 - accuracy: 0.0000e+00 - val_loss: 0.3815 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9829 - accuracy: 0.0000e+00 - val_loss: 0.3829 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9811 - accuracy: 0.0000e+00 - val_loss: 0.3842 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9792 - accuracy: 0.0000e+00 - val_loss: 0.3857 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9764 - accuracy: 0.0000e+00 - val_loss: 0.3868 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9744 - accuracy: 0.0000e+00 - val_loss: 0.3883 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9724 - accuracy: 0.0000e+00 - val_loss: 0.3895 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9705 - accuracy: 0.0000e+00 - val_loss: 0.3905 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9693 - accuracy: 0.0000e+00 - val_loss: 0.3918 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9673 - accuracy: 0.0000e+00 - val_loss: 0.3928 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9659 - accuracy: 0.0000e+00 - val_loss: 0.3938 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9646 - accuracy: 0.0000e+00 - val_loss: 0.3949 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9636 - accuracy: 0.0000e+00 - val_loss: 0.3959 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9620 - accuracy: 0.0000e+00 - val_loss: 0.3969 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9607 - accuracy: 0.0000e+00 - val_loss: 0.3978 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9596 - accuracy: 0.0000e+00 - val_loss: 0.3986 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9587 - accuracy: 0.0000e+00 - val_loss: 0.3995 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9577 - accuracy: 0.0000e+00 - val_loss: 0.4004 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9565 - accuracy: 0.0000e+00 - val_loss: 0.4012 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9557 - accuracy: 0.0000e+00 - val_loss: 0.4019 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9549 - accuracy: 0.0000e+00 - val_loss: 0.4025 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9540 - accuracy: 0.0000e+00 - val_loss: 0.4033 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9531 - accuracy: 0.0000e+00 - val_loss: 0.4039 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9524 - accuracy: 0.0000e+00 - val_loss: 0.4047 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9519 - accuracy: 0.0000e+00 - val_loss: 0.4054 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9508 - accuracy: 0.0000e+00 - val_loss: 0.4060 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9504 - accuracy: 0.0000e+00 - val_loss: 0.4067 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9495 - accuracy: 0.0000e+00 - val_loss: 0.4073 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9489 - accuracy: 0.0000e+00 - val_loss: 0.4078 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9483 - accuracy: 0.0000e+00 - val_loss: 0.4084 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9480 - accuracy: 0.0000e+00 - val_loss: 0.4088 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9472 - accuracy: 0.0000e+00 - val_loss: 0.4095 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9467 - accuracy: 0.0000e+00 - val_loss: 0.4100 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9461 - accuracy: 0.0000e+00 - val_loss: 0.4107 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9457 - accuracy: 0.0000e+00 - val_loss: 0.4112 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.4116 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9446 - accuracy: 0.0000e+00 - val_loss: 0.4121 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9441 - accuracy: 0.0000e+00 - val_loss: 0.4126 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9439 - accuracy: 0.0000e+00 - val_loss: 0.4131 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9433 - accuracy: 0.0000e+00 - val_loss: 0.4134 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9429 - accuracy: 0.0000e+00 - val_loss: 0.4138 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9426 - accuracy: 0.0000e+00 - val_loss: 0.4143 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9422 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9419 - accuracy: 0.0000e+00 - val_loss: 0.4150 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9415 - accuracy: 0.0000e+00 - val_loss: 0.4154 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9412 - accuracy: 0.0000e+00 - val_loss: 0.4158 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9408 - accuracy: 0.0000e+00 - val_loss: 0.4161 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9406 - accuracy: 0.0000e+00 - val_loss: 0.4164 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9403 - accuracy: 0.0000e+00 - val_loss: 0.4167 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.4171 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9397 - accuracy: 0.0000e+00 - val_loss: 0.4174 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9394 - accuracy: 0.0000e+00 - val_loss: 0.4177 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9392 - accuracy: 0.0000e+00 - val_loss: 0.4180 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9389 - accuracy: 0.0000e+00 - val_loss: 0.4183 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9386 - accuracy: 0.0000e+00 - val_loss: 0.4185 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9384 - accuracy: 0.0000e+00 - val_loss: 0.4189 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9381 - accuracy: 0.0000e+00 - val_loss: 0.4192 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9379 - accuracy: 0.0000e+00 - val_loss: 0.4194 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9377 - accuracy: 0.0000e+00 - val_loss: 0.4196 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9374 - accuracy: 0.0000e+00 - val_loss: 0.4198 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9373 - accuracy: 0.0000e+00 - val_loss: 0.4200 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9370 - accuracy: 0.0000e+00 - val_loss: 0.4202 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9368 - accuracy: 0.0000e+00 - val_loss: 0.4204 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9366 - accuracy: 0.0000e+00 - val_loss: 0.4207 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9364 - accuracy: 0.0000e+00 - val_loss: 0.4211 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9361 - accuracy: 0.0000e+00 - val_loss: 0.4213 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9360 - accuracy: 0.0000e+00 - val_loss: 0.4214 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9357 - accuracy: 0.0000e+00 - val_loss: 0.4216 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9354 - accuracy: 0.0000e+00 - val_loss: 0.4218 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9353 - accuracy: 0.0000e+00 - val_loss: 0.4220 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9351 - accuracy: 0.0000e+00 - val_loss: 0.4222 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9349 - accuracy: 0.0000e+00 - val_loss: 0.4223 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9347 - accuracy: 0.0000e+00 - val_loss: 0.4225 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9345 - accuracy: 0.0000e+00 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9343 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9340 - accuracy: 0.0000e+00 - val_loss: 0.4231 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9339 - accuracy: 0.0000e+00 - val_loss: 0.4232 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9337 - accuracy: 0.0000e+00 - val_loss: 0.4232 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9334 - accuracy: 0.0000e+00 - val_loss: 0.4234 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9332 - accuracy: 0.0000e+00 - val_loss: 0.4236 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9331 - accuracy: 0.0000e+00 - val_loss: 0.4238 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9328 - accuracy: 0.0000e+00 - val_loss: 0.4239 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9326 - accuracy: 0.0000e+00 - val_loss: 0.4240 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9324 - accuracy: 0.0000e+00 - val_loss: 0.4240 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9322 - accuracy: 0.0000e+00 - val_loss: 0.4241 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9320 - accuracy: 0.0000e+00 - val_loss: 0.4242 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9318 - accuracy: 0.0000e+00 - val_loss: 0.4245 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9316 - accuracy: 0.0000e+00 - val_loss: 0.4245 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9313 - accuracy: 0.0000e+00 - val_loss: 0.4246 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9312 - accuracy: 0.0000e+00 - val_loss: 0.4247 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9308 - accuracy: 0.0000e+00 - val_loss: 0.4248 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9308 - accuracy: 0.0000e+00 - val_loss: 0.4250 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9305 - accuracy: 0.0000e+00 - val_loss: 0.4251 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9302 - accuracy: 0.0000e+00 - val_loss: 0.4253 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9300 - accuracy: 0.0000e+00 - val_loss: 0.4253 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9297 - accuracy: 0.0000e+00 - val_loss: 0.4253 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9296 - accuracy: 0.0000e+00 - val_loss: 0.4254 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.4255 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.4256 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9288 - accuracy: 0.0000e+00 - val_loss: 0.4256 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9286 - accuracy: 0.0000e+00 - val_loss: 0.4256 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9282 - accuracy: 0.0000e+00 - val_loss: 0.4257 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9282 - accuracy: 0.0000e+00 - val_loss: 0.4258 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9277 - accuracy: 0.0000e+00 - val_loss: 0.4258 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9274 - accuracy: 0.0000e+00 - val_loss: 0.4259 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9272 - accuracy: 0.0000e+00 - val_loss: 0.4260 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9269 - accuracy: 0.0000e+00 - val_loss: 0.4261 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9267 - accuracy: 0.0000e+00 - val_loss: 0.4261 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9263 - accuracy: 0.0000e+00 - val_loss: 0.4262 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9261 - accuracy: 0.0000e+00 - val_loss: 0.4263 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9258 - accuracy: 0.0000e+00 - val_loss: 0.4264 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9254 - accuracy: 0.0000e+00 - val_loss: 0.4264 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9253 - accuracy: 0.0000e+00 - val_loss: 0.4265 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9248 - accuracy: 0.0000e+00 - val_loss: 0.4265 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9245 - accuracy: 0.0000e+00 - val_loss: 0.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9242 - accuracy: 0.0000e+00 - val_loss: 0.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9240 - accuracy: 0.0000e+00 - val_loss: 0.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9236 - accuracy: 0.0000e+00 - val_loss: 0.4267 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9231 - accuracy: 0.0000e+00 - val_loss: 0.4268 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9228 - accuracy: 0.0000e+00 - val_loss: 0.4268 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9225 - accuracy: 0.0000e+00 - val_loss: 0.4269 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9222 - accuracy: 0.0000e+00 - val_loss: 0.4268 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9217 - accuracy: 0.0000e+00 - val_loss: 0.4269 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9213 - accuracy: 0.0000e+00 - val_loss: 0.4270 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9210 - accuracy: 0.0000e+00 - val_loss: 0.4271 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9208 - accuracy: 0.0000e+00 - val_loss: 0.4270 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9202 - accuracy: 0.0000e+00 - val_loss: 0.4271 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9199 - accuracy: 0.0000e+00 - val_loss: 0.4272 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9194 - accuracy: 0.0000e+00 - val_loss: 0.4272 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9189 - accuracy: 0.0000e+00 - val_loss: 0.4272 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9187 - accuracy: 0.0000e+00 - val_loss: 0.4273 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9182 - accuracy: 0.0000e+00 - val_loss: 0.4273 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9177 - accuracy: 0.0000e+00 - val_loss: 0.4274 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9173 - accuracy: 0.0000e+00 - val_loss: 0.4274 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9169 - accuracy: 0.0000e+00 - val_loss: 0.4275 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9165 - accuracy: 0.0000e+00 - val_loss: 0.4276 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9159 - accuracy: 0.0000e+00 - val_loss: 0.4277 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9158 - accuracy: 0.0000e+00 - val_loss: 0.4278 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9152 - accuracy: 0.0000e+00 - val_loss: 0.4278 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9146 - accuracy: 0.0000e+00 - val_loss: 0.4278 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9142 - accuracy: 0.0000e+00 - val_loss: 0.4279 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9139 - accuracy: 0.0000e+00 - val_loss: 0.4280 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9131 - accuracy: 0.0000e+00 - val_loss: 0.4281 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9130 - accuracy: 0.0000e+00 - val_loss: 0.4281 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9124 - accuracy: 0.0000e+00 - val_loss: 0.4282 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9121 - accuracy: 0.0000e+00 - val_loss: 0.4283 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9114 - accuracy: 0.0000e+00 - val_loss: 0.4283 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9106 - accuracy: 0.0000e+00 - val_loss: 0.4286 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9101 - accuracy: 0.0000e+00 - val_loss: 0.4286 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9096 - accuracy: 0.0000e+00 - val_loss: 0.4287 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9091 - accuracy: 0.0000e+00 - val_loss: 0.4288 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9087 - accuracy: 0.0000e+00 - val_loss: 0.4289 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9083 - accuracy: 0.0000e+00 - val_loss: 0.4290 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9077 - accuracy: 0.0000e+00 - val_loss: 0.4291 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9075 - accuracy: 0.0000e+00 - val_loss: 0.4292 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9065 - accuracy: 0.0000e+00 - val_loss: 0.4293 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9062 - accuracy: 0.0000e+00 - val_loss: 0.4294 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9057 - accuracy: 0.0000e+00 - val_loss: 0.4295 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9051 - accuracy: 0.0000e+00 - val_loss: 0.4297 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9047 - accuracy: 0.0000e+00 - val_loss: 0.4299 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9044 - accuracy: 0.0000e+00 - val_loss: 0.4301 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9038 - accuracy: 0.0000e+00 - val_loss: 0.4301 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9034 - accuracy: 0.0000e+00 - val_loss: 0.4303 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.4304 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9023 - accuracy: 0.0000e+00 - val_loss: 0.4306 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9016 - accuracy: 0.0000e+00 - val_loss: 0.4307 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9016 - accuracy: 0.0000e+00 - val_loss: 0.4309 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9010 - accuracy: 0.0000e+00 - val_loss: 0.4310 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.4312 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8998 - accuracy: 0.0000e+00 - val_loss: 0.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8995 - accuracy: 0.0000e+00 - val_loss: 0.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8988 - accuracy: 0.0000e+00 - val_loss: 0.4314 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8984 - accuracy: 0.0000e+00 - val_loss: 0.4316 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8975 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8973 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8968 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8963 - accuracy: 0.0000e+00 - val_loss: 0.4323 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8957 - accuracy: 0.0000e+00 - val_loss: 0.4324 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8954 - accuracy: 0.0000e+00 - val_loss: 0.4326 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8950 - accuracy: 0.0000e+00 - val_loss: 0.4327 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8945 - accuracy: 0.0000e+00 - val_loss: 0.4328 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8941 - accuracy: 0.0000e+00 - val_loss: 0.4330 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8938 - accuracy: 0.0000e+00 - val_loss: 0.4332 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8932 - accuracy: 0.0000e+00 - val_loss: 0.4334 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8929 - accuracy: 0.0000e+00 - val_loss: 0.4335 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8926 - accuracy: 0.0000e+00 - val_loss: 0.4336 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8921 - accuracy: 0.0000e+00 - val_loss: 0.4337 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8917 - accuracy: 0.0000e+00 - val_loss: 0.4339 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.4339 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8910 - accuracy: 0.0000e+00 - val_loss: 0.4341 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8906 - accuracy: 0.0000e+00 - val_loss: 0.4342 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8908 - accuracy: 0.0000e+00 - val_loss: 0.4345 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8900 - accuracy: 0.0000e+00 - val_loss: 0.4347 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8899 - accuracy: 0.0000e+00 - val_loss: 0.4348 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8896 - accuracy: 0.0000e+00 - val_loss: 0.4351 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8890 - accuracy: 0.0000e+00 - val_loss: 0.4353 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8889 - accuracy: 0.0000e+00 - val_loss: 0.4354 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8884 - accuracy: 0.0000e+00 - val_loss: 0.4355 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.4356 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8877 - accuracy: 0.0000e+00 - val_loss: 0.4357 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8874 - accuracy: 0.0000e+00 - val_loss: 0.4359 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8871 - accuracy: 0.0000e+00 - val_loss: 0.4361 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8868 - accuracy: 0.0000e+00 - val_loss: 0.4363 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8865 - accuracy: 0.0000e+00 - val_loss: 0.4366 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8863 - accuracy: 0.0000e+00 - val_loss: 0.4367 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.4369 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.4369 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8854 - accuracy: 0.0000e+00 - val_loss: 0.4371 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8851 - accuracy: 0.0000e+00 - val_loss: 0.4372 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.4373 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8846 - accuracy: 0.0000e+00 - val_loss: 0.4374 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.4375 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8841 - accuracy: 0.0000e+00 - val_loss: 0.4378 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8838 - accuracy: 0.0000e+00 - val_loss: 0.4380 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8835 - accuracy: 0.0000e+00 - val_loss: 0.4382 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8834 - accuracy: 0.0000e+00 - val_loss: 0.4384 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8831 - accuracy: 0.0000e+00 - val_loss: 0.4386 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8829 - accuracy: 0.0000e+00 - val_loss: 0.4386 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8827 - accuracy: 0.0000e+00 - val_loss: 0.4387 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8824 - accuracy: 0.0000e+00 - val_loss: 0.4387 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8822 - accuracy: 0.0000e+00 - val_loss: 0.4389 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8820 - accuracy: 0.0000e+00 - val_loss: 0.4390 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/T_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4390 - accuracy: 0.0000e+00\n",
      "[0.4389796555042267, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_52 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.4292 - accuracy: 0.0000e+00 - val_loss: 0.4886 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.4099 - accuracy: 0.0000e+00 - val_loss: 0.4862 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3871 - accuracy: 0.0000e+00 - val_loss: 0.4840 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.3644 - accuracy: 0.0000e+00 - val_loss: 0.4823 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3463 - accuracy: 0.0000e+00 - val_loss: 0.4804 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.3245 - accuracy: 0.0000e+00 - val_loss: 0.4785 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.3087 - accuracy: 0.0000e+00 - val_loss: 0.4780 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2904 - accuracy: 0.0000e+00 - val_loss: 0.4764 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.2701 - accuracy: 0.0000e+00 - val_loss: 0.4758 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2525 - accuracy: 0.0000e+00 - val_loss: 0.4747 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2335 - accuracy: 0.0000e+00 - val_loss: 0.4736 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2185 - accuracy: 0.0000e+00 - val_loss: 0.4732 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2020 - accuracy: 0.0000e+00 - val_loss: 0.4728 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1852 - accuracy: 0.0000e+00 - val_loss: 0.4726 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1695 - accuracy: 0.0000e+00 - val_loss: 0.4733 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1523 - accuracy: 0.0000e+00 - val_loss: 0.4736 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1394 - accuracy: 0.0000e+00 - val_loss: 0.4747 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1241 - accuracy: 0.0000e+00 - val_loss: 0.4750 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1131 - accuracy: 0.0000e+00 - val_loss: 0.4762 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1028 - accuracy: 0.0000e+00 - val_loss: 0.4773 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0932 - accuracy: 0.0000e+00 - val_loss: 0.4789 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0846 - accuracy: 0.0000e+00 - val_loss: 0.4805 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0757 - accuracy: 0.0000e+00 - val_loss: 0.4821 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0677 - accuracy: 0.0000e+00 - val_loss: 0.4832 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0613 - accuracy: 0.0000e+00 - val_loss: 0.4851 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0554 - accuracy: 0.0000e+00 - val_loss: 0.4868 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0492 - accuracy: 0.0000e+00 - val_loss: 0.4883 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0443 - accuracy: 0.0000e+00 - val_loss: 0.4904 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0377 - accuracy: 0.0000e+00 - val_loss: 0.4923 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0333 - accuracy: 0.0000e+00 - val_loss: 0.4937 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0281 - accuracy: 0.0000e+00 - val_loss: 0.4956 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0231 - accuracy: 0.0000e+00 - val_loss: 0.4977 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0194 - accuracy: 0.0000e+00 - val_loss: 0.4996 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0153 - accuracy: 0.0000e+00 - val_loss: 0.5013 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0106 - accuracy: 0.0000e+00 - val_loss: 0.5035 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0077 - accuracy: 0.0000e+00 - val_loss: 0.5058 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0043 - accuracy: 0.0000e+00 - val_loss: 0.5074 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0012 - accuracy: 0.0000e+00 - val_loss: 0.5100 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9974 - accuracy: 0.0000e+00 - val_loss: 0.5119 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9949 - accuracy: 0.0000e+00 - val_loss: 0.5142 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9919 - accuracy: 0.0000e+00 - val_loss: 0.5161 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9893 - accuracy: 0.0000e+00 - val_loss: 0.5190 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9869 - accuracy: 0.0000e+00 - val_loss: 0.5214 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9845 - accuracy: 0.0000e+00 - val_loss: 0.5242 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9821 - accuracy: 0.0000e+00 - val_loss: 0.5264 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9796 - accuracy: 0.0000e+00 - val_loss: 0.5282 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9775 - accuracy: 0.0000e+00 - val_loss: 0.5313 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9753 - accuracy: 0.0000e+00 - val_loss: 0.5336 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9729 - accuracy: 0.0000e+00 - val_loss: 0.5355 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9710 - accuracy: 0.0000e+00 - val_loss: 0.5377 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9692 - accuracy: 0.0000e+00 - val_loss: 0.5406 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9672 - accuracy: 0.0000e+00 - val_loss: 0.5431 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9657 - accuracy: 0.0000e+00 - val_loss: 0.5460 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9633 - accuracy: 0.0000e+00 - val_loss: 0.5481 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9612 - accuracy: 0.0000e+00 - val_loss: 0.5499 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9596 - accuracy: 0.0000e+00 - val_loss: 0.5519 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9577 - accuracy: 0.0000e+00 - val_loss: 0.5541 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9561 - accuracy: 0.0000e+00 - val_loss: 0.5557 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9542 - accuracy: 0.0000e+00 - val_loss: 0.5579 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9526 - accuracy: 0.0000e+00 - val_loss: 0.5603 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9509 - accuracy: 0.0000e+00 - val_loss: 0.5615 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9494 - accuracy: 0.0000e+00 - val_loss: 0.5641 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9477 - accuracy: 0.0000e+00 - val_loss: 0.5651 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9465 - accuracy: 0.0000e+00 - val_loss: 0.5675 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9450 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9435 - accuracy: 0.0000e+00 - val_loss: 0.5710 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9421 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9405 - accuracy: 0.0000e+00 - val_loss: 0.5744 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9394 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9380 - accuracy: 0.0000e+00 - val_loss: 0.5775 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9365 - accuracy: 0.0000e+00 - val_loss: 0.5790 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9355 - accuracy: 0.0000e+00 - val_loss: 0.5796 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9338 - accuracy: 0.0000e+00 - val_loss: 0.5812 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9330 - accuracy: 0.0000e+00 - val_loss: 0.5830 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9316 - accuracy: 0.0000e+00 - val_loss: 0.5846 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9303 - accuracy: 0.0000e+00 - val_loss: 0.5859 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.5872 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9276 - accuracy: 0.0000e+00 - val_loss: 0.5881 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9267 - accuracy: 0.0000e+00 - val_loss: 0.5893 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9252 - accuracy: 0.0000e+00 - val_loss: 0.5910 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9241 - accuracy: 0.0000e+00 - val_loss: 0.5923 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9229 - accuracy: 0.0000e+00 - val_loss: 0.5928 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9216 - accuracy: 0.0000e+00 - val_loss: 0.5943 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9205 - accuracy: 0.0000e+00 - val_loss: 0.5953 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9193 - accuracy: 0.0000e+00 - val_loss: 0.5967 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9182 - accuracy: 0.0000e+00 - val_loss: 0.5982 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9170 - accuracy: 0.0000e+00 - val_loss: 0.5984 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9160 - accuracy: 0.0000e+00 - val_loss: 0.5999 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9150 - accuracy: 0.0000e+00 - val_loss: 0.6013 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9137 - accuracy: 0.0000e+00 - val_loss: 0.6017 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9126 - accuracy: 0.0000e+00 - val_loss: 0.6028 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9116 - accuracy: 0.0000e+00 - val_loss: 0.6038 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9105 - accuracy: 0.0000e+00 - val_loss: 0.6045 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9096 - accuracy: 0.0000e+00 - val_loss: 0.6062 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9085 - accuracy: 0.0000e+00 - val_loss: 0.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9074 - accuracy: 0.0000e+00 - val_loss: 0.6082 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9064 - accuracy: 0.0000e+00 - val_loss: 0.6088 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9055 - accuracy: 0.0000e+00 - val_loss: 0.6096 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9045 - accuracy: 0.0000e+00 - val_loss: 0.6104 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9035 - accuracy: 0.0000e+00 - val_loss: 0.6109 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9025 - accuracy: 0.0000e+00 - val_loss: 0.6121 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9015 - accuracy: 0.0000e+00 - val_loss: 0.6126 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.6131 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8995 - accuracy: 0.0000e+00 - val_loss: 0.6140 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8986 - accuracy: 0.0000e+00 - val_loss: 0.6149 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8976 - accuracy: 0.0000e+00 - val_loss: 0.6157 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8972 - accuracy: 0.0000e+00 - val_loss: 0.6155 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8959 - accuracy: 0.0000e+00 - val_loss: 0.6163 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8948 - accuracy: 0.0000e+00 - val_loss: 0.6170 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8939 - accuracy: 0.0000e+00 - val_loss: 0.6182 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8932 - accuracy: 0.0000e+00 - val_loss: 0.6181 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8923 - accuracy: 0.0000e+00 - val_loss: 0.6186 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.6189 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8902 - accuracy: 0.0000e+00 - val_loss: 0.6197 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8895 - accuracy: 0.0000e+00 - val_loss: 0.6200 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8885 - accuracy: 0.0000e+00 - val_loss: 0.6207 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8876 - accuracy: 0.0000e+00 - val_loss: 0.6207 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8866 - accuracy: 0.0000e+00 - val_loss: 0.6211 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8859 - accuracy: 0.0000e+00 - val_loss: 0.6213 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.6212 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8839 - accuracy: 0.0000e+00 - val_loss: 0.6209 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8833 - accuracy: 0.0000e+00 - val_loss: 0.6211 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8824 - accuracy: 0.0000e+00 - val_loss: 0.6211 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - val_loss: 0.6210 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8806 - accuracy: 0.0000e+00 - val_loss: 0.6204 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8798 - accuracy: 0.0000e+00 - val_loss: 0.6212 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8791 - accuracy: 0.0000e+00 - val_loss: 0.6212 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8780 - accuracy: 0.0000e+00 - val_loss: 0.6218 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8772 - accuracy: 0.0000e+00 - val_loss: 0.6214 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8766 - accuracy: 0.0000e+00 - val_loss: 0.6205 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.6206 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8748 - accuracy: 0.0000e+00 - val_loss: 0.6205 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.6210 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8727 - accuracy: 0.0000e+00 - val_loss: 0.6208 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8722 - accuracy: 0.0000e+00 - val_loss: 0.6203 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.6209 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8704 - accuracy: 0.0000e+00 - val_loss: 0.6214 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8697 - accuracy: 0.0000e+00 - val_loss: 0.6208 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8689 - accuracy: 0.0000e+00 - val_loss: 0.6201 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8680 - accuracy: 0.0000e+00 - val_loss: 0.6203 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8672 - accuracy: 0.0000e+00 - val_loss: 0.6203 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8664 - accuracy: 0.0000e+00 - val_loss: 0.6210 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8656 - accuracy: 0.0000e+00 - val_loss: 0.6214 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8649 - accuracy: 0.0000e+00 - val_loss: 0.6215 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.6222 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8633 - accuracy: 0.0000e+00 - val_loss: 0.6223 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8628 - accuracy: 0.0000e+00 - val_loss: 0.6226 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8619 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8614 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8605 - accuracy: 0.0000e+00 - val_loss: 0.6244 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8598 - accuracy: 0.0000e+00 - val_loss: 0.6255 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8590 - accuracy: 0.0000e+00 - val_loss: 0.6259 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8584 - accuracy: 0.0000e+00 - val_loss: 0.6267 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8577 - accuracy: 0.0000e+00 - val_loss: 0.6276 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8571 - accuracy: 0.0000e+00 - val_loss: 0.6281 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8564 - accuracy: 0.0000e+00 - val_loss: 0.6289 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8557 - accuracy: 0.0000e+00 - val_loss: 0.6298 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8550 - accuracy: 0.0000e+00 - val_loss: 0.6305 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8543 - accuracy: 0.0000e+00 - val_loss: 0.6310 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8538 - accuracy: 0.0000e+00 - val_loss: 0.6320 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8534 - accuracy: 0.0000e+00 - val_loss: 0.6325 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.6337 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8521 - accuracy: 0.0000e+00 - val_loss: 0.6342 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8516 - accuracy: 0.0000e+00 - val_loss: 0.6350 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8509 - accuracy: 0.0000e+00 - val_loss: 0.6359 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8504 - accuracy: 0.0000e+00 - val_loss: 0.6369 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8498 - accuracy: 0.0000e+00 - val_loss: 0.6376 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8494 - accuracy: 0.0000e+00 - val_loss: 0.6380 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8487 - accuracy: 0.0000e+00 - val_loss: 0.6391 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8481 - accuracy: 0.0000e+00 - val_loss: 0.6398 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8475 - accuracy: 0.0000e+00 - val_loss: 0.6404 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8471 - accuracy: 0.0000e+00 - val_loss: 0.6411 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8465 - accuracy: 0.0000e+00 - val_loss: 0.6418 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8461 - accuracy: 0.0000e+00 - val_loss: 0.6426 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8456 - accuracy: 0.0000e+00 - val_loss: 0.6433 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8449 - accuracy: 0.0000e+00 - val_loss: 0.6437 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8446 - accuracy: 0.0000e+00 - val_loss: 0.6440 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8441 - accuracy: 0.0000e+00 - val_loss: 0.6446 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8436 - accuracy: 0.0000e+00 - val_loss: 0.6453 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8431 - accuracy: 0.0000e+00 - val_loss: 0.6458 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8427 - accuracy: 0.0000e+00 - val_loss: 0.6466 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.6469 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8419 - accuracy: 0.0000e+00 - val_loss: 0.6471 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8415 - accuracy: 0.0000e+00 - val_loss: 0.6481 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8408 - accuracy: 0.0000e+00 - val_loss: 0.6486 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8404 - accuracy: 0.0000e+00 - val_loss: 0.6491 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8400 - accuracy: 0.0000e+00 - val_loss: 0.6496 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8396 - accuracy: 0.0000e+00 - val_loss: 0.6499 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8391 - accuracy: 0.0000e+00 - val_loss: 0.6503 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8387 - accuracy: 0.0000e+00 - val_loss: 0.6508 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8383 - accuracy: 0.0000e+00 - val_loss: 0.6509 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8379 - accuracy: 0.0000e+00 - val_loss: 0.6515 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8375 - accuracy: 0.0000e+00 - val_loss: 0.6521 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.6522 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8368 - accuracy: 0.0000e+00 - val_loss: 0.6529 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8363 - accuracy: 0.0000e+00 - val_loss: 0.6532 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8361 - accuracy: 0.0000e+00 - val_loss: 0.6539 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8357 - accuracy: 0.0000e+00 - val_loss: 0.6544 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8352 - accuracy: 0.0000e+00 - val_loss: 0.6546 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8348 - accuracy: 0.0000e+00 - val_loss: 0.6550 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.6553 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8342 - accuracy: 0.0000e+00 - val_loss: 0.6557 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8338 - accuracy: 0.0000e+00 - val_loss: 0.6561 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8334 - accuracy: 0.0000e+00 - val_loss: 0.6563 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.6566 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8329 - accuracy: 0.0000e+00 - val_loss: 0.6569 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8324 - accuracy: 0.0000e+00 - val_loss: 0.6571 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8321 - accuracy: 0.0000e+00 - val_loss: 0.6575 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8317 - accuracy: 0.0000e+00 - val_loss: 0.6577 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8314 - accuracy: 0.0000e+00 - val_loss: 0.6582 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.6583 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8308 - accuracy: 0.0000e+00 - val_loss: 0.6588 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.6589 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8303 - accuracy: 0.0000e+00 - val_loss: 0.6593 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8299 - accuracy: 0.0000e+00 - val_loss: 0.6596 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8297 - accuracy: 0.0000e+00 - val_loss: 0.6600 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8294 - accuracy: 0.0000e+00 - val_loss: 0.6605 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8291 - accuracy: 0.0000e+00 - val_loss: 0.6610 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8289 - accuracy: 0.0000e+00 - val_loss: 0.6613 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8286 - accuracy: 0.0000e+00 - val_loss: 0.6611 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8283 - accuracy: 0.0000e+00 - val_loss: 0.6613 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8280 - accuracy: 0.0000e+00 - val_loss: 0.6619 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8278 - accuracy: 0.0000e+00 - val_loss: 0.6623 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8275 - accuracy: 0.0000e+00 - val_loss: 0.6627 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8272 - accuracy: 0.0000e+00 - val_loss: 0.6629 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8270 - accuracy: 0.0000e+00 - val_loss: 0.6630 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8268 - accuracy: 0.0000e+00 - val_loss: 0.6633 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8265 - accuracy: 0.0000e+00 - val_loss: 0.6636 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.6638 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8261 - accuracy: 0.0000e+00 - val_loss: 0.6642 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8258 - accuracy: 0.0000e+00 - val_loss: 0.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8256 - accuracy: 0.0000e+00 - val_loss: 0.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8253 - accuracy: 0.0000e+00 - val_loss: 0.6647 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8251 - accuracy: 0.0000e+00 - val_loss: 0.6650 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8249 - accuracy: 0.0000e+00 - val_loss: 0.6652 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8247 - accuracy: 0.0000e+00 - val_loss: 0.6654 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8246 - accuracy: 0.0000e+00 - val_loss: 0.6659 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8243 - accuracy: 0.0000e+00 - val_loss: 0.6660 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8241 - accuracy: 0.0000e+00 - val_loss: 0.6660 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8239 - accuracy: 0.0000e+00 - val_loss: 0.6661 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8237 - accuracy: 0.0000e+00 - val_loss: 0.6664 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8235 - accuracy: 0.0000e+00 - val_loss: 0.6669 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8233 - accuracy: 0.0000e+00 - val_loss: 0.6675 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8231 - accuracy: 0.0000e+00 - val_loss: 0.6675 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8229 - accuracy: 0.0000e+00 - val_loss: 0.6676 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8227 - accuracy: 0.0000e+00 - val_loss: 0.6677 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8226 - accuracy: 0.0000e+00 - val_loss: 0.6680 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 0.6684 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8222 - accuracy: 0.0000e+00 - val_loss: 0.6684 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8220 - accuracy: 0.0000e+00 - val_loss: 0.6686 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8218 - accuracy: 0.0000e+00 - val_loss: 0.6689 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8217 - accuracy: 0.0000e+00 - val_loss: 0.6690 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8215 - accuracy: 0.0000e+00 - val_loss: 0.6692 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8214 - accuracy: 0.0000e+00 - val_loss: 0.6692 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8212 - accuracy: 0.0000e+00 - val_loss: 0.6695 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8210 - accuracy: 0.0000e+00 - val_loss: 0.6698 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/UNP_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6698 - accuracy: 0.0000e+00\n",
      "[0.6698215007781982, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_54 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.0998 - accuracy: 0.0000e+00 - val_loss: 0.5321 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0848 - accuracy: 0.0000e+00 - val_loss: 0.5362 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0754 - accuracy: 0.0000e+00 - val_loss: 0.5412 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0625 - accuracy: 0.0000e+00 - val_loss: 0.5452 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0528 - accuracy: 0.0000e+00 - val_loss: 0.5479 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0450 - accuracy: 0.0000e+00 - val_loss: 0.5380 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0351 - accuracy: 0.0000e+00 - val_loss: 0.5150 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0273 - accuracy: 0.0000e+00 - val_loss: 0.5119 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0193 - accuracy: 0.0000e+00 - val_loss: 0.5147 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0123 - accuracy: 0.0000e+00 - val_loss: 0.5173 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0068 - accuracy: 0.0000e+00 - val_loss: 0.5201 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0003 - accuracy: 0.0000e+00 - val_loss: 0.5228 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9936 - accuracy: 0.0000e+00 - val_loss: 0.5245 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9896 - accuracy: 0.0000e+00 - val_loss: 0.5266 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9843 - accuracy: 0.0000e+00 - val_loss: 0.5285 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9794 - accuracy: 0.0000e+00 - val_loss: 0.5307 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9753 - accuracy: 0.0000e+00 - val_loss: 0.5332 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9712 - accuracy: 0.0000e+00 - val_loss: 0.5354 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9679 - accuracy: 0.0000e+00 - val_loss: 0.5371 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9638 - accuracy: 0.0000e+00 - val_loss: 0.5391 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9602 - accuracy: 0.0000e+00 - val_loss: 0.5407 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9568 - accuracy: 0.0000e+00 - val_loss: 0.5427 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9539 - accuracy: 0.0000e+00 - val_loss: 0.5447 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9506 - accuracy: 0.0000e+00 - val_loss: 0.5463 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9475 - accuracy: 0.0000e+00 - val_loss: 0.5484 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9440 - accuracy: 0.0000e+00 - val_loss: 0.5497 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9413 - accuracy: 0.0000e+00 - val_loss: 0.5508 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9390 - accuracy: 0.0000e+00 - val_loss: 0.5522 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9366 - accuracy: 0.0000e+00 - val_loss: 0.5531 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9339 - accuracy: 0.0000e+00 - val_loss: 0.5545 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9313 - accuracy: 0.0000e+00 - val_loss: 0.5560 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9293 - accuracy: 0.0000e+00 - val_loss: 0.5575 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9273 - accuracy: 0.0000e+00 - val_loss: 0.5589 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9253 - accuracy: 0.0000e+00 - val_loss: 0.5594 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9231 - accuracy: 0.0000e+00 - val_loss: 0.5604 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9212 - accuracy: 0.0000e+00 - val_loss: 0.5614 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9197 - accuracy: 0.0000e+00 - val_loss: 0.5627 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9177 - accuracy: 0.0000e+00 - val_loss: 0.5634 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9160 - accuracy: 0.0000e+00 - val_loss: 0.5641 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9145 - accuracy: 0.0000e+00 - val_loss: 0.5649 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9128 - accuracy: 0.0000e+00 - val_loss: 0.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9113 - accuracy: 0.0000e+00 - val_loss: 0.5663 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9098 - accuracy: 0.0000e+00 - val_loss: 0.5665 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9085 - accuracy: 0.0000e+00 - val_loss: 0.5676 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9071 - accuracy: 0.0000e+00 - val_loss: 0.5676 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9059 - accuracy: 0.0000e+00 - val_loss: 0.5685 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9046 - accuracy: 0.0000e+00 - val_loss: 0.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9034 - accuracy: 0.0000e+00 - val_loss: 0.5703 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9021 - accuracy: 0.0000e+00 - val_loss: 0.5710 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9012 - accuracy: 0.0000e+00 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9000 - accuracy: 0.0000e+00 - val_loss: 0.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8990 - accuracy: 0.0000e+00 - val_loss: 0.5724 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8980 - accuracy: 0.0000e+00 - val_loss: 0.5726 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8969 - accuracy: 0.0000e+00 - val_loss: 0.5731 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8961 - accuracy: 0.0000e+00 - val_loss: 0.5730 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8950 - accuracy: 0.0000e+00 - val_loss: 0.5739 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8939 - accuracy: 0.0000e+00 - val_loss: 0.5742 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8932 - accuracy: 0.0000e+00 - val_loss: 0.5745 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8922 - accuracy: 0.0000e+00 - val_loss: 0.5754 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8914 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8905 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8897 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8889 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8880 - accuracy: 0.0000e+00 - val_loss: 0.5763 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8873 - accuracy: 0.0000e+00 - val_loss: 0.5766 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8864 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8835 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8830 - accuracy: 0.0000e+00 - val_loss: 0.5770 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8821 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8815 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8808 - accuracy: 0.0000e+00 - val_loss: 0.5770 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8800 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8794 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8787 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8776 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8762 - accuracy: 0.0000e+00 - val_loss: 0.5770 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8756 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8749 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8726 - accuracy: 0.0000e+00 - val_loss: 0.5779 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8718 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8707 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8700 - accuracy: 0.0000e+00 - val_loss: 0.5779 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.5778 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8690 - accuracy: 0.0000e+00 - val_loss: 0.5779 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8684 - accuracy: 0.0000e+00 - val_loss: 0.5777 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8678 - accuracy: 0.0000e+00 - val_loss: 0.5777 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8675 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8669 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8665 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8658 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8648 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8643 - accuracy: 0.0000e+00 - val_loss: 0.5770 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8637 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8633 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8629 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8623 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8614 - accuracy: 0.0000e+00 - val_loss: 0.5768 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8609 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8604 - accuracy: 0.0000e+00 - val_loss: 0.5768 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8599 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8594 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8590 - accuracy: 0.0000e+00 - val_loss: 0.5765 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8585 - accuracy: 0.0000e+00 - val_loss: 0.5763 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8581 - accuracy: 0.0000e+00 - val_loss: 0.5762 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8577 - accuracy: 0.0000e+00 - val_loss: 0.5758 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8571 - accuracy: 0.0000e+00 - val_loss: 0.5760 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8569 - accuracy: 0.0000e+00 - val_loss: 0.5753 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8563 - accuracy: 0.0000e+00 - val_loss: 0.5752 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8561 - accuracy: 0.0000e+00 - val_loss: 0.5750 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8556 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.5762 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8550 - accuracy: 0.0000e+00 - val_loss: 0.5762 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8546 - accuracy: 0.0000e+00 - val_loss: 0.5762 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8542 - accuracy: 0.0000e+00 - val_loss: 0.5765 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.5767 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.5768 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8532 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8528 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8525 - accuracy: 0.0000e+00 - val_loss: 0.5780 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8522 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8519 - accuracy: 0.0000e+00 - val_loss: 0.5780 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8515 - accuracy: 0.0000e+00 - val_loss: 0.5785 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8513 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8509 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8503 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8500 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8497 - accuracy: 0.0000e+00 - val_loss: 0.5789 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8493 - accuracy: 0.0000e+00 - val_loss: 0.5790 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8491 - accuracy: 0.0000e+00 - val_loss: 0.5789 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8488 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8485 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8483 - accuracy: 0.0000e+00 - val_loss: 0.5789 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8479 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8477 - accuracy: 0.0000e+00 - val_loss: 0.5790 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8473 - accuracy: 0.0000e+00 - val_loss: 0.5795 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8471 - accuracy: 0.0000e+00 - val_loss: 0.5798 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8467 - accuracy: 0.0000e+00 - val_loss: 0.5795 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8464 - accuracy: 0.0000e+00 - val_loss: 0.5790 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8461 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8459 - accuracy: 0.0000e+00 - val_loss: 0.5796 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8457 - accuracy: 0.0000e+00 - val_loss: 0.5792 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8453 - accuracy: 0.0000e+00 - val_loss: 0.5790 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8450 - accuracy: 0.0000e+00 - val_loss: 0.5791 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8447 - accuracy: 0.0000e+00 - val_loss: 0.5794 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8446 - accuracy: 0.0000e+00 - val_loss: 0.5793 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8442 - accuracy: 0.0000e+00 - val_loss: 0.5792 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8440 - accuracy: 0.0000e+00 - val_loss: 0.5795 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8436 - accuracy: 0.0000e+00 - val_loss: 0.5792 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8435 - accuracy: 0.0000e+00 - val_loss: 0.5792 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8433 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8429 - accuracy: 0.0000e+00 - val_loss: 0.5791 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8428 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.5787 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8422 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8420 - accuracy: 0.0000e+00 - val_loss: 0.5786 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8417 - accuracy: 0.0000e+00 - val_loss: 0.5788 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8415 - accuracy: 0.0000e+00 - val_loss: 0.5791 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8412 - accuracy: 0.0000e+00 - val_loss: 0.5787 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8410 - accuracy: 0.0000e+00 - val_loss: 0.5785 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8407 - accuracy: 0.0000e+00 - val_loss: 0.5785 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8405 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8403 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8400 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8398 - accuracy: 0.0000e+00 - val_loss: 0.5783 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8396 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8393 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8392 - accuracy: 0.0000e+00 - val_loss: 0.5782 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8390 - accuracy: 0.0000e+00 - val_loss: 0.5784 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8388 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8385 - accuracy: 0.0000e+00 - val_loss: 0.5777 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8383 - accuracy: 0.0000e+00 - val_loss: 0.5779 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8381 - accuracy: 0.0000e+00 - val_loss: 0.5786 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8378 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8377 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8374 - accuracy: 0.0000e+00 - val_loss: 0.5774 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8372 - accuracy: 0.0000e+00 - val_loss: 0.5775 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8370 - accuracy: 0.0000e+00 - val_loss: 0.5777 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8368 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8366 - accuracy: 0.0000e+00 - val_loss: 0.5776 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8364 - accuracy: 0.0000e+00 - val_loss: 0.5773 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8362 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8361 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8359 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8356 - accuracy: 0.0000e+00 - val_loss: 0.5769 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8354 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8353 - accuracy: 0.0000e+00 - val_loss: 0.5768 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8350 - accuracy: 0.0000e+00 - val_loss: 0.5771 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8349 - accuracy: 0.0000e+00 - val_loss: 0.5767 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8347 - accuracy: 0.0000e+00 - val_loss: 0.5760 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8345 - accuracy: 0.0000e+00 - val_loss: 0.5760 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8343 - accuracy: 0.0000e+00 - val_loss: 0.5764 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8341 - accuracy: 0.0000e+00 - val_loss: 0.5760 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8340 - accuracy: 0.0000e+00 - val_loss: 0.5757 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8338 - accuracy: 0.0000e+00 - val_loss: 0.5756 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8336 - accuracy: 0.0000e+00 - val_loss: 0.5750 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8334 - accuracy: 0.0000e+00 - val_loss: 0.5750 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8333 - accuracy: 0.0000e+00 - val_loss: 0.5748 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.5742 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8328 - accuracy: 0.0000e+00 - val_loss: 0.5744 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8328 - accuracy: 0.0000e+00 - val_loss: 0.5744 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.5745 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8324 - accuracy: 0.0000e+00 - val_loss: 0.5741 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8322 - accuracy: 0.0000e+00 - val_loss: 0.5740 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8321 - accuracy: 0.0000e+00 - val_loss: 0.5737 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8319 - accuracy: 0.0000e+00 - val_loss: 0.5736 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8317 - accuracy: 0.0000e+00 - val_loss: 0.5737 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8315 - accuracy: 0.0000e+00 - val_loss: 0.5733 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8314 - accuracy: 0.0000e+00 - val_loss: 0.5729 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8309 - accuracy: 0.0000e+00 - val_loss: 0.5718 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8308 - accuracy: 0.0000e+00 - val_loss: 0.5714 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8305 - accuracy: 0.0000e+00 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8304 - accuracy: 0.0000e+00 - val_loss: 0.5711 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8302 - accuracy: 0.0000e+00 - val_loss: 0.5713 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8301 - accuracy: 0.0000e+00 - val_loss: 0.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8299 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8297 - accuracy: 0.0000e+00 - val_loss: 0.5706 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8296 - accuracy: 0.0000e+00 - val_loss: 0.5701 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8294 - accuracy: 0.0000e+00 - val_loss: 0.5702 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8293 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8291 - accuracy: 0.0000e+00 - val_loss: 0.5694 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8289 - accuracy: 0.0000e+00 - val_loss: 0.5692 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8288 - accuracy: 0.0000e+00 - val_loss: 0.5693 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8286 - accuracy: 0.0000e+00 - val_loss: 0.5688 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8285 - accuracy: 0.0000e+00 - val_loss: 0.5699 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8283 - accuracy: 0.0000e+00 - val_loss: 0.5719 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8282 - accuracy: 0.0000e+00 - val_loss: 0.5717 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8280 - accuracy: 0.0000e+00 - val_loss: 0.5735 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8279 - accuracy: 0.0000e+00 - val_loss: 0.5747 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8277 - accuracy: 0.0000e+00 - val_loss: 0.5825 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8276 - accuracy: 0.0000e+00 - val_loss: 0.5755 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8274 - accuracy: 0.0000e+00 - val_loss: 0.5781 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8273 - accuracy: 0.0000e+00 - val_loss: 0.5881 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8271 - accuracy: 0.0000e+00 - val_loss: 0.5869 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8270 - accuracy: 0.0000e+00 - val_loss: 0.6006 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8268 - accuracy: 0.0000e+00 - val_loss: 0.6016 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8267 - accuracy: 0.0000e+00 - val_loss: 0.5850 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8266 - accuracy: 0.0000e+00 - val_loss: 0.6011 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8264 - accuracy: 0.0000e+00 - val_loss: 0.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.6083 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8261 - accuracy: 0.0000e+00 - val_loss: 0.6048 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8260 - accuracy: 0.0000e+00 - val_loss: 0.6042 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8258 - accuracy: 0.0000e+00 - val_loss: 0.6094 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/IBM_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6094 - accuracy: 0.0000e+00\n",
      "[0.6093510389328003, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_56 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.1600 - accuracy: 0.0000e+00 - val_loss: 0.4023 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1355 - accuracy: 0.0000e+00 - val_loss: 0.4017 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1162 - accuracy: 0.0000e+00 - val_loss: 0.4023 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0964 - accuracy: 0.0000e+00 - val_loss: 0.4023 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0836 - accuracy: 0.0000e+00 - val_loss: 0.4045 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0679 - accuracy: 0.0000e+00 - val_loss: 0.4064 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0552 - accuracy: 0.0000e+00 - val_loss: 0.4086 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0448 - accuracy: 0.0000e+00 - val_loss: 0.4119 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0352 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0259 - accuracy: 0.0000e+00 - val_loss: 0.4168 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0192 - accuracy: 0.0000e+00 - val_loss: 0.4198 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0133 - accuracy: 0.0000e+00 - val_loss: 0.4229 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0087 - accuracy: 0.0000e+00 - val_loss: 0.4271 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0027 - accuracy: 0.0000e+00 - val_loss: 0.4301 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9983 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9946 - accuracy: 0.0000e+00 - val_loss: 0.4349 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9917 - accuracy: 0.0000e+00 - val_loss: 0.4388 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9871 - accuracy: 0.0000e+00 - val_loss: 0.4415 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9839 - accuracy: 0.0000e+00 - val_loss: 0.4434 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9811 - accuracy: 0.0000e+00 - val_loss: 0.4464 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9782 - accuracy: 0.0000e+00 - val_loss: 0.4488 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9757 - accuracy: 0.0000e+00 - val_loss: 0.4519 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9729 - accuracy: 0.0000e+00 - val_loss: 0.4540 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9706 - accuracy: 0.0000e+00 - val_loss: 0.4566 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9685 - accuracy: 0.0000e+00 - val_loss: 0.4583 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9663 - accuracy: 0.0000e+00 - val_loss: 0.4606 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9644 - accuracy: 0.0000e+00 - val_loss: 0.4635 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9622 - accuracy: 0.0000e+00 - val_loss: 0.4647 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9606 - accuracy: 0.0000e+00 - val_loss: 0.4669 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9587 - accuracy: 0.0000e+00 - val_loss: 0.4689 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9577 - accuracy: 0.0000e+00 - val_loss: 0.4714 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9557 - accuracy: 0.0000e+00 - val_loss: 0.4721 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9546 - accuracy: 0.0000e+00 - val_loss: 0.4738 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9531 - accuracy: 0.0000e+00 - val_loss: 0.4749 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9517 - accuracy: 0.0000e+00 - val_loss: 0.4758 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9506 - accuracy: 0.0000e+00 - val_loss: 0.4775 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9495 - accuracy: 0.0000e+00 - val_loss: 0.4783 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9481 - accuracy: 0.0000e+00 - val_loss: 0.4797 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9470 - accuracy: 0.0000e+00 - val_loss: 0.4813 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9459 - accuracy: 0.0000e+00 - val_loss: 0.4820 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9449 - accuracy: 0.0000e+00 - val_loss: 0.4820 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9436 - accuracy: 0.0000e+00 - val_loss: 0.4826 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9426 - accuracy: 0.0000e+00 - val_loss: 0.4836 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9417 - accuracy: 0.0000e+00 - val_loss: 0.4847 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9406 - accuracy: 0.0000e+00 - val_loss: 0.4848 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9396 - accuracy: 0.0000e+00 - val_loss: 0.4847 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9385 - accuracy: 0.0000e+00 - val_loss: 0.4848 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9377 - accuracy: 0.0000e+00 - val_loss: 0.4841 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9364 - accuracy: 0.0000e+00 - val_loss: 0.4841 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9355 - accuracy: 0.0000e+00 - val_loss: 0.4844 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9346 - accuracy: 0.0000e+00 - val_loss: 0.4849 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9335 - accuracy: 0.0000e+00 - val_loss: 0.4855 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.4865 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9320 - accuracy: 0.0000e+00 - val_loss: 0.4853 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9308 - accuracy: 0.0000e+00 - val_loss: 0.4848 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9300 - accuracy: 0.0000e+00 - val_loss: 0.4855 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9291 - accuracy: 0.0000e+00 - val_loss: 0.4857 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9281 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9274 - accuracy: 0.0000e+00 - val_loss: 0.4852 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9264 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9256 - accuracy: 0.0000e+00 - val_loss: 0.4859 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9247 - accuracy: 0.0000e+00 - val_loss: 0.4865 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9241 - accuracy: 0.0000e+00 - val_loss: 0.4861 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9233 - accuracy: 0.0000e+00 - val_loss: 0.4861 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9227 - accuracy: 0.0000e+00 - val_loss: 0.4855 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9218 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9212 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9206 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9198 - accuracy: 0.0000e+00 - val_loss: 0.4866 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9191 - accuracy: 0.0000e+00 - val_loss: 0.4859 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9188 - accuracy: 0.0000e+00 - val_loss: 0.4841 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9179 - accuracy: 0.0000e+00 - val_loss: 0.4846 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9172 - accuracy: 0.0000e+00 - val_loss: 0.4853 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9168 - accuracy: 0.0000e+00 - val_loss: 0.4849 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9160 - accuracy: 0.0000e+00 - val_loss: 0.4855 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9156 - accuracy: 0.0000e+00 - val_loss: 0.4844 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9149 - accuracy: 0.0000e+00 - val_loss: 0.4846 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9144 - accuracy: 0.0000e+00 - val_loss: 0.4856 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9138 - accuracy: 0.0000e+00 - val_loss: 0.4856 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9134 - accuracy: 0.0000e+00 - val_loss: 0.4851 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9128 - accuracy: 0.0000e+00 - val_loss: 0.4848 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9123 - accuracy: 0.0000e+00 - val_loss: 0.4845 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9117 - accuracy: 0.0000e+00 - val_loss: 0.4849 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9113 - accuracy: 0.0000e+00 - val_loss: 0.4849 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9107 - accuracy: 0.0000e+00 - val_loss: 0.4853 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9101 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9097 - accuracy: 0.0000e+00 - val_loss: 0.4861 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9094 - accuracy: 0.0000e+00 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9088 - accuracy: 0.0000e+00 - val_loss: 0.4855 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9084 - accuracy: 0.0000e+00 - val_loss: 0.4851 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9080 - accuracy: 0.0000e+00 - val_loss: 0.4848 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9075 - accuracy: 0.0000e+00 - val_loss: 0.4855 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9073 - accuracy: 0.0000e+00 - val_loss: 0.4845 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9069 - accuracy: 0.0000e+00 - val_loss: 0.4838 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9062 - accuracy: 0.0000e+00 - val_loss: 0.4841 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9058 - accuracy: 0.0000e+00 - val_loss: 0.4846 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9054 - accuracy: 0.0000e+00 - val_loss: 0.4843 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9050 - accuracy: 0.0000e+00 - val_loss: 0.4842 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9048 - accuracy: 0.0000e+00 - val_loss: 0.4833 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9044 - accuracy: 0.0000e+00 - val_loss: 0.4828 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9040 - accuracy: 0.0000e+00 - val_loss: 0.4833 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9037 - accuracy: 0.0000e+00 - val_loss: 0.4827 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9033 - accuracy: 0.0000e+00 - val_loss: 0.4828 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9029 - accuracy: 0.0000e+00 - val_loss: 0.4823 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9025 - accuracy: 0.0000e+00 - val_loss: 0.4818 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9023 - accuracy: 0.0000e+00 - val_loss: 0.4815 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.4815 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9016 - accuracy: 0.0000e+00 - val_loss: 0.4814 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9012 - accuracy: 0.0000e+00 - val_loss: 0.4804 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9009 - accuracy: 0.0000e+00 - val_loss: 0.4800 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.4800 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9005 - accuracy: 0.0000e+00 - val_loss: 0.4812 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9003 - accuracy: 0.0000e+00 - val_loss: 0.4787 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8997 - accuracy: 0.0000e+00 - val_loss: 0.4791 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8994 - accuracy: 0.0000e+00 - val_loss: 0.4786 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8991 - accuracy: 0.0000e+00 - val_loss: 0.4783 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8988 - accuracy: 0.0000e+00 - val_loss: 0.4774 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8985 - accuracy: 0.0000e+00 - val_loss: 0.4772 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.4766 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8978 - accuracy: 0.0000e+00 - val_loss: 0.4762 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8975 - accuracy: 0.0000e+00 - val_loss: 0.4756 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8973 - accuracy: 0.0000e+00 - val_loss: 0.4746 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8969 - accuracy: 0.0000e+00 - val_loss: 0.4748 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8967 - accuracy: 0.0000e+00 - val_loss: 0.4754 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8965 - accuracy: 0.0000e+00 - val_loss: 0.4738 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8961 - accuracy: 0.0000e+00 - val_loss: 0.4733 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8958 - accuracy: 0.0000e+00 - val_loss: 0.4725 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8955 - accuracy: 0.0000e+00 - val_loss: 0.4728 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8953 - accuracy: 0.0000e+00 - val_loss: 0.4716 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8950 - accuracy: 0.0000e+00 - val_loss: 0.4710 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8947 - accuracy: 0.0000e+00 - val_loss: 0.4699 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8945 - accuracy: 0.0000e+00 - val_loss: 0.4709 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8940 - accuracy: 0.0000e+00 - val_loss: 0.4698 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8937 - accuracy: 0.0000e+00 - val_loss: 0.4693 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8934 - accuracy: 0.0000e+00 - val_loss: 0.4681 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8931 - accuracy: 0.0000e+00 - val_loss: 0.4676 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8928 - accuracy: 0.0000e+00 - val_loss: 0.4679 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8925 - accuracy: 0.0000e+00 - val_loss: 0.4665 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8921 - accuracy: 0.0000e+00 - val_loss: 0.4667 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8917 - accuracy: 0.0000e+00 - val_loss: 0.4655 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8915 - accuracy: 0.0000e+00 - val_loss: 0.4643 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.4635 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8908 - accuracy: 0.0000e+00 - val_loss: 0.4631 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8905 - accuracy: 0.0000e+00 - val_loss: 0.4620 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8901 - accuracy: 0.0000e+00 - val_loss: 0.4615 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8899 - accuracy: 0.0000e+00 - val_loss: 0.4608 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8894 - accuracy: 0.0000e+00 - val_loss: 0.4589 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8890 - accuracy: 0.0000e+00 - val_loss: 0.4585 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8887 - accuracy: 0.0000e+00 - val_loss: 0.4567 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8883 - accuracy: 0.0000e+00 - val_loss: 0.4561 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8880 - accuracy: 0.0000e+00 - val_loss: 0.4564 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8877 - accuracy: 0.0000e+00 - val_loss: 0.4546 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8873 - accuracy: 0.0000e+00 - val_loss: 0.4530 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8868 - accuracy: 0.0000e+00 - val_loss: 0.4529 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8866 - accuracy: 0.0000e+00 - val_loss: 0.4531 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8862 - accuracy: 0.0000e+00 - val_loss: 0.4510 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8857 - accuracy: 0.0000e+00 - val_loss: 0.4507 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8853 - accuracy: 0.0000e+00 - val_loss: 0.4501 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8851 - accuracy: 0.0000e+00 - val_loss: 0.4491 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8847 - accuracy: 0.0000e+00 - val_loss: 0.4485 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8844 - accuracy: 0.0000e+00 - val_loss: 0.4479 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8841 - accuracy: 0.0000e+00 - val_loss: 0.4469 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8839 - accuracy: 0.0000e+00 - val_loss: 0.4461 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8835 - accuracy: 0.0000e+00 - val_loss: 0.4454 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8831 - accuracy: 0.0000e+00 - val_loss: 0.4450 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8829 - accuracy: 0.0000e+00 - val_loss: 0.4444 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8826 - accuracy: 0.0000e+00 - val_loss: 0.4434 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8823 - accuracy: 0.0000e+00 - val_loss: 0.4426 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8819 - accuracy: 0.0000e+00 - val_loss: 0.4421 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.4417 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - val_loss: 0.4418 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8810 - accuracy: 0.0000e+00 - val_loss: 0.4407 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8807 - accuracy: 0.0000e+00 - val_loss: 0.4396 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8804 - accuracy: 0.0000e+00 - val_loss: 0.4383 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8801 - accuracy: 0.0000e+00 - val_loss: 0.4379 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8798 - accuracy: 0.0000e+00 - val_loss: 0.4380 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8796 - accuracy: 0.0000e+00 - val_loss: 0.4368 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8792 - accuracy: 0.0000e+00 - val_loss: 0.4371 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8790 - accuracy: 0.0000e+00 - val_loss: 0.4365 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8788 - accuracy: 0.0000e+00 - val_loss: 0.4363 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8786 - accuracy: 0.0000e+00 - val_loss: 0.4355 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8784 - accuracy: 0.0000e+00 - val_loss: 0.4355 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.4352 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8780 - accuracy: 0.0000e+00 - val_loss: 0.4349 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8779 - accuracy: 0.0000e+00 - val_loss: 0.4339 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8776 - accuracy: 0.0000e+00 - val_loss: 0.4343 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8774 - accuracy: 0.0000e+00 - val_loss: 0.4339 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8772 - accuracy: 0.0000e+00 - val_loss: 0.4335 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8770 - accuracy: 0.0000e+00 - val_loss: 0.4340 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8768 - accuracy: 0.0000e+00 - val_loss: 0.4340 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8766 - accuracy: 0.0000e+00 - val_loss: 0.4335 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8764 - accuracy: 0.0000e+00 - val_loss: 0.4333 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8763 - accuracy: 0.0000e+00 - val_loss: 0.4324 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8761 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8759 - accuracy: 0.0000e+00 - val_loss: 0.4326 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8758 - accuracy: 0.0000e+00 - val_loss: 0.4326 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8756 - accuracy: 0.0000e+00 - val_loss: 0.4331 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8755 - accuracy: 0.0000e+00 - val_loss: 0.4335 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.4343 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8752 - accuracy: 0.0000e+00 - val_loss: 0.4336 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.4335 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8749 - accuracy: 0.0000e+00 - val_loss: 0.4326 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.4325 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8746 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8744 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8743 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8740 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8739 - accuracy: 0.0000e+00 - val_loss: 0.4314 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.4310 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.4310 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8732 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8731 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8729 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8728 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8726 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.4314 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8723 - accuracy: 0.0000e+00 - val_loss: 0.4310 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8722 - accuracy: 0.0000e+00 - val_loss: 0.4310 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8721 - accuracy: 0.0000e+00 - val_loss: 0.4311 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8720 - accuracy: 0.0000e+00 - val_loss: 0.4308 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8719 - accuracy: 0.0000e+00 - val_loss: 0.4312 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8717 - accuracy: 0.0000e+00 - val_loss: 0.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8716 - accuracy: 0.0000e+00 - val_loss: 0.4315 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.4312 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8715 - accuracy: 0.0000e+00 - val_loss: 0.4315 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8714 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.4314 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.4314 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8711 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8710 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8708 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8707 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8706 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.4315 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8704 - accuracy: 0.0000e+00 - val_loss: 0.4324 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8704 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8702 - accuracy: 0.0000e+00 - val_loss: 0.4322 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8702 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8701 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8700 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8700 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8700 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8699 - accuracy: 0.0000e+00 - val_loss: 0.4323 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8697 - accuracy: 0.0000e+00 - val_loss: 0.4322 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8697 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.4316 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8695 - accuracy: 0.0000e+00 - val_loss: 0.4314 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8695 - accuracy: 0.0000e+00 - val_loss: 0.4311 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/CVS_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4311 - accuracy: 0.0000e+00\n",
      "[0.43113046884536743, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_58 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 22ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/SCHW_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 21ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_60 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2965 - accuracy: 0.0000e+00 - val_loss: 0.5651 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2686 - accuracy: 0.0000e+00 - val_loss: 0.5685 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2454 - accuracy: 0.0000e+00 - val_loss: 0.5715 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2269 - accuracy: 0.0000e+00 - val_loss: 0.5740 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.2071 - accuracy: 0.0000e+00 - val_loss: 0.5772 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1867 - accuracy: 0.0000e+00 - val_loss: 0.5813 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1673 - accuracy: 0.0000e+00 - val_loss: 0.5850 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1517 - accuracy: 0.0000e+00 - val_loss: 0.5879 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1364 - accuracy: 0.0000e+00 - val_loss: 0.5896 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1240 - accuracy: 0.0000e+00 - val_loss: 0.5901 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1090 - accuracy: 0.0000e+00 - val_loss: 0.5889 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0952 - accuracy: 0.0000e+00 - val_loss: 0.5884 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0856 - accuracy: 0.0000e+00 - val_loss: 0.5904 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0737 - accuracy: 0.0000e+00 - val_loss: 0.5950 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0639 - accuracy: 0.0000e+00 - val_loss: 0.6015 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0533 - accuracy: 0.0000e+00 - val_loss: 0.6058 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0448 - accuracy: 0.0000e+00 - val_loss: 0.6104 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0368 - accuracy: 0.0000e+00 - val_loss: 0.6142 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0288 - accuracy: 0.0000e+00 - val_loss: 0.6161 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0214 - accuracy: 0.0000e+00 - val_loss: 0.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0146 - accuracy: 0.0000e+00 - val_loss: 0.6200 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0074 - accuracy: 0.0000e+00 - val_loss: 0.6223 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0015 - accuracy: 0.0000e+00 - val_loss: 0.6241 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9948 - accuracy: 0.0000e+00 - val_loss: 0.6258 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9886 - accuracy: 0.0000e+00 - val_loss: 0.6278 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9836 - accuracy: 0.0000e+00 - val_loss: 0.6299 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9785 - accuracy: 0.0000e+00 - val_loss: 0.6300 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9725 - accuracy: 0.0000e+00 - val_loss: 0.6315 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9678 - accuracy: 0.0000e+00 - val_loss: 0.6326 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9637 - accuracy: 0.0000e+00 - val_loss: 0.6330 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9589 - accuracy: 0.0000e+00 - val_loss: 0.6346 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9551 - accuracy: 0.0000e+00 - val_loss: 0.6371 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9502 - accuracy: 0.0000e+00 - val_loss: 0.6379 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9471 - accuracy: 0.0000e+00 - val_loss: 0.6396 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9431 - accuracy: 0.0000e+00 - val_loss: 0.6400 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.6410 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9367 - accuracy: 0.0000e+00 - val_loss: 0.6418 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9334 - accuracy: 0.0000e+00 - val_loss: 0.6432 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9296 - accuracy: 0.0000e+00 - val_loss: 0.6443 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9273 - accuracy: 0.0000e+00 - val_loss: 0.6459 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9240 - accuracy: 0.0000e+00 - val_loss: 0.6469 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9216 - accuracy: 0.0000e+00 - val_loss: 0.6483 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9187 - accuracy: 0.0000e+00 - val_loss: 0.6490 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9165 - accuracy: 0.0000e+00 - val_loss: 0.6492 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9133 - accuracy: 0.0000e+00 - val_loss: 0.6501 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9107 - accuracy: 0.0000e+00 - val_loss: 0.6513 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9089 - accuracy: 0.0000e+00 - val_loss: 0.6519 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9070 - accuracy: 0.0000e+00 - val_loss: 0.6524 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9044 - accuracy: 0.0000e+00 - val_loss: 0.6534 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9025 - accuracy: 0.0000e+00 - val_loss: 0.6543 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9004 - accuracy: 0.0000e+00 - val_loss: 0.6552 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8984 - accuracy: 0.0000e+00 - val_loss: 0.6550 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8966 - accuracy: 0.0000e+00 - val_loss: 0.6557 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8947 - accuracy: 0.0000e+00 - val_loss: 0.6576 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8933 - accuracy: 0.0000e+00 - val_loss: 0.6568 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.6578 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8896 - accuracy: 0.0000e+00 - val_loss: 0.6586 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8884 - accuracy: 0.0000e+00 - val_loss: 0.6588 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8863 - accuracy: 0.0000e+00 - val_loss: 0.6597 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8851 - accuracy: 0.0000e+00 - val_loss: 0.6601 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8835 - accuracy: 0.0000e+00 - val_loss: 0.6611 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8821 - accuracy: 0.0000e+00 - val_loss: 0.6623 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8807 - accuracy: 0.0000e+00 - val_loss: 0.6623 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8798 - accuracy: 0.0000e+00 - val_loss: 0.6619 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.6628 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8771 - accuracy: 0.0000e+00 - val_loss: 0.6638 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8761 - accuracy: 0.0000e+00 - val_loss: 0.6636 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.6644 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8735 - accuracy: 0.0000e+00 - val_loss: 0.6649 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.6651 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.6658 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8703 - accuracy: 0.0000e+00 - val_loss: 0.6669 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.6660 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8684 - accuracy: 0.0000e+00 - val_loss: 0.6670 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8674 - accuracy: 0.0000e+00 - val_loss: 0.6674 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8664 - accuracy: 0.0000e+00 - val_loss: 0.6680 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8655 - accuracy: 0.0000e+00 - val_loss: 0.6683 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.6692 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8637 - accuracy: 0.0000e+00 - val_loss: 0.6695 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8629 - accuracy: 0.0000e+00 - val_loss: 0.6704 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8620 - accuracy: 0.0000e+00 - val_loss: 0.6709 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8611 - accuracy: 0.0000e+00 - val_loss: 0.6715 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8604 - accuracy: 0.0000e+00 - val_loss: 0.6716 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.6723 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8588 - accuracy: 0.0000e+00 - val_loss: 0.6729 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8581 - accuracy: 0.0000e+00 - val_loss: 0.6733 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8573 - accuracy: 0.0000e+00 - val_loss: 0.6733 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8566 - accuracy: 0.0000e+00 - val_loss: 0.6734 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8560 - accuracy: 0.0000e+00 - val_loss: 0.6741 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8554 - accuracy: 0.0000e+00 - val_loss: 0.6743 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8546 - accuracy: 0.0000e+00 - val_loss: 0.6746 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8540 - accuracy: 0.0000e+00 - val_loss: 0.6759 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8533 - accuracy: 0.0000e+00 - val_loss: 0.6755 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.6761 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8520 - accuracy: 0.0000e+00 - val_loss: 0.6768 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8516 - accuracy: 0.0000e+00 - val_loss: 0.6768 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8508 - accuracy: 0.0000e+00 - val_loss: 0.6775 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8502 - accuracy: 0.0000e+00 - val_loss: 0.6783 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8496 - accuracy: 0.0000e+00 - val_loss: 0.6785 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.6791 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8484 - accuracy: 0.0000e+00 - val_loss: 0.6795 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8479 - accuracy: 0.0000e+00 - val_loss: 0.6797 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8473 - accuracy: 0.0000e+00 - val_loss: 0.6799 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8468 - accuracy: 0.0000e+00 - val_loss: 0.6800 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8462 - accuracy: 0.0000e+00 - val_loss: 0.6805 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8456 - accuracy: 0.0000e+00 - val_loss: 0.6813 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8451 - accuracy: 0.0000e+00 - val_loss: 0.6815 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8445 - accuracy: 0.0000e+00 - val_loss: 0.6821 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8440 - accuracy: 0.0000e+00 - val_loss: 0.6827 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8436 - accuracy: 0.0000e+00 - val_loss: 0.6835 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8430 - accuracy: 0.0000e+00 - val_loss: 0.6835 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8427 - accuracy: 0.0000e+00 - val_loss: 0.6832 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8421 - accuracy: 0.0000e+00 - val_loss: 0.6838 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8417 - accuracy: 0.0000e+00 - val_loss: 0.6841 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8411 - accuracy: 0.0000e+00 - val_loss: 0.6846 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8407 - accuracy: 0.0000e+00 - val_loss: 0.6849 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8403 - accuracy: 0.0000e+00 - val_loss: 0.6851 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8397 - accuracy: 0.0000e+00 - val_loss: 0.6857 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8393 - accuracy: 0.0000e+00 - val_loss: 0.6862 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8391 - accuracy: 0.0000e+00 - val_loss: 0.6859 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8384 - accuracy: 0.0000e+00 - val_loss: 0.6865 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8380 - accuracy: 0.0000e+00 - val_loss: 0.6869 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8377 - accuracy: 0.0000e+00 - val_loss: 0.6877 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.6875 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8367 - accuracy: 0.0000e+00 - val_loss: 0.6879 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8363 - accuracy: 0.0000e+00 - val_loss: 0.6883 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8359 - accuracy: 0.0000e+00 - val_loss: 0.6885 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8356 - accuracy: 0.0000e+00 - val_loss: 0.6882 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8352 - accuracy: 0.0000e+00 - val_loss: 0.6887 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8347 - accuracy: 0.0000e+00 - val_loss: 0.6886 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8344 - accuracy: 0.0000e+00 - val_loss: 0.6894 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8340 - accuracy: 0.0000e+00 - val_loss: 0.6899 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8336 - accuracy: 0.0000e+00 - val_loss: 0.6905 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8332 - accuracy: 0.0000e+00 - val_loss: 0.6904 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8328 - accuracy: 0.0000e+00 - val_loss: 0.6904 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.6912 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8322 - accuracy: 0.0000e+00 - val_loss: 0.6906 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8318 - accuracy: 0.0000e+00 - val_loss: 0.6913 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8314 - accuracy: 0.0000e+00 - val_loss: 0.6920 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.6915 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8307 - accuracy: 0.0000e+00 - val_loss: 0.6916 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8303 - accuracy: 0.0000e+00 - val_loss: 0.6921 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8300 - accuracy: 0.0000e+00 - val_loss: 0.6925 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8297 - accuracy: 0.0000e+00 - val_loss: 0.6932 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8294 - accuracy: 0.0000e+00 - val_loss: 0.6931 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8291 - accuracy: 0.0000e+00 - val_loss: 0.6934 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8287 - accuracy: 0.0000e+00 - val_loss: 0.6939 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8285 - accuracy: 0.0000e+00 - val_loss: 0.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8281 - accuracy: 0.0000e+00 - val_loss: 0.6940 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8278 - accuracy: 0.0000e+00 - val_loss: 0.6942 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8276 - accuracy: 0.0000e+00 - val_loss: 0.6943 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8272 - accuracy: 0.0000e+00 - val_loss: 0.6946 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8269 - accuracy: 0.0000e+00 - val_loss: 0.6956 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8267 - accuracy: 0.0000e+00 - val_loss: 0.6960 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8264 - accuracy: 0.0000e+00 - val_loss: 0.6957 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8260 - accuracy: 0.0000e+00 - val_loss: 0.6961 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8258 - accuracy: 0.0000e+00 - val_loss: 0.6961 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8255 - accuracy: 0.0000e+00 - val_loss: 0.6966 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8253 - accuracy: 0.0000e+00 - val_loss: 0.6964 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8250 - accuracy: 0.0000e+00 - val_loss: 0.6973 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8247 - accuracy: 0.0000e+00 - val_loss: 0.6975 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8243 - accuracy: 0.0000e+00 - val_loss: 0.6979 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8242 - accuracy: 0.0000e+00 - val_loss: 0.6980 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8238 - accuracy: 0.0000e+00 - val_loss: 0.6981 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8237 - accuracy: 0.0000e+00 - val_loss: 0.6980 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8234 - accuracy: 0.0000e+00 - val_loss: 0.6987 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8232 - accuracy: 0.0000e+00 - val_loss: 0.6997 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8228 - accuracy: 0.0000e+00 - val_loss: 0.6997 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8226 - accuracy: 0.0000e+00 - val_loss: 0.7002 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 0.6998 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8221 - accuracy: 0.0000e+00 - val_loss: 0.7002 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8220 - accuracy: 0.0000e+00 - val_loss: 0.6997 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8216 - accuracy: 0.0000e+00 - val_loss: 0.7005 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8213 - accuracy: 0.0000e+00 - val_loss: 0.7009 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8211 - accuracy: 0.0000e+00 - val_loss: 0.7014 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8209 - accuracy: 0.0000e+00 - val_loss: 0.7018 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8207 - accuracy: 0.0000e+00 - val_loss: 0.7019 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8204 - accuracy: 0.0000e+00 - val_loss: 0.7024 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8201 - accuracy: 0.0000e+00 - val_loss: 0.7026 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8200 - accuracy: 0.0000e+00 - val_loss: 0.7032 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8197 - accuracy: 0.0000e+00 - val_loss: 0.7034 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8194 - accuracy: 0.0000e+00 - val_loss: 0.7034 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8193 - accuracy: 0.0000e+00 - val_loss: 0.7033 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8190 - accuracy: 0.0000e+00 - val_loss: 0.7036 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8188 - accuracy: 0.0000e+00 - val_loss: 0.7045 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8185 - accuracy: 0.0000e+00 - val_loss: 0.7043 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8183 - accuracy: 0.0000e+00 - val_loss: 0.7045 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8182 - accuracy: 0.0000e+00 - val_loss: 0.7057 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8178 - accuracy: 0.0000e+00 - val_loss: 0.7055 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8177 - accuracy: 0.0000e+00 - val_loss: 0.7052 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8174 - accuracy: 0.0000e+00 - val_loss: 0.7061 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8173 - accuracy: 0.0000e+00 - val_loss: 0.7074 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8170 - accuracy: 0.0000e+00 - val_loss: 0.7080 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8167 - accuracy: 0.0000e+00 - val_loss: 0.7067 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8165 - accuracy: 0.0000e+00 - val_loss: 0.7061 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.8163 - accuracy: 0.0000e+00 - val_loss: 0.7071 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8160 - accuracy: 0.0000e+00 - val_loss: 0.7076 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8158 - accuracy: 0.0000e+00 - val_loss: 0.7080 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8156 - accuracy: 0.0000e+00 - val_loss: 0.7079 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8154 - accuracy: 0.0000e+00 - val_loss: 0.7072 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8151 - accuracy: 0.0000e+00 - val_loss: 0.7075 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8149 - accuracy: 0.0000e+00 - val_loss: 0.7084 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8146 - accuracy: 0.0000e+00 - val_loss: 0.7084 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8145 - accuracy: 0.0000e+00 - val_loss: 0.7072 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8143 - accuracy: 0.0000e+00 - val_loss: 0.7077 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8140 - accuracy: 0.0000e+00 - val_loss: 0.7072 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8137 - accuracy: 0.0000e+00 - val_loss: 0.7080 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8136 - accuracy: 0.0000e+00 - val_loss: 0.7081 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8133 - accuracy: 0.0000e+00 - val_loss: 0.7065 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8130 - accuracy: 0.0000e+00 - val_loss: 0.7063 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8128 - accuracy: 0.0000e+00 - val_loss: 0.7063 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.8126 - accuracy: 0.0000e+00 - val_loss: 0.7047 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8122 - accuracy: 0.0000e+00 - val_loss: 0.7041 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8121 - accuracy: 0.0000e+00 - val_loss: 0.7033 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8118 - accuracy: 0.0000e+00 - val_loss: 0.7020 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8115 - accuracy: 0.0000e+00 - val_loss: 0.7011 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8113 - accuracy: 0.0000e+00 - val_loss: 0.7006 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8111 - accuracy: 0.0000e+00 - val_loss: 0.6997 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8110 - accuracy: 0.0000e+00 - val_loss: 0.6974 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8107 - accuracy: 0.0000e+00 - val_loss: 0.6965 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8105 - accuracy: 0.0000e+00 - val_loss: 0.6980 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8105 - accuracy: 0.0000e+00 - val_loss: 0.6970 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8102 - accuracy: 0.0000e+00 - val_loss: 0.6986 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8100 - accuracy: 0.0000e+00 - val_loss: 0.6990 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8098 - accuracy: 0.0000e+00 - val_loss: 0.7002 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8097 - accuracy: 0.0000e+00 - val_loss: 0.7001 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8095 - accuracy: 0.0000e+00 - val_loss: 0.7022 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8093 - accuracy: 0.0000e+00 - val_loss: 0.7012 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8091 - accuracy: 0.0000e+00 - val_loss: 0.7014 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8090 - accuracy: 0.0000e+00 - val_loss: 0.7011 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8088 - accuracy: 0.0000e+00 - val_loss: 0.7026 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8087 - accuracy: 0.0000e+00 - val_loss: 0.7042 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8085 - accuracy: 0.0000e+00 - val_loss: 0.7044 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8084 - accuracy: 0.0000e+00 - val_loss: 0.7028 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8083 - accuracy: 0.0000e+00 - val_loss: 0.7001 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8081 - accuracy: 0.0000e+00 - val_loss: 0.7037 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8079 - accuracy: 0.0000e+00 - val_loss: 0.7057 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8079 - accuracy: 0.0000e+00 - val_loss: 0.7026 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8076 - accuracy: 0.0000e+00 - val_loss: 0.7042 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8074 - accuracy: 0.0000e+00 - val_loss: 0.7067 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8073 - accuracy: 0.0000e+00 - val_loss: 0.7074 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8072 - accuracy: 0.0000e+00 - val_loss: 0.7064 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8070 - accuracy: 0.0000e+00 - val_loss: 0.7060 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8069 - accuracy: 0.0000e+00 - val_loss: 0.7077 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8068 - accuracy: 0.0000e+00 - val_loss: 0.7061 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8066 - accuracy: 0.0000e+00 - val_loss: 0.7077 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8065 - accuracy: 0.0000e+00 - val_loss: 0.7072 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8063 - accuracy: 0.0000e+00 - val_loss: 0.7094 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8062 - accuracy: 0.0000e+00 - val_loss: 0.7112 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8060 - accuracy: 0.0000e+00 - val_loss: 0.7087 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8059 - accuracy: 0.0000e+00 - val_loss: 0.7094 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8058 - accuracy: 0.0000e+00 - val_loss: 0.7100 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8056 - accuracy: 0.0000e+00 - val_loss: 0.7102 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8055 - accuracy: 0.0000e+00 - val_loss: 0.7104 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8055 - accuracy: 0.0000e+00 - val_loss: 0.7087 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8052 - accuracy: 0.0000e+00 - val_loss: 0.7096 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/CAT_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7096 - accuracy: 0.0000e+00\n",
      "[0.7095655202865601, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_62 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/LMT_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_64 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 19ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/MS_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 22ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_66 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2306 - accuracy: 0.0000e+00 - val_loss: 0.3452 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2034 - accuracy: 0.0000e+00 - val_loss: 0.3422 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1824 - accuracy: 0.0000e+00 - val_loss: 0.3394 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1612 - accuracy: 0.0000e+00 - val_loss: 0.3361 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1439 - accuracy: 0.0000e+00 - val_loss: 0.3321 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1236 - accuracy: 0.0000e+00 - val_loss: 0.3294 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1079 - accuracy: 0.0000e+00 - val_loss: 0.3267 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0950 - accuracy: 0.0000e+00 - val_loss: 0.3241 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0782 - accuracy: 0.0000e+00 - val_loss: 0.3224 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0644 - accuracy: 0.0000e+00 - val_loss: 0.3210 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0533 - accuracy: 0.0000e+00 - val_loss: 0.3200 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0438 - accuracy: 0.0000e+00 - val_loss: 0.3192 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0313 - accuracy: 0.0000e+00 - val_loss: 0.3188 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0238 - accuracy: 0.0000e+00 - val_loss: 0.3187 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0145 - accuracy: 0.0000e+00 - val_loss: 0.3188 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0079 - accuracy: 0.0000e+00 - val_loss: 0.3192 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0006 - accuracy: 0.0000e+00 - val_loss: 0.3196 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9936 - accuracy: 0.0000e+00 - val_loss: 0.3202 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9875 - accuracy: 0.0000e+00 - val_loss: 0.3207 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9828 - accuracy: 0.0000e+00 - val_loss: 0.3212 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9766 - accuracy: 0.0000e+00 - val_loss: 0.3219 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9714 - accuracy: 0.0000e+00 - val_loss: 0.3225 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9676 - accuracy: 0.0000e+00 - val_loss: 0.3232 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9619 - accuracy: 0.0000e+00 - val_loss: 0.3239 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9586 - accuracy: 0.0000e+00 - val_loss: 0.3248 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9549 - accuracy: 0.0000e+00 - val_loss: 0.3257 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9512 - accuracy: 0.0000e+00 - val_loss: 0.3265 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9475 - accuracy: 0.0000e+00 - val_loss: 0.3276 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9437 - accuracy: 0.0000e+00 - val_loss: 0.3285 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9411 - accuracy: 0.0000e+00 - val_loss: 0.3295 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9377 - accuracy: 0.0000e+00 - val_loss: 0.3306 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9355 - accuracy: 0.0000e+00 - val_loss: 0.3315 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9325 - accuracy: 0.0000e+00 - val_loss: 0.3325 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9294 - accuracy: 0.0000e+00 - val_loss: 0.3335 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9271 - accuracy: 0.0000e+00 - val_loss: 0.3346 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9244 - accuracy: 0.0000e+00 - val_loss: 0.3357 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9220 - accuracy: 0.0000e+00 - val_loss: 0.3366 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9193 - accuracy: 0.0000e+00 - val_loss: 0.3377 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9171 - accuracy: 0.0000e+00 - val_loss: 0.3387 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9151 - accuracy: 0.0000e+00 - val_loss: 0.3399 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9133 - accuracy: 0.0000e+00 - val_loss: 0.3411 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9107 - accuracy: 0.0000e+00 - val_loss: 0.3421 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9092 - accuracy: 0.0000e+00 - val_loss: 0.3432 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9072 - accuracy: 0.0000e+00 - val_loss: 0.3442 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9050 - accuracy: 0.0000e+00 - val_loss: 0.3452 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9037 - accuracy: 0.0000e+00 - val_loss: 0.3463 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9018 - accuracy: 0.0000e+00 - val_loss: 0.3474 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8996 - accuracy: 0.0000e+00 - val_loss: 0.3484 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8988 - accuracy: 0.0000e+00 - val_loss: 0.3493 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8966 - accuracy: 0.0000e+00 - val_loss: 0.3503 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8956 - accuracy: 0.0000e+00 - val_loss: 0.3512 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8938 - accuracy: 0.0000e+00 - val_loss: 0.3522 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8925 - accuracy: 0.0000e+00 - val_loss: 0.3533 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8909 - accuracy: 0.0000e+00 - val_loss: 0.3542 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8895 - accuracy: 0.0000e+00 - val_loss: 0.3551 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.3562 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8869 - accuracy: 0.0000e+00 - val_loss: 0.3572 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8858 - accuracy: 0.0000e+00 - val_loss: 0.3583 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8845 - accuracy: 0.0000e+00 - val_loss: 0.3593 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8832 - accuracy: 0.0000e+00 - val_loss: 0.3602 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8825 - accuracy: 0.0000e+00 - val_loss: 0.3609 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8810 - accuracy: 0.0000e+00 - val_loss: 0.3618 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8801 - accuracy: 0.0000e+00 - val_loss: 0.3628 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8792 - accuracy: 0.0000e+00 - val_loss: 0.3637 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8780 - accuracy: 0.0000e+00 - val_loss: 0.3646 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8769 - accuracy: 0.0000e+00 - val_loss: 0.3654 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8761 - accuracy: 0.0000e+00 - val_loss: 0.3664 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8752 - accuracy: 0.0000e+00 - val_loss: 0.3672 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8743 - accuracy: 0.0000e+00 - val_loss: 0.3679 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8735 - accuracy: 0.0000e+00 - val_loss: 0.3686 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8725 - accuracy: 0.0000e+00 - val_loss: 0.3694 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8716 - accuracy: 0.0000e+00 - val_loss: 0.3702 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.3710 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.3718 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8692 - accuracy: 0.0000e+00 - val_loss: 0.3726 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8685 - accuracy: 0.0000e+00 - val_loss: 0.3734 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8676 - accuracy: 0.0000e+00 - val_loss: 0.3741 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8670 - accuracy: 0.0000e+00 - val_loss: 0.3748 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8663 - accuracy: 0.0000e+00 - val_loss: 0.3754 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.3760 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8646 - accuracy: 0.0000e+00 - val_loss: 0.3767 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8640 - accuracy: 0.0000e+00 - val_loss: 0.3776 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8633 - accuracy: 0.0000e+00 - val_loss: 0.3783 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8625 - accuracy: 0.0000e+00 - val_loss: 0.3790 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8618 - accuracy: 0.0000e+00 - val_loss: 0.3798 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8611 - accuracy: 0.0000e+00 - val_loss: 0.3804 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8607 - accuracy: 0.0000e+00 - val_loss: 0.3810 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8597 - accuracy: 0.0000e+00 - val_loss: 0.3817 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8591 - accuracy: 0.0000e+00 - val_loss: 0.3825 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8584 - accuracy: 0.0000e+00 - val_loss: 0.3832 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8576 - accuracy: 0.0000e+00 - val_loss: 0.3840 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8570 - accuracy: 0.0000e+00 - val_loss: 0.3845 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8563 - accuracy: 0.0000e+00 - val_loss: 0.3852 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8557 - accuracy: 0.0000e+00 - val_loss: 0.3859 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8550 - accuracy: 0.0000e+00 - val_loss: 0.3865 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8543 - accuracy: 0.0000e+00 - val_loss: 0.3872 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8536 - accuracy: 0.0000e+00 - val_loss: 0.3879 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8529 - accuracy: 0.0000e+00 - val_loss: 0.3885 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8524 - accuracy: 0.0000e+00 - val_loss: 0.3891 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8518 - accuracy: 0.0000e+00 - val_loss: 0.3898 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8512 - accuracy: 0.0000e+00 - val_loss: 0.3904 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8506 - accuracy: 0.0000e+00 - val_loss: 0.3910 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8500 - accuracy: 0.0000e+00 - val_loss: 0.3917 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8492 - accuracy: 0.0000e+00 - val_loss: 0.3923 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8487 - accuracy: 0.0000e+00 - val_loss: 0.3928 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8481 - accuracy: 0.0000e+00 - val_loss: 0.3933 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8475 - accuracy: 0.0000e+00 - val_loss: 0.3939 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8469 - accuracy: 0.0000e+00 - val_loss: 0.3945 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8463 - accuracy: 0.0000e+00 - val_loss: 0.3951 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8456 - accuracy: 0.0000e+00 - val_loss: 0.3956 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8451 - accuracy: 0.0000e+00 - val_loss: 0.3962 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8446 - accuracy: 0.0000e+00 - val_loss: 0.3966 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8441 - accuracy: 0.0000e+00 - val_loss: 0.3972 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8435 - accuracy: 0.0000e+00 - val_loss: 0.3977 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8429 - accuracy: 0.0000e+00 - val_loss: 0.3983 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8423 - accuracy: 0.0000e+00 - val_loss: 0.3988 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8419 - accuracy: 0.0000e+00 - val_loss: 0.3994 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8413 - accuracy: 0.0000e+00 - val_loss: 0.3998 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8408 - accuracy: 0.0000e+00 - val_loss: 0.4003 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8403 - accuracy: 0.0000e+00 - val_loss: 0.4008 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8399 - accuracy: 0.0000e+00 - val_loss: 0.4013 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8394 - accuracy: 0.0000e+00 - val_loss: 0.4018 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8390 - accuracy: 0.0000e+00 - val_loss: 0.4022 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8384 - accuracy: 0.0000e+00 - val_loss: 0.4027 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8381 - accuracy: 0.0000e+00 - val_loss: 0.4032 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8376 - accuracy: 0.0000e+00 - val_loss: 0.4036 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8372 - accuracy: 0.0000e+00 - val_loss: 0.4041 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8367 - accuracy: 0.0000e+00 - val_loss: 0.4046 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8365 - accuracy: 0.0000e+00 - val_loss: 0.4050 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8359 - accuracy: 0.0000e+00 - val_loss: 0.4054 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8355 - accuracy: 0.0000e+00 - val_loss: 0.4059 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8351 - accuracy: 0.0000e+00 - val_loss: 0.4064 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8347 - accuracy: 0.0000e+00 - val_loss: 0.4068 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8344 - accuracy: 0.0000e+00 - val_loss: 0.4073 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8340 - accuracy: 0.0000e+00 - val_loss: 0.4077 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8337 - accuracy: 0.0000e+00 - val_loss: 0.4082 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8333 - accuracy: 0.0000e+00 - val_loss: 0.4086 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.4091 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8327 - accuracy: 0.0000e+00 - val_loss: 0.4096 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8324 - accuracy: 0.0000e+00 - val_loss: 0.4100 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8321 - accuracy: 0.0000e+00 - val_loss: 0.4104 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8317 - accuracy: 0.0000e+00 - val_loss: 0.4109 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8315 - accuracy: 0.0000e+00 - val_loss: 0.4113 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8312 - accuracy: 0.0000e+00 - val_loss: 0.4116 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8309 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.4123 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8303 - accuracy: 0.0000e+00 - val_loss: 0.4127 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8301 - accuracy: 0.0000e+00 - val_loss: 0.4130 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8298 - accuracy: 0.0000e+00 - val_loss: 0.4135 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8295 - accuracy: 0.0000e+00 - val_loss: 0.4139 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8292 - accuracy: 0.0000e+00 - val_loss: 0.4142 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8290 - accuracy: 0.0000e+00 - val_loss: 0.4146 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8288 - accuracy: 0.0000e+00 - val_loss: 0.4149 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8285 - accuracy: 0.0000e+00 - val_loss: 0.4152 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8283 - accuracy: 0.0000e+00 - val_loss: 0.4155 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8280 - accuracy: 0.0000e+00 - val_loss: 0.4158 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8277 - accuracy: 0.0000e+00 - val_loss: 0.4161 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8275 - accuracy: 0.0000e+00 - val_loss: 0.4164 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8272 - accuracy: 0.0000e+00 - val_loss: 0.4167 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8271 - accuracy: 0.0000e+00 - val_loss: 0.4170 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8268 - accuracy: 0.0000e+00 - val_loss: 0.4173 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8266 - accuracy: 0.0000e+00 - val_loss: 0.4176 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8264 - accuracy: 0.0000e+00 - val_loss: 0.4179 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8261 - accuracy: 0.0000e+00 - val_loss: 0.4182 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8259 - accuracy: 0.0000e+00 - val_loss: 0.4185 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8258 - accuracy: 0.0000e+00 - val_loss: 0.4188 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8256 - accuracy: 0.0000e+00 - val_loss: 0.4190 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8254 - accuracy: 0.0000e+00 - val_loss: 0.4192 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8251 - accuracy: 0.0000e+00 - val_loss: 0.4194 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8250 - accuracy: 0.0000e+00 - val_loss: 0.4197 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8248 - accuracy: 0.0000e+00 - val_loss: 0.4199 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8246 - accuracy: 0.0000e+00 - val_loss: 0.4201 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8244 - accuracy: 0.0000e+00 - val_loss: 0.4204 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8242 - accuracy: 0.0000e+00 - val_loss: 0.4206 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8240 - accuracy: 0.0000e+00 - val_loss: 0.4208 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8239 - accuracy: 0.0000e+00 - val_loss: 0.4209 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8237 - accuracy: 0.0000e+00 - val_loss: 0.4211 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8235 - accuracy: 0.0000e+00 - val_loss: 0.4214 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8233 - accuracy: 0.0000e+00 - val_loss: 0.4216 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8232 - accuracy: 0.0000e+00 - val_loss: 0.4217 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8230 - accuracy: 0.0000e+00 - val_loss: 0.4219 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8229 - accuracy: 0.0000e+00 - val_loss: 0.4221 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8227 - accuracy: 0.0000e+00 - val_loss: 0.4222 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8225 - accuracy: 0.0000e+00 - val_loss: 0.4225 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8224 - accuracy: 0.0000e+00 - val_loss: 0.4227 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8223 - accuracy: 0.0000e+00 - val_loss: 0.4229 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8221 - accuracy: 0.0000e+00 - val_loss: 0.4231 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8220 - accuracy: 0.0000e+00 - val_loss: 0.4232 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8218 - accuracy: 0.0000e+00 - val_loss: 0.4234 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8217 - accuracy: 0.0000e+00 - val_loss: 0.4237 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8216 - accuracy: 0.0000e+00 - val_loss: 0.4239 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8214 - accuracy: 0.0000e+00 - val_loss: 0.4240 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8213 - accuracy: 0.0000e+00 - val_loss: 0.4242 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8211 - accuracy: 0.0000e+00 - val_loss: 0.4244 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8210 - accuracy: 0.0000e+00 - val_loss: 0.4246 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8209 - accuracy: 0.0000e+00 - val_loss: 0.4248 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8207 - accuracy: 0.0000e+00 - val_loss: 0.4249 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8206 - accuracy: 0.0000e+00 - val_loss: 0.4252 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8205 - accuracy: 0.0000e+00 - val_loss: 0.4254 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8204 - accuracy: 0.0000e+00 - val_loss: 0.4255 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8203 - accuracy: 0.0000e+00 - val_loss: 0.4256 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8201 - accuracy: 0.0000e+00 - val_loss: 0.4258 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8200 - accuracy: 0.0000e+00 - val_loss: 0.4259 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8199 - accuracy: 0.0000e+00 - val_loss: 0.4261 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8198 - accuracy: 0.0000e+00 - val_loss: 0.4263 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8197 - accuracy: 0.0000e+00 - val_loss: 0.4265 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8195 - accuracy: 0.0000e+00 - val_loss: 0.4267 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8194 - accuracy: 0.0000e+00 - val_loss: 0.4268 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8193 - accuracy: 0.0000e+00 - val_loss: 0.4269 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8192 - accuracy: 0.0000e+00 - val_loss: 0.4271 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8191 - accuracy: 0.0000e+00 - val_loss: 0.4272 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8190 - accuracy: 0.0000e+00 - val_loss: 0.4273 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 0.4275 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8188 - accuracy: 0.0000e+00 - val_loss: 0.4277 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8187 - accuracy: 0.0000e+00 - val_loss: 0.4278 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8186 - accuracy: 0.0000e+00 - val_loss: 0.4280 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8185 - accuracy: 0.0000e+00 - val_loss: 0.4281 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8184 - accuracy: 0.0000e+00 - val_loss: 0.4282 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8183 - accuracy: 0.0000e+00 - val_loss: 0.4283 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8182 - accuracy: 0.0000e+00 - val_loss: 0.4284 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8181 - accuracy: 0.0000e+00 - val_loss: 0.4286 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8180 - accuracy: 0.0000e+00 - val_loss: 0.4287 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8179 - accuracy: 0.0000e+00 - val_loss: 0.4288 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8178 - accuracy: 0.0000e+00 - val_loss: 0.4289 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8177 - accuracy: 0.0000e+00 - val_loss: 0.4291 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8176 - accuracy: 0.0000e+00 - val_loss: 0.4292 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8176 - accuracy: 0.0000e+00 - val_loss: 0.4293 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8174 - accuracy: 0.0000e+00 - val_loss: 0.4294 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8174 - accuracy: 0.0000e+00 - val_loss: 0.4296 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8173 - accuracy: 0.0000e+00 - val_loss: 0.4297 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8172 - accuracy: 0.0000e+00 - val_loss: 0.4298 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8171 - accuracy: 0.0000e+00 - val_loss: 0.4299 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8170 - accuracy: 0.0000e+00 - val_loss: 0.4301 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8169 - accuracy: 0.0000e+00 - val_loss: 0.4302 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8169 - accuracy: 0.0000e+00 - val_loss: 0.4303 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8168 - accuracy: 0.0000e+00 - val_loss: 0.4303 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8167 - accuracy: 0.0000e+00 - val_loss: 0.4304 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8166 - accuracy: 0.0000e+00 - val_loss: 0.4305 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8165 - accuracy: 0.0000e+00 - val_loss: 0.4306 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8164 - accuracy: 0.0000e+00 - val_loss: 0.4308 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8164 - accuracy: 0.0000e+00 - val_loss: 0.4308 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8163 - accuracy: 0.0000e+00 - val_loss: 0.4309 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8162 - accuracy: 0.0000e+00 - val_loss: 0.4310 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8162 - accuracy: 0.0000e+00 - val_loss: 0.4312 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8161 - accuracy: 0.0000e+00 - val_loss: 0.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8160 - accuracy: 0.0000e+00 - val_loss: 0.4314 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8159 - accuracy: 0.0000e+00 - val_loss: 0.4315 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8158 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8158 - accuracy: 0.0000e+00 - val_loss: 0.4317 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8157 - accuracy: 0.0000e+00 - val_loss: 0.4318 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8156 - accuracy: 0.0000e+00 - val_loss: 0.4319 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8155 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8155 - accuracy: 0.0000e+00 - val_loss: 0.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8154 - accuracy: 0.0000e+00 - val_loss: 0.4322 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8153 - accuracy: 0.0000e+00 - val_loss: 0.4323 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8152 - accuracy: 0.0000e+00 - val_loss: 0.4324 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/INTC_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4324 - accuracy: 0.0000e+00\n",
      "[0.4323643445968628, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_68 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.2584 - accuracy: 0.0000e+00 - val_loss: 0.2002 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2335 - accuracy: 0.0000e+00 - val_loss: 0.3471 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2134 - accuracy: 0.0000e+00 - val_loss: 0.3474 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1900 - accuracy: 0.0000e+00 - val_loss: 0.3457 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1700 - accuracy: 0.0000e+00 - val_loss: 0.3444 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1521 - accuracy: 0.0000e+00 - val_loss: 0.3437 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.1356 - accuracy: 0.0000e+00 - val_loss: 0.3430 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.1151 - accuracy: 0.0000e+00 - val_loss: 0.3431 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1024 - accuracy: 0.0000e+00 - val_loss: 0.3431 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0928 - accuracy: 0.0000e+00 - val_loss: 0.3435 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0785 - accuracy: 0.0000e+00 - val_loss: 0.3438 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 1.0700 - accuracy: 0.0000e+00 - val_loss: 0.3444 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0595 - accuracy: 0.0000e+00 - val_loss: 0.3452 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0512 - accuracy: 0.0000e+00 - val_loss: 0.3459 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0452 - accuracy: 0.0000e+00 - val_loss: 0.3467 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0386 - accuracy: 0.0000e+00 - val_loss: 0.3478 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0329 - accuracy: 0.0000e+00 - val_loss: 0.3490 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0255 - accuracy: 0.0000e+00 - val_loss: 0.3498 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0216 - accuracy: 0.0000e+00 - val_loss: 0.3509 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0160 - accuracy: 0.0000e+00 - val_loss: 0.3517 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0116 - accuracy: 0.0000e+00 - val_loss: 0.3528 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0069 - accuracy: 0.0000e+00 - val_loss: 0.3537 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0031 - accuracy: 0.0000e+00 - val_loss: 0.3545 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9985 - accuracy: 0.0000e+00 - val_loss: 0.3553 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9948 - accuracy: 0.0000e+00 - val_loss: 0.3559 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9913 - accuracy: 0.0000e+00 - val_loss: 0.3565 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9882 - accuracy: 0.0000e+00 - val_loss: 0.3571 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9851 - accuracy: 0.0000e+00 - val_loss: 0.3576 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9822 - accuracy: 0.0000e+00 - val_loss: 0.3583 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9793 - accuracy: 0.0000e+00 - val_loss: 0.3588 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9764 - accuracy: 0.0000e+00 - val_loss: 0.3592 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9734 - accuracy: 0.0000e+00 - val_loss: 0.3598 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9709 - accuracy: 0.0000e+00 - val_loss: 0.3603 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9681 - accuracy: 0.0000e+00 - val_loss: 0.3608 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9665 - accuracy: 0.0000e+00 - val_loss: 0.3615 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9633 - accuracy: 0.0000e+00 - val_loss: 0.3615 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9614 - accuracy: 0.0000e+00 - val_loss: 0.3621 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9595 - accuracy: 0.0000e+00 - val_loss: 0.3624 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9571 - accuracy: 0.0000e+00 - val_loss: 0.3627 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9550 - accuracy: 0.0000e+00 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9525 - accuracy: 0.0000e+00 - val_loss: 0.3634 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9511 - accuracy: 0.0000e+00 - val_loss: 0.3638 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9490 - accuracy: 0.0000e+00 - val_loss: 0.3641 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9474 - accuracy: 0.0000e+00 - val_loss: 0.3645 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9453 - accuracy: 0.0000e+00 - val_loss: 0.3649 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9432 - accuracy: 0.0000e+00 - val_loss: 0.3652 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9417 - accuracy: 0.0000e+00 - val_loss: 0.3654 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9400 - accuracy: 0.0000e+00 - val_loss: 0.3654 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9382 - accuracy: 0.0000e+00 - val_loss: 0.3656 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9365 - accuracy: 0.0000e+00 - val_loss: 0.3655 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9354 - accuracy: 0.0000e+00 - val_loss: 0.3658 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9336 - accuracy: 0.0000e+00 - val_loss: 0.3662 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9317 - accuracy: 0.0000e+00 - val_loss: 0.3662 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9304 - accuracy: 0.0000e+00 - val_loss: 0.3664 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9290 - accuracy: 0.0000e+00 - val_loss: 0.3661 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9274 - accuracy: 0.0000e+00 - val_loss: 0.3657 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9261 - accuracy: 0.0000e+00 - val_loss: 0.3659 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9248 - accuracy: 0.0000e+00 - val_loss: 0.3661 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9232 - accuracy: 0.0000e+00 - val_loss: 0.3662 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9220 - accuracy: 0.0000e+00 - val_loss: 0.3663 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9210 - accuracy: 0.0000e+00 - val_loss: 0.3663 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9196 - accuracy: 0.0000e+00 - val_loss: 0.3662 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9184 - accuracy: 0.0000e+00 - val_loss: 0.3660 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9172 - accuracy: 0.0000e+00 - val_loss: 0.3661 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9164 - accuracy: 0.0000e+00 - val_loss: 0.3661 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9151 - accuracy: 0.0000e+00 - val_loss: 0.3661 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9142 - accuracy: 0.0000e+00 - val_loss: 0.3660 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9131 - accuracy: 0.0000e+00 - val_loss: 0.3655 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9119 - accuracy: 0.0000e+00 - val_loss: 0.3655 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9110 - accuracy: 0.0000e+00 - val_loss: 0.3653 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9101 - accuracy: 0.0000e+00 - val_loss: 0.3652 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9093 - accuracy: 0.0000e+00 - val_loss: 0.3652 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9083 - accuracy: 0.0000e+00 - val_loss: 0.3652 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9075 - accuracy: 0.0000e+00 - val_loss: 0.3648 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9065 - accuracy: 0.0000e+00 - val_loss: 0.3648 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9058 - accuracy: 0.0000e+00 - val_loss: 0.3645 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9051 - accuracy: 0.0000e+00 - val_loss: 0.3645 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9043 - accuracy: 0.0000e+00 - val_loss: 0.3643 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9036 - accuracy: 0.0000e+00 - val_loss: 0.3644 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9028 - accuracy: 0.0000e+00 - val_loss: 0.3641 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9022 - accuracy: 0.0000e+00 - val_loss: 0.3641 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9015 - accuracy: 0.0000e+00 - val_loss: 0.3639 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9007 - accuracy: 0.0000e+00 - val_loss: 0.3638 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.3638 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8995 - accuracy: 0.0000e+00 - val_loss: 0.3635 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8988 - accuracy: 0.0000e+00 - val_loss: 0.3635 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8982 - accuracy: 0.0000e+00 - val_loss: 0.3634 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8977 - accuracy: 0.0000e+00 - val_loss: 0.3634 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8971 - accuracy: 0.0000e+00 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8964 - accuracy: 0.0000e+00 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8959 - accuracy: 0.0000e+00 - val_loss: 0.3630 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8953 - accuracy: 0.0000e+00 - val_loss: 0.3630 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8948 - accuracy: 0.0000e+00 - val_loss: 0.3626 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8942 - accuracy: 0.0000e+00 - val_loss: 0.3626 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8936 - accuracy: 0.0000e+00 - val_loss: 0.3625 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8932 - accuracy: 0.0000e+00 - val_loss: 0.3627 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8926 - accuracy: 0.0000e+00 - val_loss: 0.3624 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8921 - accuracy: 0.0000e+00 - val_loss: 0.3623 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8916 - accuracy: 0.0000e+00 - val_loss: 0.3623 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.3622 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8905 - accuracy: 0.0000e+00 - val_loss: 0.3618 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8901 - accuracy: 0.0000e+00 - val_loss: 0.3618 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8895 - accuracy: 0.0000e+00 - val_loss: 0.3615 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8890 - accuracy: 0.0000e+00 - val_loss: 0.3613 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8885 - accuracy: 0.0000e+00 - val_loss: 0.3612 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8880 - accuracy: 0.0000e+00 - val_loss: 0.3612 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8874 - accuracy: 0.0000e+00 - val_loss: 0.3611 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8870 - accuracy: 0.0000e+00 - val_loss: 0.3609 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8864 - accuracy: 0.0000e+00 - val_loss: 0.3611 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8860 - accuracy: 0.0000e+00 - val_loss: 0.3609 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8854 - accuracy: 0.0000e+00 - val_loss: 0.3612 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.3612 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.3612 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8841 - accuracy: 0.0000e+00 - val_loss: 0.3615 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8834 - accuracy: 0.0000e+00 - val_loss: 0.3610 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8829 - accuracy: 0.0000e+00 - val_loss: 0.3610 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8825 - accuracy: 0.0000e+00 - val_loss: 0.3614 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8819 - accuracy: 0.0000e+00 - val_loss: 0.3620 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - val_loss: 0.3622 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8809 - accuracy: 0.0000e+00 - val_loss: 0.3623 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8803 - accuracy: 0.0000e+00 - val_loss: 0.3625 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.3630 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8793 - accuracy: 0.0000e+00 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8790 - accuracy: 0.0000e+00 - val_loss: 0.3636 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8784 - accuracy: 0.0000e+00 - val_loss: 0.3632 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8781 - accuracy: 0.0000e+00 - val_loss: 0.3640 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8776 - accuracy: 0.0000e+00 - val_loss: 0.3642 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8771 - accuracy: 0.0000e+00 - val_loss: 0.3652 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.3662 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8764 - accuracy: 0.0000e+00 - val_loss: 0.3670 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8757 - accuracy: 0.0000e+00 - val_loss: 0.3670 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.3681 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.3687 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.3691 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8741 - accuracy: 0.0000e+00 - val_loss: 0.3700 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.3715 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.3722 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8730 - accuracy: 0.0000e+00 - val_loss: 0.3727 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8726 - accuracy: 0.0000e+00 - val_loss: 0.3736 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8723 - accuracy: 0.0000e+00 - val_loss: 0.3741 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8719 - accuracy: 0.0000e+00 - val_loss: 0.3742 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8716 - accuracy: 0.0000e+00 - val_loss: 0.3753 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8712 - accuracy: 0.0000e+00 - val_loss: 0.3764 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8709 - accuracy: 0.0000e+00 - val_loss: 0.3770 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8705 - accuracy: 0.0000e+00 - val_loss: 0.3770 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8702 - accuracy: 0.0000e+00 - val_loss: 0.3776 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8698 - accuracy: 0.0000e+00 - val_loss: 0.3778 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8696 - accuracy: 0.0000e+00 - val_loss: 0.3784 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8693 - accuracy: 0.0000e+00 - val_loss: 0.3792 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8689 - accuracy: 0.0000e+00 - val_loss: 0.3798 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8686 - accuracy: 0.0000e+00 - val_loss: 0.3804 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8684 - accuracy: 0.0000e+00 - val_loss: 0.3814 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8680 - accuracy: 0.0000e+00 - val_loss: 0.3820 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8677 - accuracy: 0.0000e+00 - val_loss: 0.3836 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8674 - accuracy: 0.0000e+00 - val_loss: 0.3843 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8672 - accuracy: 0.0000e+00 - val_loss: 0.3842 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8668 - accuracy: 0.0000e+00 - val_loss: 0.3855 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8666 - accuracy: 0.0000e+00 - val_loss: 0.3870 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8663 - accuracy: 0.0000e+00 - val_loss: 0.3877 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8661 - accuracy: 0.0000e+00 - val_loss: 0.3868 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8658 - accuracy: 0.0000e+00 - val_loss: 0.3869 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8655 - accuracy: 0.0000e+00 - val_loss: 0.3885 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8653 - accuracy: 0.0000e+00 - val_loss: 0.3882 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8650 - accuracy: 0.0000e+00 - val_loss: 0.3884 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8648 - accuracy: 0.0000e+00 - val_loss: 0.3891 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8645 - accuracy: 0.0000e+00 - val_loss: 0.3896 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8643 - accuracy: 0.0000e+00 - val_loss: 0.3911 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8641 - accuracy: 0.0000e+00 - val_loss: 0.3920 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8638 - accuracy: 0.0000e+00 - val_loss: 0.3925 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8636 - accuracy: 0.0000e+00 - val_loss: 0.3936 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8634 - accuracy: 0.0000e+00 - val_loss: 0.3950 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8631 - accuracy: 0.0000e+00 - val_loss: 0.3953 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8629 - accuracy: 0.0000e+00 - val_loss: 0.3967 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8627 - accuracy: 0.0000e+00 - val_loss: 0.3973 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8625 - accuracy: 0.0000e+00 - val_loss: 0.3977 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8623 - accuracy: 0.0000e+00 - val_loss: 0.3982 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8621 - accuracy: 0.0000e+00 - val_loss: 0.4000 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8619 - accuracy: 0.0000e+00 - val_loss: 0.4006 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8617 - accuracy: 0.0000e+00 - val_loss: 0.4009 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8616 - accuracy: 0.0000e+00 - val_loss: 0.4010 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8614 - accuracy: 0.0000e+00 - val_loss: 0.4017 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8611 - accuracy: 0.0000e+00 - val_loss: 0.4023 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8610 - accuracy: 0.0000e+00 - val_loss: 0.4025 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8608 - accuracy: 0.0000e+00 - val_loss: 0.4025 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8607 - accuracy: 0.0000e+00 - val_loss: 0.4031 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8605 - accuracy: 0.0000e+00 - val_loss: 0.4036 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8603 - accuracy: 0.0000e+00 - val_loss: 0.4039 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8601 - accuracy: 0.0000e+00 - val_loss: 0.4046 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8600 - accuracy: 0.0000e+00 - val_loss: 0.4055 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8598 - accuracy: 0.0000e+00 - val_loss: 0.4057 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8597 - accuracy: 0.0000e+00 - val_loss: 0.4059 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.4063 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8594 - accuracy: 0.0000e+00 - val_loss: 0.4070 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8593 - accuracy: 0.0000e+00 - val_loss: 0.4069 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8591 - accuracy: 0.0000e+00 - val_loss: 0.4072 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8590 - accuracy: 0.0000e+00 - val_loss: 0.4077 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8589 - accuracy: 0.0000e+00 - val_loss: 0.4079 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8587 - accuracy: 0.0000e+00 - val_loss: 0.4082 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8586 - accuracy: 0.0000e+00 - val_loss: 0.4082 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8585 - accuracy: 0.0000e+00 - val_loss: 0.4086 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8583 - accuracy: 0.0000e+00 - val_loss: 0.4087 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8582 - accuracy: 0.0000e+00 - val_loss: 0.4087 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8581 - accuracy: 0.0000e+00 - val_loss: 0.4094 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8580 - accuracy: 0.0000e+00 - val_loss: 0.4098 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8578 - accuracy: 0.0000e+00 - val_loss: 0.4092 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8577 - accuracy: 0.0000e+00 - val_loss: 0.4097 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8576 - accuracy: 0.0000e+00 - val_loss: 0.4099 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8575 - accuracy: 0.0000e+00 - val_loss: 0.4104 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8574 - accuracy: 0.0000e+00 - val_loss: 0.4107 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8573 - accuracy: 0.0000e+00 - val_loss: 0.4104 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8571 - accuracy: 0.0000e+00 - val_loss: 0.4105 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8570 - accuracy: 0.0000e+00 - val_loss: 0.4108 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8570 - accuracy: 0.0000e+00 - val_loss: 0.4107 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8568 - accuracy: 0.0000e+00 - val_loss: 0.4109 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8567 - accuracy: 0.0000e+00 - val_loss: 0.4112 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8566 - accuracy: 0.0000e+00 - val_loss: 0.4114 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8565 - accuracy: 0.0000e+00 - val_loss: 0.4114 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8564 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8563 - accuracy: 0.0000e+00 - val_loss: 0.4123 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8562 - accuracy: 0.0000e+00 - val_loss: 0.4125 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8561 - accuracy: 0.0000e+00 - val_loss: 0.4122 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8560 - accuracy: 0.0000e+00 - val_loss: 0.4124 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8559 - accuracy: 0.0000e+00 - val_loss: 0.4130 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8558 - accuracy: 0.0000e+00 - val_loss: 0.4127 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8557 - accuracy: 0.0000e+00 - val_loss: 0.4128 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8557 - accuracy: 0.0000e+00 - val_loss: 0.4134 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8555 - accuracy: 0.0000e+00 - val_loss: 0.4137 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8554 - accuracy: 0.0000e+00 - val_loss: 0.4139 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8554 - accuracy: 0.0000e+00 - val_loss: 0.4137 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8553 - accuracy: 0.0000e+00 - val_loss: 0.4140 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8552 - accuracy: 0.0000e+00 - val_loss: 0.4141 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8551 - accuracy: 0.0000e+00 - val_loss: 0.4142 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8550 - accuracy: 0.0000e+00 - val_loss: 0.4143 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8549 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8548 - accuracy: 0.0000e+00 - val_loss: 0.4148 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8547 - accuracy: 0.0000e+00 - val_loss: 0.4149 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8547 - accuracy: 0.0000e+00 - val_loss: 0.4150 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8546 - accuracy: 0.0000e+00 - val_loss: 0.4154 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8545 - accuracy: 0.0000e+00 - val_loss: 0.4151 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8544 - accuracy: 0.0000e+00 - val_loss: 0.4152 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8543 - accuracy: 0.0000e+00 - val_loss: 0.4153 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8543 - accuracy: 0.0000e+00 - val_loss: 0.4153 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8542 - accuracy: 0.0000e+00 - val_loss: 0.4160 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8541 - accuracy: 0.0000e+00 - val_loss: 0.4159 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8540 - accuracy: 0.0000e+00 - val_loss: 0.4161 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.4163 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.4161 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8538 - accuracy: 0.0000e+00 - val_loss: 0.4161 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8537 - accuracy: 0.0000e+00 - val_loss: 0.4163 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8536 - accuracy: 0.0000e+00 - val_loss: 0.4163 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8536 - accuracy: 0.0000e+00 - val_loss: 0.4164 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8535 - accuracy: 0.0000e+00 - val_loss: 0.4166 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8534 - accuracy: 0.0000e+00 - val_loss: 0.4171 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8534 - accuracy: 0.0000e+00 - val_loss: 0.4168 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8533 - accuracy: 0.0000e+00 - val_loss: 0.4173 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8532 - accuracy: 0.0000e+00 - val_loss: 0.4171 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/ADP_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4171 - accuracy: 0.0000e+00\n",
      "[0.4170699715614319, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_70 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.2184 - accuracy: 0.0000e+00 - val_loss: 0.7947 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2000 - accuracy: 0.0000e+00 - val_loss: 0.7943 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1794 - accuracy: 0.0000e+00 - val_loss: 0.7904 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1648 - accuracy: 0.0000e+00 - val_loss: 0.7820 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1479 - accuracy: 0.0000e+00 - val_loss: 0.7798 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1350 - accuracy: 0.0000e+00 - val_loss: 0.7770 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1200 - accuracy: 0.0000e+00 - val_loss: 0.7793 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1098 - accuracy: 0.0000e+00 - val_loss: 0.7809 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0980 - accuracy: 0.0000e+00 - val_loss: 0.7850 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0859 - accuracy: 0.0000e+00 - val_loss: 0.7896 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0750 - accuracy: 0.0000e+00 - val_loss: 0.7937 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0676 - accuracy: 0.0000e+00 - val_loss: 0.8003 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0600 - accuracy: 0.0000e+00 - val_loss: 0.8065 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0519 - accuracy: 0.0000e+00 - val_loss: 0.8107 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0444 - accuracy: 0.0000e+00 - val_loss: 0.8162 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0381 - accuracy: 0.0000e+00 - val_loss: 0.8212 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0311 - accuracy: 0.0000e+00 - val_loss: 0.8251 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0256 - accuracy: 0.0000e+00 - val_loss: 0.8289 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0208 - accuracy: 0.0000e+00 - val_loss: 0.8324 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0149 - accuracy: 0.0000e+00 - val_loss: 0.8356 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0092 - accuracy: 0.0000e+00 - val_loss: 0.8388 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0047 - accuracy: 0.0000e+00 - val_loss: 0.8414 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0004 - accuracy: 0.0000e+00 - val_loss: 0.8449 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9954 - accuracy: 0.0000e+00 - val_loss: 0.8478 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.9909 - accuracy: 0.0000e+00 - val_loss: 0.8504 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9861 - accuracy: 0.0000e+00 - val_loss: 0.8530 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9819 - accuracy: 0.0000e+00 - val_loss: 0.8554 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9787 - accuracy: 0.0000e+00 - val_loss: 0.8570 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9736 - accuracy: 0.0000e+00 - val_loss: 0.8593 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9697 - accuracy: 0.0000e+00 - val_loss: 0.8616 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9656 - accuracy: 0.0000e+00 - val_loss: 0.8629 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9625 - accuracy: 0.0000e+00 - val_loss: 0.8645 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9593 - accuracy: 0.0000e+00 - val_loss: 0.8664 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9554 - accuracy: 0.0000e+00 - val_loss: 0.8686 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9513 - accuracy: 0.0000e+00 - val_loss: 0.8699 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9481 - accuracy: 0.0000e+00 - val_loss: 0.8711 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9442 - accuracy: 0.0000e+00 - val_loss: 0.8725 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9410 - accuracy: 0.0000e+00 - val_loss: 0.8741 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9378 - accuracy: 0.0000e+00 - val_loss: 0.8754 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9344 - accuracy: 0.0000e+00 - val_loss: 0.8767 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9316 - accuracy: 0.0000e+00 - val_loss: 0.8776 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9287 - accuracy: 0.0000e+00 - val_loss: 0.8789 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9255 - accuracy: 0.0000e+00 - val_loss: 0.8796 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9225 - accuracy: 0.0000e+00 - val_loss: 0.8807 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9200 - accuracy: 0.0000e+00 - val_loss: 0.8820 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9173 - accuracy: 0.0000e+00 - val_loss: 0.8826 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9150 - accuracy: 0.0000e+00 - val_loss: 0.8836 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9123 - accuracy: 0.0000e+00 - val_loss: 0.8843 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9105 - accuracy: 0.0000e+00 - val_loss: 0.8853 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9078 - accuracy: 0.0000e+00 - val_loss: 0.8858 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9060 - accuracy: 0.0000e+00 - val_loss: 0.8865 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9034 - accuracy: 0.0000e+00 - val_loss: 0.8872 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9011 - accuracy: 0.0000e+00 - val_loss: 0.8878 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8991 - accuracy: 0.0000e+00 - val_loss: 0.8886 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8975 - accuracy: 0.0000e+00 - val_loss: 0.8892 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8954 - accuracy: 0.0000e+00 - val_loss: 0.8895 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8933 - accuracy: 0.0000e+00 - val_loss: 0.8902 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8913 - accuracy: 0.0000e+00 - val_loss: 0.8906 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8899 - accuracy: 0.0000e+00 - val_loss: 0.8912 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.8916 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8868 - accuracy: 0.0000e+00 - val_loss: 0.8920 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.8923 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8839 - accuracy: 0.0000e+00 - val_loss: 0.8926 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8826 - accuracy: 0.0000e+00 - val_loss: 0.8930 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8814 - accuracy: 0.0000e+00 - val_loss: 0.8934 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8801 - accuracy: 0.0000e+00 - val_loss: 0.8936 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.8939 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.8941 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8765 - accuracy: 0.0000e+00 - val_loss: 0.8945 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.8948 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8744 - accuracy: 0.0000e+00 - val_loss: 0.8950 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8735 - accuracy: 0.0000e+00 - val_loss: 0.8953 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8724 - accuracy: 0.0000e+00 - val_loss: 0.8954 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8713 - accuracy: 0.0000e+00 - val_loss: 0.8956 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8706 - accuracy: 0.0000e+00 - val_loss: 0.8958 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8697 - accuracy: 0.0000e+00 - val_loss: 0.8961 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8688 - accuracy: 0.0000e+00 - val_loss: 0.8960 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8680 - accuracy: 0.0000e+00 - val_loss: 0.8963 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8672 - accuracy: 0.0000e+00 - val_loss: 0.8964 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8666 - accuracy: 0.0000e+00 - val_loss: 0.8967 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8657 - accuracy: 0.0000e+00 - val_loss: 0.8967 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8650 - accuracy: 0.0000e+00 - val_loss: 0.8967 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8642 - accuracy: 0.0000e+00 - val_loss: 0.8970 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8635 - accuracy: 0.0000e+00 - val_loss: 0.8972 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8629 - accuracy: 0.0000e+00 - val_loss: 0.8974 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8623 - accuracy: 0.0000e+00 - val_loss: 0.8974 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8617 - accuracy: 0.0000e+00 - val_loss: 0.8976 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8610 - accuracy: 0.0000e+00 - val_loss: 0.8979 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8604 - accuracy: 0.0000e+00 - val_loss: 0.8980 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8596 - accuracy: 0.0000e+00 - val_loss: 0.8981 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8591 - accuracy: 0.0000e+00 - val_loss: 0.8983 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8586 - accuracy: 0.0000e+00 - val_loss: 0.8982 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8581 - accuracy: 0.0000e+00 - val_loss: 0.8983 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8576 - accuracy: 0.0000e+00 - val_loss: 0.8984 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8571 - accuracy: 0.0000e+00 - val_loss: 0.8986 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8563 - accuracy: 0.0000e+00 - val_loss: 0.8986 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8560 - accuracy: 0.0000e+00 - val_loss: 0.8986 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8555 - accuracy: 0.0000e+00 - val_loss: 0.8987 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8550 - accuracy: 0.0000e+00 - val_loss: 0.8987 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8544 - accuracy: 0.0000e+00 - val_loss: 0.8987 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8539 - accuracy: 0.0000e+00 - val_loss: 0.8989 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8536 - accuracy: 0.0000e+00 - val_loss: 0.8990 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8529 - accuracy: 0.0000e+00 - val_loss: 0.8991 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8527 - accuracy: 0.0000e+00 - val_loss: 0.8992 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8521 - accuracy: 0.0000e+00 - val_loss: 0.8993 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8517 - accuracy: 0.0000e+00 - val_loss: 0.8994 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8513 - accuracy: 0.0000e+00 - val_loss: 0.8994 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8509 - accuracy: 0.0000e+00 - val_loss: 0.8995 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8504 - accuracy: 0.0000e+00 - val_loss: 0.8996 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8499 - accuracy: 0.0000e+00 - val_loss: 0.8996 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8496 - accuracy: 0.0000e+00 - val_loss: 0.8997 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8493 - accuracy: 0.0000e+00 - val_loss: 0.8998 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8489 - accuracy: 0.0000e+00 - val_loss: 0.9000 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8485 - accuracy: 0.0000e+00 - val_loss: 0.9001 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8482 - accuracy: 0.0000e+00 - val_loss: 0.9003 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8477 - accuracy: 0.0000e+00 - val_loss: 0.9004 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8474 - accuracy: 0.0000e+00 - val_loss: 0.9005 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8471 - accuracy: 0.0000e+00 - val_loss: 0.9006 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8467 - accuracy: 0.0000e+00 - val_loss: 0.9008 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8464 - accuracy: 0.0000e+00 - val_loss: 0.9009 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8460 - accuracy: 0.0000e+00 - val_loss: 0.9013 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8457 - accuracy: 0.0000e+00 - val_loss: 0.9017 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8454 - accuracy: 0.0000e+00 - val_loss: 0.9017 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8451 - accuracy: 0.0000e+00 - val_loss: 0.9020 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8448 - accuracy: 0.0000e+00 - val_loss: 0.9022 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8445 - accuracy: 0.0000e+00 - val_loss: 0.9028 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8442 - accuracy: 0.0000e+00 - val_loss: 0.9026 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8438 - accuracy: 0.0000e+00 - val_loss: 0.9033 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8436 - accuracy: 0.0000e+00 - val_loss: 0.9039 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8433 - accuracy: 0.0000e+00 - val_loss: 0.9048 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8430 - accuracy: 0.0000e+00 - val_loss: 0.9049 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8429 - accuracy: 0.0000e+00 - val_loss: 0.9057 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.9059 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8421 - accuracy: 0.0000e+00 - val_loss: 0.9070 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8418 - accuracy: 0.0000e+00 - val_loss: 0.9072 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8416 - accuracy: 0.0000e+00 - val_loss: 0.9082 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8413 - accuracy: 0.0000e+00 - val_loss: 0.9082 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8411 - accuracy: 0.0000e+00 - val_loss: 0.9082 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8408 - accuracy: 0.0000e+00 - val_loss: 0.9086 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8407 - accuracy: 0.0000e+00 - val_loss: 0.9100 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8403 - accuracy: 0.0000e+00 - val_loss: 0.9113 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8400 - accuracy: 0.0000e+00 - val_loss: 0.9119 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8400 - accuracy: 0.0000e+00 - val_loss: 0.9107 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8397 - accuracy: 0.0000e+00 - val_loss: 0.9111 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8393 - accuracy: 0.0000e+00 - val_loss: 0.9130 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8391 - accuracy: 0.0000e+00 - val_loss: 0.9137 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8388 - accuracy: 0.0000e+00 - val_loss: 0.9141 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8386 - accuracy: 0.0000e+00 - val_loss: 0.9153 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8384 - accuracy: 0.0000e+00 - val_loss: 0.9157 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8382 - accuracy: 0.0000e+00 - val_loss: 0.9165 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8379 - accuracy: 0.0000e+00 - val_loss: 0.9167 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8377 - accuracy: 0.0000e+00 - val_loss: 0.9171 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8374 - accuracy: 0.0000e+00 - val_loss: 0.9175 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8373 - accuracy: 0.0000e+00 - val_loss: 0.9180 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8370 - accuracy: 0.0000e+00 - val_loss: 0.9185 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8369 - accuracy: 0.0000e+00 - val_loss: 0.9188 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8366 - accuracy: 0.0000e+00 - val_loss: 0.9190 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8364 - accuracy: 0.0000e+00 - val_loss: 0.9196 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8362 - accuracy: 0.0000e+00 - val_loss: 0.9200 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8360 - accuracy: 0.0000e+00 - val_loss: 0.9209 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8358 - accuracy: 0.0000e+00 - val_loss: 0.9211 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8357 - accuracy: 0.0000e+00 - val_loss: 0.9209 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8354 - accuracy: 0.0000e+00 - val_loss: 0.9209 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8352 - accuracy: 0.0000e+00 - val_loss: 0.9212 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8350 - accuracy: 0.0000e+00 - val_loss: 0.9217 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8349 - accuracy: 0.0000e+00 - val_loss: 0.9223 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8347 - accuracy: 0.0000e+00 - val_loss: 0.9230 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8346 - accuracy: 0.0000e+00 - val_loss: 0.9232 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8343 - accuracy: 0.0000e+00 - val_loss: 0.9234 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8341 - accuracy: 0.0000e+00 - val_loss: 0.9238 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8341 - accuracy: 0.0000e+00 - val_loss: 0.9240 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8339 - accuracy: 0.0000e+00 - val_loss: 0.9244 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8337 - accuracy: 0.0000e+00 - val_loss: 0.9243 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8335 - accuracy: 0.0000e+00 - val_loss: 0.9247 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8334 - accuracy: 0.0000e+00 - val_loss: 0.9247 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8332 - accuracy: 0.0000e+00 - val_loss: 0.9255 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8331 - accuracy: 0.0000e+00 - val_loss: 0.9253 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8329 - accuracy: 0.0000e+00 - val_loss: 0.9254 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8327 - accuracy: 0.0000e+00 - val_loss: 0.9255 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.9258 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.9262 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8323 - accuracy: 0.0000e+00 - val_loss: 0.9263 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8321 - accuracy: 0.0000e+00 - val_loss: 0.9262 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8319 - accuracy: 0.0000e+00 - val_loss: 0.9262 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8318 - accuracy: 0.0000e+00 - val_loss: 0.9266 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8317 - accuracy: 0.0000e+00 - val_loss: 0.9268 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8315 - accuracy: 0.0000e+00 - val_loss: 0.9268 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8314 - accuracy: 0.0000e+00 - val_loss: 0.9269 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8313 - accuracy: 0.0000e+00 - val_loss: 0.9268 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8311 - accuracy: 0.0000e+00 - val_loss: 0.9269 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8310 - accuracy: 0.0000e+00 - val_loss: 0.9273 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8309 - accuracy: 0.0000e+00 - val_loss: 0.9275 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8307 - accuracy: 0.0000e+00 - val_loss: 0.9275 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.9275 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8305 - accuracy: 0.0000e+00 - val_loss: 0.9276 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8304 - accuracy: 0.0000e+00 - val_loss: 0.9279 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8302 - accuracy: 0.0000e+00 - val_loss: 0.9280 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8301 - accuracy: 0.0000e+00 - val_loss: 0.9280 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8300 - accuracy: 0.0000e+00 - val_loss: 0.9282 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8298 - accuracy: 0.0000e+00 - val_loss: 0.9282 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8297 - accuracy: 0.0000e+00 - val_loss: 0.9283 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8297 - accuracy: 0.0000e+00 - val_loss: 0.9282 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8295 - accuracy: 0.0000e+00 - val_loss: 0.9285 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8294 - accuracy: 0.0000e+00 - val_loss: 0.9285 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8293 - accuracy: 0.0000e+00 - val_loss: 0.9286 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8292 - accuracy: 0.0000e+00 - val_loss: 0.9285 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8291 - accuracy: 0.0000e+00 - val_loss: 0.9286 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8289 - accuracy: 0.0000e+00 - val_loss: 0.9287 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8288 - accuracy: 0.0000e+00 - val_loss: 0.9289 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8287 - accuracy: 0.0000e+00 - val_loss: 0.9289 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8287 - accuracy: 0.0000e+00 - val_loss: 0.9290 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8285 - accuracy: 0.0000e+00 - val_loss: 0.9291 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8285 - accuracy: 0.0000e+00 - val_loss: 0.9291 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8283 - accuracy: 0.0000e+00 - val_loss: 0.9292 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8282 - accuracy: 0.0000e+00 - val_loss: 0.9292 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8282 - accuracy: 0.0000e+00 - val_loss: 0.9291 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8280 - accuracy: 0.0000e+00 - val_loss: 0.9292 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8280 - accuracy: 0.0000e+00 - val_loss: 0.9293 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8279 - accuracy: 0.0000e+00 - val_loss: 0.9295 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8278 - accuracy: 0.0000e+00 - val_loss: 0.9294 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8277 - accuracy: 0.0000e+00 - val_loss: 0.9294 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8276 - accuracy: 0.0000e+00 - val_loss: 0.9294 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8275 - accuracy: 0.0000e+00 - val_loss: 0.9294 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8274 - accuracy: 0.0000e+00 - val_loss: 0.9296 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8274 - accuracy: 0.0000e+00 - val_loss: 0.9295 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8272 - accuracy: 0.0000e+00 - val_loss: 0.9295 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8272 - accuracy: 0.0000e+00 - val_loss: 0.9296 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8271 - accuracy: 0.0000e+00 - val_loss: 0.9296 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8271 - accuracy: 0.0000e+00 - val_loss: 0.9296 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8270 - accuracy: 0.0000e+00 - val_loss: 0.9297 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8269 - accuracy: 0.0000e+00 - val_loss: 0.9297 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8268 - accuracy: 0.0000e+00 - val_loss: 0.9298 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8267 - accuracy: 0.0000e+00 - val_loss: 0.9298 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8266 - accuracy: 0.0000e+00 - val_loss: 0.9298 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8266 - accuracy: 0.0000e+00 - val_loss: 0.9298 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8265 - accuracy: 0.0000e+00 - val_loss: 0.9299 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8264 - accuracy: 0.0000e+00 - val_loss: 0.9299 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.9298 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8263 - accuracy: 0.0000e+00 - val_loss: 0.9299 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8262 - accuracy: 0.0000e+00 - val_loss: 0.9299 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8261 - accuracy: 0.0000e+00 - val_loss: 0.9300 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8261 - accuracy: 0.0000e+00 - val_loss: 0.9300 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8260 - accuracy: 0.0000e+00 - val_loss: 0.9300 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8260 - accuracy: 0.0000e+00 - val_loss: 0.9300 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8259 - accuracy: 0.0000e+00 - val_loss: 0.9301 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8258 - accuracy: 0.0000e+00 - val_loss: 0.9301 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8258 - accuracy: 0.0000e+00 - val_loss: 0.9301 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8257 - accuracy: 0.0000e+00 - val_loss: 0.9302 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8257 - accuracy: 0.0000e+00 - val_loss: 0.9301 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8256 - accuracy: 0.0000e+00 - val_loss: 0.9301 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8255 - accuracy: 0.0000e+00 - val_loss: 0.9302 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8255 - accuracy: 0.0000e+00 - val_loss: 0.9302 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8254 - accuracy: 0.0000e+00 - val_loss: 0.9302 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8253 - accuracy: 0.0000e+00 - val_loss: 0.9302 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8253 - accuracy: 0.0000e+00 - val_loss: 0.9302 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8252 - accuracy: 0.0000e+00 - val_loss: 0.9303 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/SBUX_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9303 - accuracy: 0.0000e+00\n",
      "[0.9302896857261658, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_72 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 18ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/CI_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 20ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_74 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 18ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 13ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/AMD_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 21ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_76 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 23ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/AXP_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_78 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.3351 - accuracy: 0.0000e+00 - val_loss: 0.3522 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.3140 - accuracy: 0.0000e+00 - val_loss: 0.3512 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2992 - accuracy: 0.0000e+00 - val_loss: 0.3499 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.2800 - accuracy: 0.0000e+00 - val_loss: 0.3489 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2663 - accuracy: 0.0000e+00 - val_loss: 0.3482 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2491 - accuracy: 0.0000e+00 - val_loss: 0.3480 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.2339 - accuracy: 0.0000e+00 - val_loss: 0.3479 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.2190 - accuracy: 0.0000e+00 - val_loss: 0.3481 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.2040 - accuracy: 0.0000e+00 - val_loss: 0.3489 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.1915 - accuracy: 0.0000e+00 - val_loss: 0.3499 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1792 - accuracy: 0.0000e+00 - val_loss: 0.3511 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1650 - accuracy: 0.0000e+00 - val_loss: 0.3522 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1535 - accuracy: 0.0000e+00 - val_loss: 0.3535 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1420 - accuracy: 0.0000e+00 - val_loss: 0.3550 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.1304 - accuracy: 0.0000e+00 - val_loss: 0.3569 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1202 - accuracy: 0.0000e+00 - val_loss: 0.3588 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.1100 - accuracy: 0.0000e+00 - val_loss: 0.3604 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.1001 - accuracy: 0.0000e+00 - val_loss: 0.3625 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0916 - accuracy: 0.0000e+00 - val_loss: 0.3643 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0837 - accuracy: 0.0000e+00 - val_loss: 0.3667 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0745 - accuracy: 0.0000e+00 - val_loss: 0.3687 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0669 - accuracy: 0.0000e+00 - val_loss: 0.3704 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0607 - accuracy: 0.0000e+00 - val_loss: 0.3724 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0538 - accuracy: 0.0000e+00 - val_loss: 0.3747 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0484 - accuracy: 0.0000e+00 - val_loss: 0.3770 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0423 - accuracy: 0.0000e+00 - val_loss: 0.3791 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0370 - accuracy: 0.0000e+00 - val_loss: 0.3808 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0303 - accuracy: 0.0000e+00 - val_loss: 0.3827 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0266 - accuracy: 0.0000e+00 - val_loss: 0.3846 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 1.0211 - accuracy: 0.0000e+00 - val_loss: 0.3862 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 1.0171 - accuracy: 0.0000e+00 - val_loss: 0.3881 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 1.0130 - accuracy: 0.0000e+00 - val_loss: 0.3900 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 1.0090 - accuracy: 0.0000e+00 - val_loss: 0.3914 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 1.0063 - accuracy: 0.0000e+00 - val_loss: 0.3932 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 1.0023 - accuracy: 0.0000e+00 - val_loss: 0.3948 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9986 - accuracy: 0.0000e+00 - val_loss: 0.3960 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9961 - accuracy: 0.0000e+00 - val_loss: 0.3976 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9928 - accuracy: 0.0000e+00 - val_loss: 0.3990 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9897 - accuracy: 0.0000e+00 - val_loss: 0.4004 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.9878 - accuracy: 0.0000e+00 - val_loss: 0.4017 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9850 - accuracy: 0.0000e+00 - val_loss: 0.4028 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9826 - accuracy: 0.0000e+00 - val_loss: 0.4043 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9803 - accuracy: 0.0000e+00 - val_loss: 0.4051 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9779 - accuracy: 0.0000e+00 - val_loss: 0.4063 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9762 - accuracy: 0.0000e+00 - val_loss: 0.4075 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9739 - accuracy: 0.0000e+00 - val_loss: 0.4082 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9723 - accuracy: 0.0000e+00 - val_loss: 0.4089 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9706 - accuracy: 0.0000e+00 - val_loss: 0.4097 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9689 - accuracy: 0.0000e+00 - val_loss: 0.4108 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9671 - accuracy: 0.0000e+00 - val_loss: 0.4117 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9654 - accuracy: 0.0000e+00 - val_loss: 0.4120 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9637 - accuracy: 0.0000e+00 - val_loss: 0.4124 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9623 - accuracy: 0.0000e+00 - val_loss: 0.4134 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9612 - accuracy: 0.0000e+00 - val_loss: 0.4140 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9591 - accuracy: 0.0000e+00 - val_loss: 0.4144 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9583 - accuracy: 0.0000e+00 - val_loss: 0.4147 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9566 - accuracy: 0.0000e+00 - val_loss: 0.4153 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9553 - accuracy: 0.0000e+00 - val_loss: 0.4154 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9539 - accuracy: 0.0000e+00 - val_loss: 0.4162 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9526 - accuracy: 0.0000e+00 - val_loss: 0.4167 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9515 - accuracy: 0.0000e+00 - val_loss: 0.4172 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9508 - accuracy: 0.0000e+00 - val_loss: 0.4170 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9490 - accuracy: 0.0000e+00 - val_loss: 0.4172 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9478 - accuracy: 0.0000e+00 - val_loss: 0.4177 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9469 - accuracy: 0.0000e+00 - val_loss: 0.4181 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9457 - accuracy: 0.0000e+00 - val_loss: 0.4181 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9442 - accuracy: 0.0000e+00 - val_loss: 0.4183 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9435 - accuracy: 0.0000e+00 - val_loss: 0.4185 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9421 - accuracy: 0.0000e+00 - val_loss: 0.4192 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9412 - accuracy: 0.0000e+00 - val_loss: 0.4193 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9399 - accuracy: 0.0000e+00 - val_loss: 0.4193 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9388 - accuracy: 0.0000e+00 - val_loss: 0.4197 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9376 - accuracy: 0.0000e+00 - val_loss: 0.4200 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9367 - accuracy: 0.0000e+00 - val_loss: 0.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9357 - accuracy: 0.0000e+00 - val_loss: 0.4210 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9347 - accuracy: 0.0000e+00 - val_loss: 0.4210 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9337 - accuracy: 0.0000e+00 - val_loss: 0.4213 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9327 - accuracy: 0.0000e+00 - val_loss: 0.4217 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9317 - accuracy: 0.0000e+00 - val_loss: 0.4221 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9309 - accuracy: 0.0000e+00 - val_loss: 0.4222 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9299 - accuracy: 0.0000e+00 - val_loss: 0.4225 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9289 - accuracy: 0.0000e+00 - val_loss: 0.4229 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9282 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9270 - accuracy: 0.0000e+00 - val_loss: 0.4228 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9262 - accuracy: 0.0000e+00 - val_loss: 0.4234 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9253 - accuracy: 0.0000e+00 - val_loss: 0.4233 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9245 - accuracy: 0.0000e+00 - val_loss: 0.4239 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9235 - accuracy: 0.0000e+00 - val_loss: 0.4243 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9226 - accuracy: 0.0000e+00 - val_loss: 0.4247 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9220 - accuracy: 0.0000e+00 - val_loss: 0.4249 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9211 - accuracy: 0.0000e+00 - val_loss: 0.4254 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9204 - accuracy: 0.0000e+00 - val_loss: 0.4250 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9197 - accuracy: 0.0000e+00 - val_loss: 0.4246 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9188 - accuracy: 0.0000e+00 - val_loss: 0.4249 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9179 - accuracy: 0.0000e+00 - val_loss: 0.4257 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9172 - accuracy: 0.0000e+00 - val_loss: 0.4258 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9166 - accuracy: 0.0000e+00 - val_loss: 0.4261 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9156 - accuracy: 0.0000e+00 - val_loss: 0.4266 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9149 - accuracy: 0.0000e+00 - val_loss: 0.4269 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9143 - accuracy: 0.0000e+00 - val_loss: 0.4271 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9139 - accuracy: 0.0000e+00 - val_loss: 0.4271 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9131 - accuracy: 0.0000e+00 - val_loss: 0.4275 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9122 - accuracy: 0.0000e+00 - val_loss: 0.4280 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9117 - accuracy: 0.0000e+00 - val_loss: 0.4283 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9111 - accuracy: 0.0000e+00 - val_loss: 0.4288 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9104 - accuracy: 0.0000e+00 - val_loss: 0.4294 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9097 - accuracy: 0.0000e+00 - val_loss: 0.4294 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9092 - accuracy: 0.0000e+00 - val_loss: 0.4297 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9086 - accuracy: 0.0000e+00 - val_loss: 0.4301 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9079 - accuracy: 0.0000e+00 - val_loss: 0.4303 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9075 - accuracy: 0.0000e+00 - val_loss: 0.4308 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9069 - accuracy: 0.0000e+00 - val_loss: 0.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9063 - accuracy: 0.0000e+00 - val_loss: 0.4315 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9061 - accuracy: 0.0000e+00 - val_loss: 0.4312 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9052 - accuracy: 0.0000e+00 - val_loss: 0.4315 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9047 - accuracy: 0.0000e+00 - val_loss: 0.4320 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9043 - accuracy: 0.0000e+00 - val_loss: 0.4324 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.9038 - accuracy: 0.0000e+00 - val_loss: 0.4325 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9033 - accuracy: 0.0000e+00 - val_loss: 0.4328 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.9027 - accuracy: 0.0000e+00 - val_loss: 0.4332 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.9023 - accuracy: 0.0000e+00 - val_loss: 0.4339 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.9019 - accuracy: 0.0000e+00 - val_loss: 0.4341 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9014 - accuracy: 0.0000e+00 - val_loss: 0.4348 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9010 - accuracy: 0.0000e+00 - val_loss: 0.4353 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.9006 - accuracy: 0.0000e+00 - val_loss: 0.4360 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.9002 - accuracy: 0.0000e+00 - val_loss: 0.4359 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8996 - accuracy: 0.0000e+00 - val_loss: 0.4365 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8993 - accuracy: 0.0000e+00 - val_loss: 0.4368 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8988 - accuracy: 0.0000e+00 - val_loss: 0.4373 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8984 - accuracy: 0.0000e+00 - val_loss: 0.4377 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8981 - accuracy: 0.0000e+00 - val_loss: 0.4382 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8976 - accuracy: 0.0000e+00 - val_loss: 0.4388 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8973 - accuracy: 0.0000e+00 - val_loss: 0.4395 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8968 - accuracy: 0.0000e+00 - val_loss: 0.4399 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8965 - accuracy: 0.0000e+00 - val_loss: 0.4403 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8961 - accuracy: 0.0000e+00 - val_loss: 0.4407 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8957 - accuracy: 0.0000e+00 - val_loss: 0.4415 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8954 - accuracy: 0.0000e+00 - val_loss: 0.4417 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8949 - accuracy: 0.0000e+00 - val_loss: 0.4423 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8946 - accuracy: 0.0000e+00 - val_loss: 0.4428 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8943 - accuracy: 0.0000e+00 - val_loss: 0.4433 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8939 - accuracy: 0.0000e+00 - val_loss: 0.4439 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8936 - accuracy: 0.0000e+00 - val_loss: 0.4447 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8933 - accuracy: 0.0000e+00 - val_loss: 0.4453 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8929 - accuracy: 0.0000e+00 - val_loss: 0.4458 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8926 - accuracy: 0.0000e+00 - val_loss: 0.4461 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8923 - accuracy: 0.0000e+00 - val_loss: 0.4464 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8919 - accuracy: 0.0000e+00 - val_loss: 0.4472 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8917 - accuracy: 0.0000e+00 - val_loss: 0.4476 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8914 - accuracy: 0.0000e+00 - val_loss: 0.4484 - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8911 - accuracy: 0.0000e+00 - val_loss: 0.4490 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8907 - accuracy: 0.0000e+00 - val_loss: 0.4496 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8904 - accuracy: 0.0000e+00 - val_loss: 0.4501 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8901 - accuracy: 0.0000e+00 - val_loss: 0.4509 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8898 - accuracy: 0.0000e+00 - val_loss: 0.4516 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8895 - accuracy: 0.0000e+00 - val_loss: 0.4520 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8892 - accuracy: 0.0000e+00 - val_loss: 0.4528 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8889 - accuracy: 0.0000e+00 - val_loss: 0.4536 - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8886 - accuracy: 0.0000e+00 - val_loss: 0.4540 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8884 - accuracy: 0.0000e+00 - val_loss: 0.4546 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8881 - accuracy: 0.0000e+00 - val_loss: 0.4554 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8878 - accuracy: 0.0000e+00 - val_loss: 0.4563 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8875 - accuracy: 0.0000e+00 - val_loss: 0.4570 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8873 - accuracy: 0.0000e+00 - val_loss: 0.4572 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8871 - accuracy: 0.0000e+00 - val_loss: 0.4580 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 0.4584 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8865 - accuracy: 0.0000e+00 - val_loss: 0.4590 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8863 - accuracy: 0.0000e+00 - val_loss: 0.4597 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8860 - accuracy: 0.0000e+00 - val_loss: 0.4604 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8858 - accuracy: 0.0000e+00 - val_loss: 0.4609 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8856 - accuracy: 0.0000e+00 - val_loss: 0.4616 - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.4622 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8850 - accuracy: 0.0000e+00 - val_loss: 0.4630 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8849 - accuracy: 0.0000e+00 - val_loss: 0.4636 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8845 - accuracy: 0.0000e+00 - val_loss: 0.4641 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8843 - accuracy: 0.0000e+00 - val_loss: 0.4649 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8841 - accuracy: 0.0000e+00 - val_loss: 0.4652 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8839 - accuracy: 0.0000e+00 - val_loss: 0.4657 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8836 - accuracy: 0.0000e+00 - val_loss: 0.4664 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8834 - accuracy: 0.0000e+00 - val_loss: 0.4668 - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8832 - accuracy: 0.0000e+00 - val_loss: 0.4674 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8831 - accuracy: 0.0000e+00 - val_loss: 0.4681 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8828 - accuracy: 0.0000e+00 - val_loss: 0.4686 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8826 - accuracy: 0.0000e+00 - val_loss: 0.4693 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8824 - accuracy: 0.0000e+00 - val_loss: 0.4696 - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8822 - accuracy: 0.0000e+00 - val_loss: 0.4707 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8821 - accuracy: 0.0000e+00 - val_loss: 0.4709 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8818 - accuracy: 0.0000e+00 - val_loss: 0.4713 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8816 - accuracy: 0.0000e+00 - val_loss: 0.4718 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8815 - accuracy: 0.0000e+00 - val_loss: 0.4724 - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8812 - accuracy: 0.0000e+00 - val_loss: 0.4729 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8811 - accuracy: 0.0000e+00 - val_loss: 0.4735 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8809 - accuracy: 0.0000e+00 - val_loss: 0.4737 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8807 - accuracy: 0.0000e+00 - val_loss: 0.4742 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8805 - accuracy: 0.0000e+00 - val_loss: 0.4747 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8804 - accuracy: 0.0000e+00 - val_loss: 0.4750 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8802 - accuracy: 0.0000e+00 - val_loss: 0.4755 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8800 - accuracy: 0.0000e+00 - val_loss: 0.4761 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8799 - accuracy: 0.0000e+00 - val_loss: 0.4766 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8797 - accuracy: 0.0000e+00 - val_loss: 0.4770 - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8795 - accuracy: 0.0000e+00 - val_loss: 0.4774 - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8794 - accuracy: 0.0000e+00 - val_loss: 0.4776 - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8792 - accuracy: 0.0000e+00 - val_loss: 0.4781 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8790 - accuracy: 0.0000e+00 - val_loss: 0.4787 - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8789 - accuracy: 0.0000e+00 - val_loss: 0.4790 - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8788 - accuracy: 0.0000e+00 - val_loss: 0.4792 - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8786 - accuracy: 0.0000e+00 - val_loss: 0.4796 - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8784 - accuracy: 0.0000e+00 - val_loss: 0.4801 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8783 - accuracy: 0.0000e+00 - val_loss: 0.4802 - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8782 - accuracy: 0.0000e+00 - val_loss: 0.4807 - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8780 - accuracy: 0.0000e+00 - val_loss: 0.4809 - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8779 - accuracy: 0.0000e+00 - val_loss: 0.4811 - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8778 - accuracy: 0.0000e+00 - val_loss: 0.4818 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8777 - accuracy: 0.0000e+00 - val_loss: 0.4821 - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8775 - accuracy: 0.0000e+00 - val_loss: 0.4824 - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8773 - accuracy: 0.0000e+00 - val_loss: 0.4826 - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8772 - accuracy: 0.0000e+00 - val_loss: 0.4828 - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8771 - accuracy: 0.0000e+00 - val_loss: 0.4833 - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8769 - accuracy: 0.0000e+00 - val_loss: 0.4839 - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8769 - accuracy: 0.0000e+00 - val_loss: 0.4838 - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 0.4838 - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8766 - accuracy: 0.0000e+00 - val_loss: 0.4843 - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8764 - accuracy: 0.0000e+00 - val_loss: 0.4848 - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8764 - accuracy: 0.0000e+00 - val_loss: 0.4849 - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8762 - accuracy: 0.0000e+00 - val_loss: 0.4850 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8761 - accuracy: 0.0000e+00 - val_loss: 0.4852 - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8760 - accuracy: 0.0000e+00 - val_loss: 0.4858 - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.8758 - accuracy: 0.0000e+00 - val_loss: 0.4859 - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8757 - accuracy: 0.0000e+00 - val_loss: 0.4863 - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8756 - accuracy: 0.0000e+00 - val_loss: 0.4866 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8755 - accuracy: 0.0000e+00 - val_loss: 0.4867 - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8754 - accuracy: 0.0000e+00 - val_loss: 0.4869 - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8753 - accuracy: 0.0000e+00 - val_loss: 0.4874 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8751 - accuracy: 0.0000e+00 - val_loss: 0.4876 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8750 - accuracy: 0.0000e+00 - val_loss: 0.4881 - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8749 - accuracy: 0.0000e+00 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8748 - accuracy: 0.0000e+00 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8747 - accuracy: 0.0000e+00 - val_loss: 0.4890 - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8746 - accuracy: 0.0000e+00 - val_loss: 0.4894 - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8745 - accuracy: 0.0000e+00 - val_loss: 0.4897 - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8744 - accuracy: 0.0000e+00 - val_loss: 0.4899 - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8743 - accuracy: 0.0000e+00 - val_loss: 0.4900 - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8742 - accuracy: 0.0000e+00 - val_loss: 0.4902 - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8741 - accuracy: 0.0000e+00 - val_loss: 0.4904 - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8740 - accuracy: 0.0000e+00 - val_loss: 0.4905 - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8739 - accuracy: 0.0000e+00 - val_loss: 0.4906 - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8738 - accuracy: 0.0000e+00 - val_loss: 0.4910 - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.8737 - accuracy: 0.0000e+00 - val_loss: 0.4914 - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8736 - accuracy: 0.0000e+00 - val_loss: 0.4920 - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8735 - accuracy: 0.0000e+00 - val_loss: 0.4921 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.8734 - accuracy: 0.0000e+00 - val_loss: 0.4924 - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.4925 - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8733 - accuracy: 0.0000e+00 - val_loss: 0.4927 - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.8732 - accuracy: 0.0000e+00 - val_loss: 0.4929 - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.8730 - accuracy: 0.0000e+00 - val_loss: 0.4930 - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.8730 - accuracy: 0.0000e+00 - val_loss: 0.4936 - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/GILD_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4936 - accuracy: 0.0000e+00\n",
      "[0.4935605823993683, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_80 (Dense)            (None, 21)                462       \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 1)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 484\n",
      "Trainable params: 484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 23ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 101/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 102/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 103/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 104/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 105/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 106/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 107/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 108/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 109/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 110/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 111/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 112/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 113/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 114/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 115/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 116/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 117/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 118/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 119/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 120/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 121/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 122/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 123/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 124/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 125/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 126/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 127/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 128/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 129/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 130/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 131/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 132/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 133/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 134/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 135/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 136/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 137/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 138/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 139/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 140/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 141/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 142/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 143/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 144/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 145/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 146/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 147/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 148/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 149/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 150/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 151/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 152/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 153/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 154/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 155/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 156/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 157/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 158/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 159/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 160/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 161/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 162/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 163/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 164/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 165/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 166/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 167/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 168/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 169/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 170/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 171/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 172/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 173/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 174/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 175/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 176/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 177/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 178/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 179/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 180/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 181/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 182/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 183/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 184/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 185/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 186/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 187/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 188/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 189/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 190/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 191/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 192/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 193/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 194/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 195/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 196/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 197/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 198/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 199/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 200/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 201/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 202/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 203/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 204/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 205/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 206/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 207/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 208/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 209/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 210/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 211/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 212/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 213/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 214/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 215/256\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 216/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 217/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 218/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 219/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 220/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 221/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 222/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 223/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 224/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 225/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 226/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 227/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 228/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 229/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 230/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 231/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 232/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 233/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 234/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 235/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 236/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 237/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 238/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 239/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 240/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 241/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 242/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 243/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 244/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 245/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 246/256\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 247/256\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 248/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 249/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 250/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 251/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 252/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 253/256\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 254/256\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 255/256\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 256/256\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: 01_Paper_1/Models/C_FNN_Model.hd5/assets\n",
      "1/1 [==============================] - 0s 22ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "[nan, 0.0]\n",
      "shape of df_train (70, 24) shape of df_test (18, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/media/ntu/volume1/home/s122md304_04/.local/lib/python3.8/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# prediction_dict = {}\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m financial_data_trend_standardized:\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     model, history, scores, prediction = create_model(df)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     model, history, scores \u001b[38;5;241m=\u001b[39m create_model(df)\n\u001b[1;32m      9\u001b[0m     ticker \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m     model_dict[ticker] \u001b[38;5;241m=\u001b[39m model\n",
      "Cell \u001b[0;32mIn [12], line 49\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(df_trend_standardize)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of df_train\u001b[39m\u001b[38;5;124m'\u001b[39m, df_train\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of df_test\u001b[39m\u001b[38;5;124m'\u001b[39m, df_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# standardize dataset, get numpy form of training and test dataset\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m df_train_stand, df_test_stand \u001b[38;5;241m=\u001b[39m \u001b[43mstandardize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m X_train_stand, X_test_stand \u001b[38;5;241m=\u001b[39m df_train_stand\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Return DJIA\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy(), df_test_stand\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Return DJIA\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m     51\u001b[0m Y_train_stand, Y_test_stand \u001b[38;5;241m=\u001b[39m df_train_stand[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Return DJIA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy(), df_test_stand[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelative Return DJIA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "Cell \u001b[0;32mIn [12], line 33\u001b[0m, in \u001b[0;36mcreate_model.<locals>.standardize\u001b[0;34m(df_train, df_test)\u001b[0m\n\u001b[1;32m     30\u001b[0m     df_train_stand[c] \u001b[38;5;241m=\u001b[39m scale\u001b[38;5;241m.\u001b[39mtransform(df_train_stand[[c]])\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# transform the testing data column\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     df_test_stand[c] \u001b[38;5;241m=\u001b[39m \u001b[43mscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test_stand\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_train_stand, df_test_stand\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:975\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    972\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    974\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m--> 975\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "model_dict = {}\n",
    "history_dict = {}\n",
    "scores_dict = {}\n",
    "# prediction_dict = {}\n",
    "\n",
    "for df in financial_data_trend_standardized:\n",
    "#     model, history, scores, prediction = create_model(df)\n",
    "    model, history, scores = create_model(df)\n",
    "    ticker = df['ticker'].iloc[0]\n",
    "    model_dict[ticker] = model\n",
    "    history_dict[ticker] = history\n",
    "    scores_dict[ticker] = scores    \n",
    "#     prediction_dict[ticker] = prediction    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "from tensorflow.keras.models import load_model\n",
    "test_model = load_model('test.hd5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_data_relative_return = load_file(file_path=folder_path, file_name='financial_data_relative_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quarter end</th>\n",
       "      <th>Assets</th>\n",
       "      <th>Current Assets</th>\n",
       "      <th>Liabilities</th>\n",
       "      <th>Current Liabilities</th>\n",
       "      <th>Shareholders equity</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>EPS basic</th>\n",
       "      <th>EPS diluted</th>\n",
       "      <th>Price</th>\n",
       "      <th>Price high</th>\n",
       "      <th>Price low</th>\n",
       "      <th>ROE</th>\n",
       "      <th>P/B ratio</th>\n",
       "      <th>P/E ratio</th>\n",
       "      <th>Cumulative dividends per share</th>\n",
       "      <th>Dividend payout ratio</th>\n",
       "      <th>Long-term debt to equity ratio</th>\n",
       "      <th>Net margin</th>\n",
       "      <th>Asset turnover</th>\n",
       "      <th>Free cash flow per share</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Relative Return DJIA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996-03-31</td>\n",
       "      <td>2.302130e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.979710e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.927100e+10</td>\n",
       "      <td>1.709800e+10</td>\n",
       "      <td>1.517000e+09</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.39</td>\n",
       "      <td>17.69</td>\n",
       "      <td>15.08</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>5.52</td>\n",
       "      <td>25.22</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.4321</td>\n",
       "      <td>1.7870</td>\n",
       "      <td>0.0933</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>GE</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996-06-30</td>\n",
       "      <td>2.420810e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.098380e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.929600e+10</td>\n",
       "      <td>1.906600e+10</td>\n",
       "      <td>1.908000e+09</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.16</td>\n",
       "      <td>15.33</td>\n",
       "      <td>12.98</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>4.82</td>\n",
       "      <td>21.24</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.4308</td>\n",
       "      <td>1.6990</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.057477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-09-30</td>\n",
       "      <td>2.491820e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.161300e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.999900e+10</td>\n",
       "      <td>2.002100e+10</td>\n",
       "      <td>1.788000e+09</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.52</td>\n",
       "      <td>14.69</td>\n",
       "      <td>12.35</td>\n",
       "      <td>0.2396</td>\n",
       "      <td>4.57</td>\n",
       "      <td>19.64</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.4302</td>\n",
       "      <td>1.6267</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.11</td>\n",
       "      <td>GE</td>\n",
       "      <td>3.823103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>2.724020e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.382700e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.112500e+10</td>\n",
       "      <td>2.299400e+10</td>\n",
       "      <td>2.067000e+09</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.50</td>\n",
       "      <td>13.42</td>\n",
       "      <td>11.58</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>4.11</td>\n",
       "      <td>17.61</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.4304</td>\n",
       "      <td>1.5822</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.19</td>\n",
       "      <td>GE</td>\n",
       "      <td>8.703655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>2.700680e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.364860e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.046400e+10</td>\n",
       "      <td>2.015700e+10</td>\n",
       "      <td>1.677000e+09</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.59</td>\n",
       "      <td>25.52</td>\n",
       "      <td>19.67</td>\n",
       "      <td>0.2462</td>\n",
       "      <td>7.15</td>\n",
       "      <td>30.80</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.4327</td>\n",
       "      <td>1.5219</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.07</td>\n",
       "      <td>GE</td>\n",
       "      <td>2.860977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1997-06-30</td>\n",
       "      <td>2.788970e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.441410e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.162200e+10</td>\n",
       "      <td>2.199700e+10</td>\n",
       "      <td>2.162000e+09</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22.66</td>\n",
       "      <td>24.88</td>\n",
       "      <td>20.44</td>\n",
       "      <td>0.2498</td>\n",
       "      <td>7.29</td>\n",
       "      <td>30.15</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>1.4797</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>GE</td>\n",
       "      <td>14.200151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1997-09-30</td>\n",
       "      <td>2.853540e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.485530e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.369600e+10</td>\n",
       "      <td>2.199100e+10</td>\n",
       "      <td>2.014000e+09</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.47</td>\n",
       "      <td>22.75</td>\n",
       "      <td>16.19</td>\n",
       "      <td>0.2496</td>\n",
       "      <td>6.03</td>\n",
       "      <td>24.96</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.4296</td>\n",
       "      <td>1.4097</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>GE</td>\n",
       "      <td>3.288564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>3.040120e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.658920e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.443800e+10</td>\n",
       "      <td>2.669500e+10</td>\n",
       "      <td>2.350000e+09</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.02</td>\n",
       "      <td>18.06</td>\n",
       "      <td>15.98</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>4.96</td>\n",
       "      <td>21.14</td>\n",
       "      <td>1.21</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>1.3532</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.46</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.593827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1998-03-31</td>\n",
       "      <td>3.097540e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.712220e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.471900e+10</td>\n",
       "      <td>2.262600e+10</td>\n",
       "      <td>1.891000e+09</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.57</td>\n",
       "      <td>28.82</td>\n",
       "      <td>34.65</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>8.19</td>\n",
       "      <td>34.58</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.4343</td>\n",
       "      <td>1.2610</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>GE</td>\n",
       "      <td>10.824884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1998-06-30</td>\n",
       "      <td>3.188820e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.793370e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.558500e+10</td>\n",
       "      <td>2.507000e+10</td>\n",
       "      <td>2.450000e+09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.74</td>\n",
       "      <td>28.25</td>\n",
       "      <td>32.29</td>\n",
       "      <td>24.21</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>7.96</td>\n",
       "      <td>33.11</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.4345</td>\n",
       "      <td>1.3704</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.680509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1998-09-30</td>\n",
       "      <td>3.345750e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.932580e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.716500e+10</td>\n",
       "      <td>2.413600e+10</td>\n",
       "      <td>2.284000e+09</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>28.79</td>\n",
       "      <td>30.67</td>\n",
       "      <td>26.90</td>\n",
       "      <td>0.2530</td>\n",
       "      <td>7.89</td>\n",
       "      <td>32.72</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.4359</td>\n",
       "      <td>1.5454</td>\n",
       "      <td>0.0911</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.93</td>\n",
       "      <td>GE</td>\n",
       "      <td>-14.126668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>3.559350e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.127800e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.888000e+10</td>\n",
       "      <td>2.863700e+10</td>\n",
       "      <td>2.671000e+09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>26.32</td>\n",
       "      <td>29.21</td>\n",
       "      <td>23.42</td>\n",
       "      <td>0.2541</td>\n",
       "      <td>6.94</td>\n",
       "      <td>29.24</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.4389</td>\n",
       "      <td>1.5345</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.76</td>\n",
       "      <td>GE</td>\n",
       "      <td>14.495922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1999-03-31</td>\n",
       "      <td>3.617360e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.182620e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.879300e+10</td>\n",
       "      <td>2.416500e+10</td>\n",
       "      <td>2.155000e+09</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>34.74</td>\n",
       "      <td>38.06</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>8.77</td>\n",
       "      <td>37.22</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.4443</td>\n",
       "      <td>1.5596</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>GE</td>\n",
       "      <td>6.499350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1999-06-30</td>\n",
       "      <td>3.683830e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.236600e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.961100e+10</td>\n",
       "      <td>2.741000e+10</td>\n",
       "      <td>2.820000e+09</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>36.21</td>\n",
       "      <td>39.15</td>\n",
       "      <td>33.27</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>9.17</td>\n",
       "      <td>37.72</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.4451</td>\n",
       "      <td>1.5362</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>GE</td>\n",
       "      <td>10.840433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1999-09-30</td>\n",
       "      <td>3.802240e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.350620e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.997300e+10</td>\n",
       "      <td>2.720000e+10</td>\n",
       "      <td>2.653000e+09</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "      <td>37.51</td>\n",
       "      <td>40.83</td>\n",
       "      <td>34.19</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>9.31</td>\n",
       "      <td>37.64</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.4454</td>\n",
       "      <td>1.6596</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>GE</td>\n",
       "      <td>-6.095984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>4.052000e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.574290e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.255700e+10</td>\n",
       "      <td>3.285500e+10</td>\n",
       "      <td>3.089000e+09</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>45.69</td>\n",
       "      <td>53.17</td>\n",
       "      <td>38.21</td>\n",
       "      <td>0.2664</td>\n",
       "      <td>11.23</td>\n",
       "      <td>44.22</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.4471</td>\n",
       "      <td>1.6784</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.79</td>\n",
       "      <td>GE</td>\n",
       "      <td>10.309037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-03-31</td>\n",
       "      <td>4.216370e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.722540e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.449800e+10</td>\n",
       "      <td>2.999600e+10</td>\n",
       "      <td>2.592000e+09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>48.31</td>\n",
       "      <td>54.96</td>\n",
       "      <td>41.67</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>11.23</td>\n",
       "      <td>45.01</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.4422</td>\n",
       "      <td>1.6498</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>GE</td>\n",
       "      <td>-5.209130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-06-30</td>\n",
       "      <td>4.240400e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.731820e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.592500e+10</td>\n",
       "      <td>3.286200e+10</td>\n",
       "      <td>3.378000e+09</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>51.83</td>\n",
       "      <td>55.98</td>\n",
       "      <td>47.69</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>11.52</td>\n",
       "      <td>46.41</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>1.6898</td>\n",
       "      <td>0.0953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>GE</td>\n",
       "      <td>-4.464226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-09-30</td>\n",
       "      <td>4.311410e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.785180e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.774500e+10</td>\n",
       "      <td>3.201400e+10</td>\n",
       "      <td>3.180000e+09</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>55.00</td>\n",
       "      <td>60.50</td>\n",
       "      <td>49.50</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>11.85</td>\n",
       "      <td>47.28</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.4421</td>\n",
       "      <td>1.5879</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.27</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.967382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-12-31</td>\n",
       "      <td>4.370060e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.815780e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.049200e+10</td>\n",
       "      <td>3.498100e+10</td>\n",
       "      <td>3.585000e+09</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>53.56</td>\n",
       "      <td>59.94</td>\n",
       "      <td>47.19</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>11.11</td>\n",
       "      <td>43.90</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.4435</td>\n",
       "      <td>1.6266</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.41</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.233964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001-03-31</td>\n",
       "      <td>4.379850e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.821410e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.093000e+10</td>\n",
       "      <td>3.049300e+10</td>\n",
       "      <td>2.573000e+09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>38.73</td>\n",
       "      <td>41.59</td>\n",
       "      <td>35.88</td>\n",
       "      <td>0.2607</td>\n",
       "      <td>7.62</td>\n",
       "      <td>30.50</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>1.5755</td>\n",
       "      <td>0.0976</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "      <td>GE</td>\n",
       "      <td>-9.469013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001-06-30</td>\n",
       "      <td>4.453470e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.882160e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.224900e+10</td>\n",
       "      <td>3.197700e+10</td>\n",
       "      <td>3.897000e+09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>38.92</td>\n",
       "      <td>49.59</td>\n",
       "      <td>28.25</td>\n",
       "      <td>0.2628</td>\n",
       "      <td>7.59</td>\n",
       "      <td>30.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.4651</td>\n",
       "      <td>1.5051</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.66</td>\n",
       "      <td>GE</td>\n",
       "      <td>5.942787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2001-09-30</td>\n",
       "      <td>4.600970e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.016380e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.359700e+10</td>\n",
       "      <td>2.946800e+10</td>\n",
       "      <td>3.281000e+09</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>45.73</td>\n",
       "      <td>52.90</td>\n",
       "      <td>38.57</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>8.69</td>\n",
       "      <td>33.38</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>1.5119</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "      <td>GE</td>\n",
       "      <td>-18.528940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2001-12-31</td>\n",
       "      <td>4.950230e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.349840e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.482400e+10</td>\n",
       "      <td>3.397500e+10</td>\n",
       "      <td>3.933000e+09</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.40</td>\n",
       "      <td>41.98</td>\n",
       "      <td>47.99</td>\n",
       "      <td>35.98</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>7.77</td>\n",
       "      <td>30.64</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.4791</td>\n",
       "      <td>1.4557</td>\n",
       "      <td>0.1087</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.20</td>\n",
       "      <td>GE</td>\n",
       "      <td>11.632211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2002-03-31</td>\n",
       "      <td>5.007360e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.403440e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.517100e+10</td>\n",
       "      <td>3.052100e+10</td>\n",
       "      <td>2.503000e+09</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>24.69</td>\n",
       "      <td>27.98</td>\n",
       "      <td>21.40</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>4.47</td>\n",
       "      <td>17.51</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.4962</td>\n",
       "      <td>1.7016</td>\n",
       "      <td>0.1081</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.19</td>\n",
       "      <td>GE</td>\n",
       "      <td>3.264052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2002-06-30</td>\n",
       "      <td>5.408880e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.768400e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.872700e+10</td>\n",
       "      <td>3.321400e+10</td>\n",
       "      <td>4.426000e+09</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.44</td>\n",
       "      <td>28.00</td>\n",
       "      <td>32.98</td>\n",
       "      <td>23.02</td>\n",
       "      <td>0.2545</td>\n",
       "      <td>5.05</td>\n",
       "      <td>20.00</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.4919</td>\n",
       "      <td>2.1320</td>\n",
       "      <td>0.1112</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>GE</td>\n",
       "      <td>-12.422979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2002-09-30</td>\n",
       "      <td>5.555230e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.878270e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.225800e+10</td>\n",
       "      <td>3.258500e+10</td>\n",
       "      <td>4.087000e+09</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>32.61</td>\n",
       "      <td>37.80</td>\n",
       "      <td>27.42</td>\n",
       "      <td>0.2589</td>\n",
       "      <td>5.53</td>\n",
       "      <td>23.13</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.4789</td>\n",
       "      <td>2.1529</td>\n",
       "      <td>0.1147</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.76</td>\n",
       "      <td>GE</td>\n",
       "      <td>-21.586480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>5.752440e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.060650e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.370600e+10</td>\n",
       "      <td>3.537800e+10</td>\n",
       "      <td>3.102000e+09</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>38.17</td>\n",
       "      <td>41.84</td>\n",
       "      <td>34.49</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>6.10</td>\n",
       "      <td>23.86</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.5149</td>\n",
       "      <td>2.2075</td>\n",
       "      <td>0.1072</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.94</td>\n",
       "      <td>GE</td>\n",
       "      <td>9.157952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>5.836340e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.117630e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.635800e+10</td>\n",
       "      <td>3.031900e+10</td>\n",
       "      <td>2.999000e+09</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>29.34</td>\n",
       "      <td>31.30</td>\n",
       "      <td>27.37</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>4.61</td>\n",
       "      <td>19.43</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.5050</td>\n",
       "      <td>2.3464</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>GE</td>\n",
       "      <td>-4.604385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2003-06-30</td>\n",
       "      <td>6.148570e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.374330e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.196800e+10</td>\n",
       "      <td>3.337300e+10</td>\n",
       "      <td>3.794000e+09</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>29.66</td>\n",
       "      <td>32.42</td>\n",
       "      <td>26.90</td>\n",
       "      <td>0.2116</td>\n",
       "      <td>4.47</td>\n",
       "      <td>18.77</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.5359</td>\n",
       "      <td>2.3483</td>\n",
       "      <td>0.1062</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.60</td>\n",
       "      <td>GE</td>\n",
       "      <td>11.065568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2003-09-30</td>\n",
       "      <td>6.269330e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.486610e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.248100e+10</td>\n",
       "      <td>3.339400e+10</td>\n",
       "      <td>3.649000e+09</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>28.58</td>\n",
       "      <td>31.66</td>\n",
       "      <td>25.50</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>3.98</td>\n",
       "      <td>18.80</td>\n",
       "      <td>4.64</td>\n",
       "      <td>0.5618</td>\n",
       "      <td>2.2476</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.86</td>\n",
       "      <td>GE</td>\n",
       "      <td>3.086155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>6.474830e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.625230e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.918000e+10</td>\n",
       "      <td>3.710100e+10</td>\n",
       "      <td>4.560000e+09</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "      <td>24.65</td>\n",
       "      <td>28.00</td>\n",
       "      <td>21.30</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>3.41</td>\n",
       "      <td>17.99</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>2.1471</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.90</td>\n",
       "      <td>GE</td>\n",
       "      <td>11.139218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2004-03-31</td>\n",
       "      <td>6.621060e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.693290e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.648600e+10</td>\n",
       "      <td>3.335000e+10</td>\n",
       "      <td>3.240000e+09</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>35.20</td>\n",
       "      <td>37.75</td>\n",
       "      <td>32.65</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>4.48</td>\n",
       "      <td>22.71</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.5163</td>\n",
       "      <td>1.9595</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.55</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.500979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2004-06-30</td>\n",
       "      <td>6.970850e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.820720e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.828200e+10</td>\n",
       "      <td>3.703500e+10</td>\n",
       "      <td>3.924000e+09</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>32.98</td>\n",
       "      <td>34.53</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0.1828</td>\n",
       "      <td>3.89</td>\n",
       "      <td>21.28</td>\n",
       "      <td>5.24</td>\n",
       "      <td>0.5254</td>\n",
       "      <td>1.7873</td>\n",
       "      <td>0.1091</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.51</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.682274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>7.046200e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.865260e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.017770e+11</td>\n",
       "      <td>3.827200e+10</td>\n",
       "      <td>4.051000e+09</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>31.52</td>\n",
       "      <td>33.49</td>\n",
       "      <td>29.55</td>\n",
       "      <td>0.1725</td>\n",
       "      <td>3.39</td>\n",
       "      <td>20.34</td>\n",
       "      <td>5.44</td>\n",
       "      <td>0.5252</td>\n",
       "      <td>1.8308</td>\n",
       "      <td>0.1082</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.63</td>\n",
       "      <td>GE</td>\n",
       "      <td>-3.568084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>7.505070e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.233030e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.108210e+11</td>\n",
       "      <td>4.420900e+10</td>\n",
       "      <td>5.604000e+09</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>31.73</td>\n",
       "      <td>34.57</td>\n",
       "      <td>28.88</td>\n",
       "      <td>0.1693</td>\n",
       "      <td>3.29</td>\n",
       "      <td>20.21</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.5114</td>\n",
       "      <td>1.9190</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.08</td>\n",
       "      <td>GE</td>\n",
       "      <td>6.523767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2005-03-31</td>\n",
       "      <td>7.522230e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.207630e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.128720e+11</td>\n",
       "      <td>3.972600e+10</td>\n",
       "      <td>3.965000e+09</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>34.51</td>\n",
       "      <td>36.34</td>\n",
       "      <td>32.67</td>\n",
       "      <td>0.1656</td>\n",
       "      <td>3.30</td>\n",
       "      <td>21.57</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.5068</td>\n",
       "      <td>1.8865</td>\n",
       "      <td>0.1102</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.65</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.570958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2005-06-30</td>\n",
       "      <td>7.403620e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.107760e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.123840e+11</td>\n",
       "      <td>4.155600e+10</td>\n",
       "      <td>4.647000e+09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>34.31</td>\n",
       "      <td>35.78</td>\n",
       "      <td>32.85</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>3.22</td>\n",
       "      <td>20.79</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>1.9541</td>\n",
       "      <td>0.1115</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.232469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2005-09-30</td>\n",
       "      <td>6.626720e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.403150e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.129810e+11</td>\n",
       "      <td>4.192700e+10</td>\n",
       "      <td>4.677000e+09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>35.75</td>\n",
       "      <td>37.34</td>\n",
       "      <td>34.15</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>3.37</td>\n",
       "      <td>20.91</td>\n",
       "      <td>6.32</td>\n",
       "      <td>0.4934</td>\n",
       "      <td>1.9260</td>\n",
       "      <td>0.1128</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.02</td>\n",
       "      <td>GE</td>\n",
       "      <td>2.821215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>6.733210e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.559160e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.093510e+11</td>\n",
       "      <td>2.703300e+10</td>\n",
       "      <td>3.422000e+09</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>35.92</td>\n",
       "      <td>36.89</td>\n",
       "      <td>34.95</td>\n",
       "      <td>0.1493</td>\n",
       "      <td>3.36</td>\n",
       "      <td>20.29</td>\n",
       "      <td>6.57</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>1.9413</td>\n",
       "      <td>0.1112</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.87</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.393139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2006-03-31</td>\n",
       "      <td>6.748650e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.591970e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.075250e+11</td>\n",
       "      <td>3.802900e+10</td>\n",
       "      <td>4.440000e+09</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>36.55</td>\n",
       "      <td>38.49</td>\n",
       "      <td>34.62</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>3.48</td>\n",
       "      <td>23.28</td>\n",
       "      <td>6.82</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>2.0737</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>GE</td>\n",
       "      <td>3.544488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2006-06-30</td>\n",
       "      <td>6.624370e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.453710e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.087920e+11</td>\n",
       "      <td>4.004800e+10</td>\n",
       "      <td>4.946000e+09</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.47</td>\n",
       "      <td>33.86</td>\n",
       "      <td>35.65</td>\n",
       "      <td>32.06</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>3.27</td>\n",
       "      <td>21.30</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>2.1779</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.39</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.293211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2006-09-30</td>\n",
       "      <td>6.820650e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.624720e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.113820e+11</td>\n",
       "      <td>4.069300e+10</td>\n",
       "      <td>4.867000e+09</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>34.01</td>\n",
       "      <td>35.24</td>\n",
       "      <td>32.78</td>\n",
       "      <td>0.1618</td>\n",
       "      <td>3.23</td>\n",
       "      <td>20.99</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.5863</td>\n",
       "      <td>2.1810</td>\n",
       "      <td>0.1212</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.71</td>\n",
       "      <td>GE</td>\n",
       "      <td>4.532616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2006-12-31</td>\n",
       "      <td>6.972390e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.773470e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.123140e+11</td>\n",
       "      <td>4.462100e+10</td>\n",
       "      <td>6.576000e+09</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.65</td>\n",
       "      <td>33.92</td>\n",
       "      <td>35.63</td>\n",
       "      <td>32.21</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>3.14</td>\n",
       "      <td>20.43</td>\n",
       "      <td>7.60</td>\n",
       "      <td>0.5107</td>\n",
       "      <td>2.3221</td>\n",
       "      <td>0.1275</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.56</td>\n",
       "      <td>GE</td>\n",
       "      <td>6.288540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2007-03-31</td>\n",
       "      <td>7.140800e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.920590e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.141690e+11</td>\n",
       "      <td>4.019500e+10</td>\n",
       "      <td>4.508000e+09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>39.11</td>\n",
       "      <td>42.15</td>\n",
       "      <td>36.07</td>\n",
       "      <td>0.1871</td>\n",
       "      <td>3.58</td>\n",
       "      <td>19.65</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>2.4093</td>\n",
       "      <td>0.1262</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.37</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.727654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2007-06-30</td>\n",
       "      <td>7.385330e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.143190e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.170230e+11</td>\n",
       "      <td>4.231600e+10</td>\n",
       "      <td>5.420000e+09</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>39.14</td>\n",
       "      <td>42.07</td>\n",
       "      <td>36.20</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>3.53</td>\n",
       "      <td>19.19</td>\n",
       "      <td>8.16</td>\n",
       "      <td>0.5244</td>\n",
       "      <td>2.4165</td>\n",
       "      <td>0.1273</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.11</td>\n",
       "      <td>GE</td>\n",
       "      <td>7.863396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2007-09-30</td>\n",
       "      <td>7.617060e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.413670e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.128310e+11</td>\n",
       "      <td>4.253400e+10</td>\n",
       "      <td>5.539000e+09</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>37.16</td>\n",
       "      <td>39.77</td>\n",
       "      <td>34.55</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>3.25</td>\n",
       "      <td>17.70</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.5198</td>\n",
       "      <td>2.7583</td>\n",
       "      <td>0.1299</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.97</td>\n",
       "      <td>GE</td>\n",
       "      <td>3.454183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2007-12-31</td>\n",
       "      <td>7.953370e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.717740e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.155590e+11</td>\n",
       "      <td>4.769300e+10</td>\n",
       "      <td>6.741000e+09</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>36.09</td>\n",
       "      <td>38.28</td>\n",
       "      <td>33.90</td>\n",
       "      <td>0.1933</td>\n",
       "      <td>3.23</td>\n",
       "      <td>16.71</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.5257</td>\n",
       "      <td>2.7606</td>\n",
       "      <td>0.1286</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.15</td>\n",
       "      <td>GE</td>\n",
       "      <td>-4.784306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2008-03-31</td>\n",
       "      <td>8.338900e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.096560e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.160000e+11</td>\n",
       "      <td>4.227300e+10</td>\n",
       "      <td>4.304000e+09</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>19.16</td>\n",
       "      <td>25.75</td>\n",
       "      <td>12.58</td>\n",
       "      <td>0.1908</td>\n",
       "      <td>1.66</td>\n",
       "      <td>8.71</td>\n",
       "      <td>9.06</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>2.9886</td>\n",
       "      <td>0.1259</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.63</td>\n",
       "      <td>GE</td>\n",
       "      <td>-8.639528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2008-06-30</td>\n",
       "      <td>8.469880e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.198080e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.183860e+11</td>\n",
       "      <td>4.689100e+10</td>\n",
       "      <td>5.072000e+09</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>26.27</td>\n",
       "      <td>30.39</td>\n",
       "      <td>22.16</td>\n",
       "      <td>0.1872</td>\n",
       "      <td>2.26</td>\n",
       "      <td>11.94</td>\n",
       "      <td>9.37</td>\n",
       "      <td>0.5587</td>\n",
       "      <td>2.9671</td>\n",
       "      <td>0.1207</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.91</td>\n",
       "      <td>GE</td>\n",
       "      <td>-7.671903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2008-09-30</td>\n",
       "      <td>8.295500e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.081870e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.123270e+11</td>\n",
       "      <td>4.723400e+10</td>\n",
       "      <td>4.312000e+09</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>32.34</td>\n",
       "      <td>38.52</td>\n",
       "      <td>26.15</td>\n",
       "      <td>0.1768</td>\n",
       "      <td>2.72</td>\n",
       "      <td>14.83</td>\n",
       "      <td>9.68</td>\n",
       "      <td>0.6048</td>\n",
       "      <td>2.9371</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.88</td>\n",
       "      <td>GE</td>\n",
       "      <td>-4.370962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>7.977690e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.841570e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.046650e+11</td>\n",
       "      <td>4.611700e+10</td>\n",
       "      <td>3.722000e+09</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>34.70</td>\n",
       "      <td>37.74</td>\n",
       "      <td>31.65</td>\n",
       "      <td>0.1543</td>\n",
       "      <td>3.08</td>\n",
       "      <td>16.76</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.7199</td>\n",
       "      <td>3.1536</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.84</td>\n",
       "      <td>GE</td>\n",
       "      <td>-23.561686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2009-03-31</td>\n",
       "      <td>7.608070e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.513310e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.010190e+11</td>\n",
       "      <td>3.841100e+10</td>\n",
       "      <td>2.811000e+09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>15.51</td>\n",
       "      <td>16.87</td>\n",
       "      <td>14.15</td>\n",
       "      <td>0.1452</td>\n",
       "      <td>1.57</td>\n",
       "      <td>8.97</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.8033</td>\n",
       "      <td>3.2435</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>GE</td>\n",
       "      <td>-15.896465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2009-06-30</td>\n",
       "      <td>7.781680e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.577090e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.121190e+11</td>\n",
       "      <td>3.908200e+10</td>\n",
       "      <td>2.671000e+09</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>14.01</td>\n",
       "      <td>17.52</td>\n",
       "      <td>10.50</td>\n",
       "      <td>0.1243</td>\n",
       "      <td>1.47</td>\n",
       "      <td>9.04</td>\n",
       "      <td>10.40</td>\n",
       "      <td>0.8009</td>\n",
       "      <td>3.0273</td>\n",
       "      <td>0.0782</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>GE</td>\n",
       "      <td>9.824917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>7.878460e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.620370e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.175290e+11</td>\n",
       "      <td>3.779900e+10</td>\n",
       "      <td>2.494000e+09</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>12.18</td>\n",
       "      <td>14.55</td>\n",
       "      <td>9.80</td>\n",
       "      <td>0.1054</td>\n",
       "      <td>1.15</td>\n",
       "      <td>9.52</td>\n",
       "      <td>10.50</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>3.0468</td>\n",
       "      <td>0.0711</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.76</td>\n",
       "      <td>GE</td>\n",
       "      <td>12.897010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>7.818180e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.566820e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.172910e+11</td>\n",
       "      <td>4.149100e+10</td>\n",
       "      <td>3.049000e+09</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>11.55</td>\n",
       "      <td>17.24</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>1.05</td>\n",
       "      <td>10.69</td>\n",
       "      <td>10.60</td>\n",
       "      <td>0.6039</td>\n",
       "      <td>2.8836</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.99</td>\n",
       "      <td>GE</td>\n",
       "      <td>6.812167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>7.773550e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.544040e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.151950e+11</td>\n",
       "      <td>3.660500e+10</td>\n",
       "      <td>1.945000e+09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>17.05</td>\n",
       "      <td>18.94</td>\n",
       "      <td>15.15</td>\n",
       "      <td>0.0853</td>\n",
       "      <td>1.55</td>\n",
       "      <td>16.88</td>\n",
       "      <td>10.70</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>2.7571</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.69</td>\n",
       "      <td>GE</td>\n",
       "      <td>4.423824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2010-06-30</td>\n",
       "      <td>7.499300e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.292050e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.139340e+11</td>\n",
       "      <td>3.744400e+10</td>\n",
       "      <td>3.109000e+09</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>16.98</td>\n",
       "      <td>19.70</td>\n",
       "      <td>14.27</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18.46</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.4145</td>\n",
       "      <td>2.6217</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.81</td>\n",
       "      <td>GE</td>\n",
       "      <td>-11.080510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2010-09-30</td>\n",
       "      <td>7.588240e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.382140e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.155360e+11</td>\n",
       "      <td>3.588800e+10</td>\n",
       "      <td>2.055000e+09</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>15.22</td>\n",
       "      <td>16.70</td>\n",
       "      <td>13.75</td>\n",
       "      <td>0.0854</td>\n",
       "      <td>1.43</td>\n",
       "      <td>15.85</td>\n",
       "      <td>10.92</td>\n",
       "      <td>0.4547</td>\n",
       "      <td>2.6617</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.86</td>\n",
       "      <td>GE</td>\n",
       "      <td>9.295916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>7.512160e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.270180e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.189360e+11</td>\n",
       "      <td>4.027400e+10</td>\n",
       "      <td>4.535000e+09</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>17.06</td>\n",
       "      <td>18.49</td>\n",
       "      <td>15.63</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18.75</td>\n",
       "      <td>11.06</td>\n",
       "      <td>0.4321</td>\n",
       "      <td>2.4662</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.79</td>\n",
       "      <td>GE</td>\n",
       "      <td>6.939804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>7.278370e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.020480e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.235350e+11</td>\n",
       "      <td>3.844800e+10</td>\n",
       "      <td>3.433000e+09</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>19.88</td>\n",
       "      <td>21.65</td>\n",
       "      <td>18.12</td>\n",
       "      <td>0.1088</td>\n",
       "      <td>1.78</td>\n",
       "      <td>18.75</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.4145</td>\n",
       "      <td>2.3284</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>GE</td>\n",
       "      <td>6.189944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>7.391300e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.087330e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.280740e+11</td>\n",
       "      <td>3.562500e+10</td>\n",
       "      <td>3.764000e+09</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>19.41</td>\n",
       "      <td>20.85</td>\n",
       "      <td>17.97</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>1.67</td>\n",
       "      <td>16.18</td>\n",
       "      <td>11.35</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>2.1695</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.55</td>\n",
       "      <td>GE</td>\n",
       "      <td>0.738461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>7.377000e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.109270e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.244610e+11</td>\n",
       "      <td>3.536700e+10</td>\n",
       "      <td>3.224000e+09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>17.13</td>\n",
       "      <td>19.53</td>\n",
       "      <td>14.72</td>\n",
       "      <td>0.1119</td>\n",
       "      <td>1.42</td>\n",
       "      <td>13.49</td>\n",
       "      <td>11.50</td>\n",
       "      <td>0.4437</td>\n",
       "      <td>2.1555</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.73</td>\n",
       "      <td>GE</td>\n",
       "      <td>-13.870858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>7.172420e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.991080e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.164380e+11</td>\n",
       "      <td>3.786000e+10</td>\n",
       "      <td>3.730000e+09</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>16.15</td>\n",
       "      <td>18.28</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>1.37</td>\n",
       "      <td>12.33</td>\n",
       "      <td>11.67</td>\n",
       "      <td>0.4922</td>\n",
       "      <td>2.0909</td>\n",
       "      <td>0.0891</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.64</td>\n",
       "      <td>GE</td>\n",
       "      <td>10.617426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>7.077190e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.861310e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.198670e+11</td>\n",
       "      <td>3.518200e+10</td>\n",
       "      <td>3.034000e+09</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>19.62</td>\n",
       "      <td>21.00</td>\n",
       "      <td>18.23</td>\n",
       "      <td>0.1047</td>\n",
       "      <td>1.78</td>\n",
       "      <td>15.95</td>\n",
       "      <td>11.84</td>\n",
       "      <td>0.5293</td>\n",
       "      <td>1.9479</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.47</td>\n",
       "      <td>GE</td>\n",
       "      <td>7.741934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>6.941210e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.702220e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.201190e+11</td>\n",
       "      <td>3.650100e+10</td>\n",
       "      <td>3.105000e+09</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>19.43</td>\n",
       "      <td>20.84</td>\n",
       "      <td>18.02</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>1.72</td>\n",
       "      <td>16.06</td>\n",
       "      <td>12.01</td>\n",
       "      <td>0.5713</td>\n",
       "      <td>1.9132</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.44</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.586918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2012-09-30</td>\n",
       "      <td>6.988800e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.708640e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.225520e+11</td>\n",
       "      <td>3.634900e+10</td>\n",
       "      <td>3.491000e+09</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>21.16</td>\n",
       "      <td>22.96</td>\n",
       "      <td>19.36</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>1.86</td>\n",
       "      <td>18.40</td>\n",
       "      <td>12.18</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>1.9149</td>\n",
       "      <td>0.0916</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.46</td>\n",
       "      <td>GE</td>\n",
       "      <td>4.234566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>6.853280e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.568580e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.230260e+11</td>\n",
       "      <td>3.932700e+10</td>\n",
       "      <td>4.011000e+09</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>21.52</td>\n",
       "      <td>23.18</td>\n",
       "      <td>19.87</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>1.84</td>\n",
       "      <td>17.08</td>\n",
       "      <td>12.37</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>1.9190</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.77</td>\n",
       "      <td>GE</td>\n",
       "      <td>-2.524092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2013-03-31</td>\n",
       "      <td>6.706900e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.416400e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.237140e+11</td>\n",
       "      <td>3.501000e+10</td>\n",
       "      <td>3.527000e+09</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.34</td>\n",
       "      <td>22.29</td>\n",
       "      <td>23.90</td>\n",
       "      <td>20.68</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>1.88</td>\n",
       "      <td>17.28</td>\n",
       "      <td>12.56</td>\n",
       "      <td>0.5319</td>\n",
       "      <td>1.8939</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.26</td>\n",
       "      <td>GE</td>\n",
       "      <td>10.149276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>6.605410e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.317300e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.225090e+11</td>\n",
       "      <td>3.512300e+10</td>\n",
       "      <td>3.133000e+09</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>22.78</td>\n",
       "      <td>24.45</td>\n",
       "      <td>21.11</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>1.90</td>\n",
       "      <td>17.00</td>\n",
       "      <td>12.75</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>1.8879</td>\n",
       "      <td>0.0971</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.54</td>\n",
       "      <td>GE</td>\n",
       "      <td>2.242432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>6.614380e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.323930e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.226920e+11</td>\n",
       "      <td>3.572500e+10</td>\n",
       "      <td>3.191000e+09</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>23.86</td>\n",
       "      <td>24.95</td>\n",
       "      <td>22.76</td>\n",
       "      <td>0.1127</td>\n",
       "      <td>1.98</td>\n",
       "      <td>17.67</td>\n",
       "      <td>12.94</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>1.8491</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.501969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>6.565600e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.197770e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.305660e+11</td>\n",
       "      <td>4.018700e+10</td>\n",
       "      <td>3.206000e+09</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>25.80</td>\n",
       "      <td>28.09</td>\n",
       "      <td>23.50</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>2.13</td>\n",
       "      <td>19.40</td>\n",
       "      <td>13.16</td>\n",
       "      <td>0.6149</td>\n",
       "      <td>1.6977</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.89</td>\n",
       "      <td>GE</td>\n",
       "      <td>8.810388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>6.522520e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.142330e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.318360e+11</td>\n",
       "      <td>3.417800e+10</td>\n",
       "      <td>2.999000e+09</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>26.13</td>\n",
       "      <td>27.94</td>\n",
       "      <td>24.32</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>2.01</td>\n",
       "      <td>20.57</td>\n",
       "      <td>13.38</td>\n",
       "      <td>0.6601</td>\n",
       "      <td>1.6763</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.28</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.710277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>6.518690e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.118020e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.340130e+11</td>\n",
       "      <td>3.623300e+10</td>\n",
       "      <td>3.545000e+09</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>26.48</td>\n",
       "      <td>27.53</td>\n",
       "      <td>25.43</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>2.01</td>\n",
       "      <td>21.53</td>\n",
       "      <td>13.60</td>\n",
       "      <td>0.6602</td>\n",
       "      <td>1.6181</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.37</td>\n",
       "      <td>GE</td>\n",
       "      <td>2.205994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>6.500210e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.065100e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.349980e+11</td>\n",
       "      <td>3.617400e+10</td>\n",
       "      <td>3.537000e+09</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>25.95</td>\n",
       "      <td>27.15</td>\n",
       "      <td>24.75</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1.94</td>\n",
       "      <td>20.27</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.6646</td>\n",
       "      <td>1.5791</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.49</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.249135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>6.483490e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.115160e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.281590e+11</td>\n",
       "      <td>4.200400e+10</td>\n",
       "      <td>5.152000e+09</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>25.40</td>\n",
       "      <td>27.10</td>\n",
       "      <td>23.69</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>1.89</td>\n",
       "      <td>19.24</td>\n",
       "      <td>14.05</td>\n",
       "      <td>0.5867</td>\n",
       "      <td>1.5638</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.46</td>\n",
       "      <td>GE</td>\n",
       "      <td>4.356110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>6.176310e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.003330e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.085600e+11</td>\n",
       "      <td>2.935600e+10</td>\n",
       "      <td>-1.357300e+10</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>24.84</td>\n",
       "      <td>26.27</td>\n",
       "      <td>23.41</td>\n",
       "      <td>-0.0106</td>\n",
       "      <td>1.95</td>\n",
       "      <td>16.56</td>\n",
       "      <td>14.28</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.7541</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.42</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.286166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>6.109260e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.926470e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.095030e+11</td>\n",
       "      <td>3.169500e+10</td>\n",
       "      <td>-1.360000e+09</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>26.63</td>\n",
       "      <td>28.68</td>\n",
       "      <td>24.57</td>\n",
       "      <td>-0.0519</td>\n",
       "      <td>2.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.51</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.6927</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.61</td>\n",
       "      <td>GE</td>\n",
       "      <td>-0.816783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2015-09-30</td>\n",
       "      <td>5.813100e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.613170e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.112050e+11</td>\n",
       "      <td>3.168000e+10</td>\n",
       "      <td>2.506000e+09</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>23.35</td>\n",
       "      <td>27.33</td>\n",
       "      <td>19.37</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.74</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.6187</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.03</td>\n",
       "      <td>GE</td>\n",
       "      <td>-8.319882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>4.926920e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.895820e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.827400e+10</td>\n",
       "      <td>2.465500e+10</td>\n",
       "      <td>6.301000e+09</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>28.14</td>\n",
       "      <td>31.49</td>\n",
       "      <td>24.79</td>\n",
       "      <td>-0.0575</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.97</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.4786</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.53</td>\n",
       "      <td>GE</td>\n",
       "      <td>6.749346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>4.621930e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.664030e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.108700e+10</td>\n",
       "      <td>2.784500e+10</td>\n",
       "      <td>1.910000e+08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>29.57</td>\n",
       "      <td>32.05</td>\n",
       "      <td>27.10</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>2.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.20</td>\n",
       "      <td>1.2153</td>\n",
       "      <td>1.4513</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.521322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>4.014610e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.117080e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.499000e+10</td>\n",
       "      <td>3.349400e+10</td>\n",
       "      <td>2.890000e+09</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>31.04</td>\n",
       "      <td>33.02</td>\n",
       "      <td>29.06</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>3.14</td>\n",
       "      <td>40.31</td>\n",
       "      <td>15.43</td>\n",
       "      <td>0.7567</td>\n",
       "      <td>1.4415</td>\n",
       "      <td>0.0971</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.415581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>3.876940e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.010980e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.188200e+10</td>\n",
       "      <td>2.926600e+10</td>\n",
       "      <td>2.027000e+09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.66</td>\n",
       "      <td>0.7656</td>\n",
       "      <td>1.4129</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>GE</td>\n",
       "      <td>1.065528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>3.651830e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.846680e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.582800e+10</td>\n",
       "      <td>3.308800e+10</td>\n",
       "      <td>3.723000e+09</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>30.29</td>\n",
       "      <td>32.38</td>\n",
       "      <td>28.19</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>3.27</td>\n",
       "      <td>25.89</td>\n",
       "      <td>15.90</td>\n",
       "      <td>1.0157</td>\n",
       "      <td>1.3859</td>\n",
       "      <td>0.0661</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.06</td>\n",
       "      <td>GE</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>3.516430e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.724160e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.453400e+10</td>\n",
       "      <td>2.766000e+10</td>\n",
       "      <td>6.530000e+08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>30.55</td>\n",
       "      <td>31.84</td>\n",
       "      <td>29.25</td>\n",
       "      <td>0.1121</td>\n",
       "      <td>3.52</td>\n",
       "      <td>33.94</td>\n",
       "      <td>16.14</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>1.3374</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>GE</td>\n",
       "      <td>4.367149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>3.554730e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.764980e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.414800e+10</td>\n",
       "      <td>2.955800e+10</td>\n",
       "      <td>1.367000e+09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>28.66</td>\n",
       "      <td>30.54</td>\n",
       "      <td>26.79</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>3.34</td>\n",
       "      <td>29.24</td>\n",
       "      <td>16.38</td>\n",
       "      <td>1.1295</td>\n",
       "      <td>1.3983</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.20</td>\n",
       "      <td>GE</td>\n",
       "      <td>3.153225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>3.780380e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.805440e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.610500e+10</td>\n",
       "      <td>3.347200e+10</td>\n",
       "      <td>1.836000e+09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>25.59</td>\n",
       "      <td>27.59</td>\n",
       "      <td>23.58</td>\n",
       "      <td>0.0951</td>\n",
       "      <td>2.99</td>\n",
       "      <td>31.59</td>\n",
       "      <td>16.62</td>\n",
       "      <td>1.1667</td>\n",
       "      <td>1.4134</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.10</td>\n",
       "      <td>GE</td>\n",
       "      <td>4.603687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>3.779450e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.925610e+11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.426300e+10</td>\n",
       "      <td>3.140200e+10</td>\n",
       "      <td>-9.642000e+09</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>-1.14</td>\n",
       "      <td>21.07</td>\n",
       "      <td>24.89</td>\n",
       "      <td>17.25</td>\n",
       "      <td>-0.0861</td>\n",
       "      <td>2.40</td>\n",
       "      <td>26.34</td>\n",
       "      <td>16.74</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.6897</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.66</td>\n",
       "      <td>GE</td>\n",
       "      <td>9.185031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quarter end        Assets  Current Assets   Liabilities  \\\n",
       "0   1996-03-31  2.302130e+11             0.0  1.979710e+11   \n",
       "1   1996-06-30  2.420810e+11             0.0  2.098380e+11   \n",
       "2   1996-09-30  2.491820e+11             0.0  2.161300e+11   \n",
       "3   1996-12-31  2.724020e+11             0.0  2.382700e+11   \n",
       "4   1997-03-31  2.700680e+11             0.0  2.364860e+11   \n",
       "5   1997-06-30  2.788970e+11             0.0  2.441410e+11   \n",
       "6   1997-09-30  2.853540e+11             0.0  2.485530e+11   \n",
       "7   1997-12-31  3.040120e+11             0.0  2.658920e+11   \n",
       "8   1998-03-31  3.097540e+11             0.0  2.712220e+11   \n",
       "9   1998-06-30  3.188820e+11             0.0  2.793370e+11   \n",
       "10  1998-09-30  3.345750e+11             0.0  2.932580e+11   \n",
       "11  1998-12-31  3.559350e+11             0.0  3.127800e+11   \n",
       "12  1999-03-31  3.617360e+11             0.0  3.182620e+11   \n",
       "13  1999-06-30  3.683830e+11             0.0  3.236600e+11   \n",
       "14  1999-09-30  3.802240e+11             0.0  3.350620e+11   \n",
       "15  1999-12-31  4.052000e+11             0.0  3.574290e+11   \n",
       "16  2000-03-31  4.216370e+11             0.0  3.722540e+11   \n",
       "17  2000-06-30  4.240400e+11             0.0  3.731820e+11   \n",
       "18  2000-09-30  4.311410e+11             0.0  3.785180e+11   \n",
       "19  2000-12-31  4.370060e+11             0.0  3.815780e+11   \n",
       "20  2001-03-31  4.379850e+11             0.0  3.821410e+11   \n",
       "21  2001-06-30  4.453470e+11             0.0  3.882160e+11   \n",
       "22  2001-09-30  4.600970e+11             0.0  4.016380e+11   \n",
       "23  2001-12-31  4.950230e+11             0.0  4.349840e+11   \n",
       "24  2002-03-31  5.007360e+11             0.0  4.403440e+11   \n",
       "25  2002-06-30  5.408880e+11             0.0  4.768400e+11   \n",
       "26  2002-09-30  5.555230e+11             0.0  4.878270e+11   \n",
       "27  2002-12-31  5.752440e+11             0.0  5.060650e+11   \n",
       "28  2003-03-31  5.836340e+11             0.0  5.117630e+11   \n",
       "29  2003-06-30  6.148570e+11             0.0  5.374330e+11   \n",
       "30  2003-09-30  6.269330e+11             0.0  5.486610e+11   \n",
       "31  2003-12-31  6.474830e+11             0.0  5.625230e+11   \n",
       "32  2004-03-31  6.621060e+11             0.0  5.693290e+11   \n",
       "33  2004-06-30  6.970850e+11             0.0  5.820720e+11   \n",
       "34  2004-09-30  7.046200e+11             0.0  5.865260e+11   \n",
       "35  2004-12-31  7.505070e+11             0.0  6.233030e+11   \n",
       "36  2005-03-31  7.522230e+11             0.0  6.207630e+11   \n",
       "37  2005-06-30  7.403620e+11             0.0  6.107760e+11   \n",
       "38  2005-09-30  6.626720e+11             0.0  5.403150e+11   \n",
       "39  2005-12-31  6.733210e+11             0.0  5.559160e+11   \n",
       "40  2006-03-31  6.748650e+11             0.0  5.591970e+11   \n",
       "41  2006-06-30  6.624370e+11             0.0  5.453710e+11   \n",
       "42  2006-09-30  6.820650e+11             0.0  5.624720e+11   \n",
       "43  2006-12-31  6.972390e+11             0.0  5.773470e+11   \n",
       "44  2007-03-31  7.140800e+11             0.0  5.920590e+11   \n",
       "45  2007-06-30  7.385330e+11             0.0  6.143190e+11   \n",
       "46  2007-09-30  7.617060e+11             0.0  6.413670e+11   \n",
       "47  2007-12-31  7.953370e+11             0.0  6.717740e+11   \n",
       "48  2008-03-31  8.338900e+11             0.0  7.096560e+11   \n",
       "49  2008-06-30  8.469880e+11             0.0  7.198080e+11   \n",
       "50  2008-09-30  8.295500e+11             0.0  7.081870e+11   \n",
       "51  2008-12-31  7.977690e+11             0.0  6.841570e+11   \n",
       "52  2009-03-31  7.608070e+11             0.0  6.513310e+11   \n",
       "53  2009-06-30  7.781680e+11             0.0  6.577090e+11   \n",
       "54  2009-09-30  7.878460e+11             0.0  6.620370e+11   \n",
       "55  2009-12-31  7.818180e+11             0.0  6.566820e+11   \n",
       "56  2010-03-31  7.773550e+11             0.0  6.544040e+11   \n",
       "57  2010-06-30  7.499300e+11             0.0  6.292050e+11   \n",
       "58  2010-09-30  7.588240e+11             0.0  6.382140e+11   \n",
       "59  2010-12-31  7.512160e+11             0.0  6.270180e+11   \n",
       "60  2011-03-31  7.278370e+11             0.0  6.020480e+11   \n",
       "61  2011-06-30  7.391300e+11             0.0  6.087330e+11   \n",
       "62  2011-09-30  7.377000e+11             0.0  6.109270e+11   \n",
       "63  2011-12-31  7.172420e+11             0.0  5.991080e+11   \n",
       "64  2012-03-31  7.077190e+11             0.0  5.861310e+11   \n",
       "65  2012-06-30  6.941210e+11             0.0  5.702220e+11   \n",
       "66  2012-09-30  6.988800e+11             0.0  5.708640e+11   \n",
       "67  2012-12-31  6.853280e+11             0.0  5.568580e+11   \n",
       "68  2013-03-31  6.706900e+11             0.0  5.416400e+11   \n",
       "69  2013-06-30  6.605410e+11             0.0  5.317300e+11   \n",
       "70  2013-09-30  6.614380e+11             0.0  5.323930e+11   \n",
       "71  2013-12-31  6.565600e+11             0.0  5.197770e+11   \n",
       "72  2014-03-31  6.522520e+11             0.0  5.142330e+11   \n",
       "73  2014-06-30  6.518690e+11             0.0  5.118020e+11   \n",
       "74  2014-09-30  6.500210e+11             0.0  5.065100e+11   \n",
       "75  2014-12-31  6.483490e+11             0.0  5.115160e+11   \n",
       "76  2015-03-31  6.176310e+11             0.0  5.003330e+11   \n",
       "77  2015-06-30  6.109260e+11             0.0  4.926470e+11   \n",
       "78  2015-09-30  5.813100e+11             0.0  4.613170e+11   \n",
       "79  2015-12-31  4.926920e+11             0.0  3.895820e+11   \n",
       "80  2016-03-31  4.621930e+11             0.0  3.664030e+11   \n",
       "81  2016-06-30  4.014610e+11             0.0  3.117080e+11   \n",
       "82  2016-09-30  3.876940e+11             0.0  3.010980e+11   \n",
       "83  2016-12-31  3.651830e+11             0.0  2.846680e+11   \n",
       "84  2017-03-31  3.516430e+11             0.0  2.724160e+11   \n",
       "85  2017-06-30  3.554730e+11             0.0  2.764980e+11   \n",
       "86  2017-09-30  3.780380e+11             0.0  2.805440e+11   \n",
       "87  2017-12-31  3.779450e+11             0.0  2.925610e+11   \n",
       "\n",
       "    Current Liabilities  Shareholders equity       Revenue      Earnings  \\\n",
       "0                   0.0         2.927100e+10  1.709800e+10  1.517000e+09   \n",
       "1                   0.0         2.929600e+10  1.906600e+10  1.908000e+09   \n",
       "2                   0.0         2.999900e+10  2.002100e+10  1.788000e+09   \n",
       "3                   0.0         3.112500e+10  2.299400e+10  2.067000e+09   \n",
       "4                   0.0         3.046400e+10  2.015700e+10  1.677000e+09   \n",
       "5                   0.0         3.162200e+10  2.199700e+10  2.162000e+09   \n",
       "6                   0.0         3.369600e+10  2.199100e+10  2.014000e+09   \n",
       "7                   0.0         3.443800e+10  2.669500e+10  2.350000e+09   \n",
       "8                   0.0         3.471900e+10  2.262600e+10  1.891000e+09   \n",
       "9                   0.0         3.558500e+10  2.507000e+10  2.450000e+09   \n",
       "10                  0.0         3.716500e+10  2.413600e+10  2.284000e+09   \n",
       "11                  0.0         3.888000e+10  2.863700e+10  2.671000e+09   \n",
       "12                  0.0         3.879300e+10  2.416500e+10  2.155000e+09   \n",
       "13                  0.0         3.961100e+10  2.741000e+10  2.820000e+09   \n",
       "14                  0.0         3.997300e+10  2.720000e+10  2.653000e+09   \n",
       "15                  0.0         4.255700e+10  3.285500e+10  3.089000e+09   \n",
       "16                  0.0         4.449800e+10  2.999600e+10  2.592000e+09   \n",
       "17                  0.0         4.592500e+10  3.286200e+10  3.378000e+09   \n",
       "18                  0.0         4.774500e+10  3.201400e+10  3.180000e+09   \n",
       "19                  0.0         5.049200e+10  3.498100e+10  3.585000e+09   \n",
       "20                  0.0         5.093000e+10  3.049300e+10  2.573000e+09   \n",
       "21                  0.0         5.224900e+10  3.197700e+10  3.897000e+09   \n",
       "22                  0.0         5.359700e+10  2.946800e+10  3.281000e+09   \n",
       "23                  0.0         5.482400e+10  3.397500e+10  3.933000e+09   \n",
       "24                  0.0         5.517100e+10  3.052100e+10  2.503000e+09   \n",
       "25                  0.0         5.872700e+10  3.321400e+10  4.426000e+09   \n",
       "26                  0.0         6.225800e+10  3.258500e+10  4.087000e+09   \n",
       "27                  0.0         6.370600e+10  3.537800e+10  3.102000e+09   \n",
       "28                  0.0         6.635800e+10  3.031900e+10  2.999000e+09   \n",
       "29                  0.0         7.196800e+10  3.337300e+10  3.794000e+09   \n",
       "30                  0.0         7.248100e+10  3.339400e+10  3.649000e+09   \n",
       "31                  0.0         7.918000e+10  3.710100e+10  4.560000e+09   \n",
       "32                  0.0         8.648600e+10  3.335000e+10  3.240000e+09   \n",
       "33                  0.0         9.828200e+10  3.703500e+10  3.924000e+09   \n",
       "34                  0.0         1.017770e+11  3.827200e+10  4.051000e+09   \n",
       "35                  0.0         1.108210e+11  4.420900e+10  5.604000e+09   \n",
       "36                  0.0         1.128720e+11  3.972600e+10  3.965000e+09   \n",
       "37                  0.0         1.123840e+11  4.155600e+10  4.647000e+09   \n",
       "38                  0.0         1.129810e+11  4.192700e+10  4.677000e+09   \n",
       "39                  0.0         1.093510e+11  2.703300e+10  3.422000e+09   \n",
       "40                  0.0         1.075250e+11  3.802900e+10  4.440000e+09   \n",
       "41                  0.0         1.087920e+11  4.004800e+10  4.946000e+09   \n",
       "42                  0.0         1.113820e+11  4.069300e+10  4.867000e+09   \n",
       "43                  0.0         1.123140e+11  4.462100e+10  6.576000e+09   \n",
       "44                  0.0         1.141690e+11  4.019500e+10  4.508000e+09   \n",
       "45                  0.0         1.170230e+11  4.231600e+10  5.420000e+09   \n",
       "46                  0.0         1.128310e+11  4.253400e+10  5.539000e+09   \n",
       "47                  0.0         1.155590e+11  4.769300e+10  6.741000e+09   \n",
       "48                  0.0         1.160000e+11  4.227300e+10  4.304000e+09   \n",
       "49                  0.0         1.183860e+11  4.689100e+10  5.072000e+09   \n",
       "50                  0.0         1.123270e+11  4.723400e+10  4.312000e+09   \n",
       "51                  0.0         1.046650e+11  4.611700e+10  3.722000e+09   \n",
       "52                  0.0         1.010190e+11  3.841100e+10  2.811000e+09   \n",
       "53                  0.0         1.121190e+11  3.908200e+10  2.671000e+09   \n",
       "54                  0.0         1.175290e+11  3.779900e+10  2.494000e+09   \n",
       "55                  0.0         1.172910e+11  4.149100e+10  3.049000e+09   \n",
       "56                  0.0         1.151950e+11  3.660500e+10  1.945000e+09   \n",
       "57                  0.0         1.139340e+11  3.744400e+10  3.109000e+09   \n",
       "58                  0.0         1.155360e+11  3.588800e+10  2.055000e+09   \n",
       "59                  0.0         1.189360e+11  4.027400e+10  4.535000e+09   \n",
       "60                  0.0         1.235350e+11  3.844800e+10  3.433000e+09   \n",
       "61                  0.0         1.280740e+11  3.562500e+10  3.764000e+09   \n",
       "62                  0.0         1.244610e+11  3.536700e+10  3.224000e+09   \n",
       "63                  0.0         1.164380e+11  3.786000e+10  3.730000e+09   \n",
       "64                  0.0         1.198670e+11  3.518200e+10  3.034000e+09   \n",
       "65                  0.0         1.201190e+11  3.650100e+10  3.105000e+09   \n",
       "66                  0.0         1.225520e+11  3.634900e+10  3.491000e+09   \n",
       "67                  0.0         1.230260e+11  3.932700e+10  4.011000e+09   \n",
       "68                  0.0         1.237140e+11  3.501000e+10  3.527000e+09   \n",
       "69                  0.0         1.225090e+11  3.512300e+10  3.133000e+09   \n",
       "70                  0.0         1.226920e+11  3.572500e+10  3.191000e+09   \n",
       "71                  0.0         1.305660e+11  4.018700e+10  3.206000e+09   \n",
       "72                  0.0         1.318360e+11  3.417800e+10  2.999000e+09   \n",
       "73                  0.0         1.340130e+11  3.623300e+10  3.545000e+09   \n",
       "74                  0.0         1.349980e+11  3.617400e+10  3.537000e+09   \n",
       "75                  0.0         1.281590e+11  4.200400e+10  5.152000e+09   \n",
       "76                  0.0         1.085600e+11  2.935600e+10 -1.357300e+10   \n",
       "77                  0.0         1.095030e+11  3.169500e+10 -1.360000e+09   \n",
       "78                  0.0         1.112050e+11  3.168000e+10  2.506000e+09   \n",
       "79                  0.0         9.827400e+10  2.465500e+10  6.301000e+09   \n",
       "80                  0.0         9.108700e+10  2.784500e+10  1.910000e+08   \n",
       "81                  0.0         8.499000e+10  3.349400e+10  2.890000e+09   \n",
       "82                  0.0         8.188200e+10  2.926600e+10  2.027000e+09   \n",
       "83                  0.0         7.582800e+10  3.308800e+10  3.723000e+09   \n",
       "84                  0.0         7.453400e+10  2.766000e+10  6.530000e+08   \n",
       "85                  0.0         7.414800e+10  2.955800e+10  1.367000e+09   \n",
       "86                  0.0         7.610500e+10  3.347200e+10  1.836000e+09   \n",
       "87                  0.0         6.426300e+10  3.140200e+10 -9.642000e+09   \n",
       "\n",
       "    EPS basic  EPS diluted  Price  Price high  Price low     ROE  P/B ratio  \\\n",
       "0        0.91         0.00  16.39       17.69      15.08  0.2329       5.52   \n",
       "1        1.15         0.00  14.16       15.33      12.98  0.2370       4.82   \n",
       "2        1.09         0.00  13.52       14.69      12.35  0.2396       4.57   \n",
       "3        1.25         0.00  12.50       13.42      11.58  0.2433       4.11   \n",
       "4        0.51         0.00  22.59       25.52      19.67  0.2462       7.15   \n",
       "5        0.66         0.00  22.66       24.88      20.44  0.2498       7.29   \n",
       "6        0.62         0.00  19.47       22.75      16.19  0.2496       6.03   \n",
       "7        0.71         0.00  17.02       18.06      15.98  0.2520       4.96   \n",
       "8        0.58         0.57  28.82       34.65      23.00  0.2504       8.19   \n",
       "9        0.75         0.74  28.25       32.29      24.21  0.2515       7.96   \n",
       "10       0.70         0.68  28.79       30.67      26.90  0.2530       7.89   \n",
       "11       0.81         0.81  26.32       29.21      23.42  0.2541       6.94   \n",
       "12       0.66         0.65  34.74       38.06      31.42  0.2542       8.77   \n",
       "13       0.86         0.85  36.21       39.15      33.27  0.2572       9.17   \n",
       "14       0.81         0.79  37.51       40.83      34.19  0.2620       9.31   \n",
       "15       0.94         0.93  45.69       53.17      38.21  0.2664      11.23   \n",
       "16       0.26         0.26  48.31       54.96      41.67  0.2677      11.23   \n",
       "17       0.34         0.33  51.83       55.98      47.69  0.2709      11.52   \n",
       "18       0.33         0.32  55.00       60.50      49.50  0.2709      11.85   \n",
       "19       0.36         0.36  53.56       59.94      47.19  0.2700      11.11   \n",
       "20       0.26         0.26  38.73       41.59      35.88  0.2607       7.62   \n",
       "21       0.44         0.43  38.92       49.59      28.25  0.2628       7.59   \n",
       "22       0.33         0.32  45.73       52.90      38.57  0.2574       8.69   \n",
       "23       0.39         0.40  41.98       47.99      35.98  0.2587       7.77   \n",
       "24       0.25         0.25  24.69       27.98      21.40  0.2523       4.47   \n",
       "25       0.45         0.44  28.00       32.98      23.02  0.2545       5.05   \n",
       "26       0.51         0.51  32.61       37.80      27.42  0.2589       5.53   \n",
       "27       0.31         0.31  38.17       41.84      34.49  0.2354       6.10   \n",
       "28       0.32         0.32  29.34       31.30      27.37  0.2328       4.61   \n",
       "29       0.38         0.38  29.66       32.42      26.90  0.2116       4.47   \n",
       "30       0.36         0.36  28.58       31.66      25.50  0.1974       3.98   \n",
       "31       0.50         0.49  24.65       28.00      21.30  0.2069       3.41   \n",
       "32       0.32         0.32  35.20       37.75      32.65  0.1966       4.48   \n",
       "33       0.38         0.38  32.98       34.53      31.42  0.1828       3.89   \n",
       "34       0.38         0.38  31.52       33.49      29.55  0.1725       3.39   \n",
       "35       0.53         0.52  31.73       34.57      28.88  0.1693       3.29   \n",
       "36       0.37         0.37  34.51       36.34      32.67  0.1656       3.30   \n",
       "37       0.44         0.44  34.31       35.78      32.85  0.1669       3.22   \n",
       "38       0.44         0.44  35.75       37.34      34.15  0.1683       3.37   \n",
       "39       0.33         0.32  35.92       36.89      34.95  0.1493       3.36   \n",
       "40       0.40         0.39  36.55       38.49      34.62  0.1554       3.48   \n",
       "41       0.48         0.47  33.86       35.65      32.06  0.1594       3.27   \n",
       "42       0.48         0.48  34.01       35.24      32.78  0.1618       3.23   \n",
       "43       0.63         0.65  33.92       35.63      32.21  0.1893       3.14   \n",
       "44       0.44         0.44  39.11       42.15      36.07  0.1871       3.58   \n",
       "45       0.53         0.53  39.14       42.07      36.20  0.1879       3.53   \n",
       "46       0.54         0.54  37.16       39.77      34.55  0.1932       3.25   \n",
       "47       0.70         0.69  36.09       38.28      33.90  0.1933       3.23   \n",
       "48       0.44         0.44  19.16       25.75      12.58  0.1908       1.66   \n",
       "49       0.51         0.51  26.27       30.39      22.16  0.1872       2.26   \n",
       "50       0.43         0.43  32.34       38.52      26.15  0.1768       2.72   \n",
       "51       0.42         0.35  34.70       37.74      31.65  0.1543       3.08   \n",
       "52       0.26         0.26  15.51       16.87      14.15  0.1452       1.57   \n",
       "53       0.24         0.24  14.01       17.52      10.50  0.1243       1.47   \n",
       "54       0.23         0.23  12.18       14.55       9.80  0.1054       1.15   \n",
       "55       0.28         0.28  11.55       17.24       5.87  0.0958       1.05   \n",
       "56       0.17         0.17  17.05       18.94      15.15  0.0853       1.55   \n",
       "57       0.28         0.28  16.98       19.70      14.27  0.0888       1.57   \n",
       "58       0.18         0.18  15.22       16.70      13.75  0.0854       1.43   \n",
       "59       0.43         0.43  17.06       18.49      15.63  0.0979       1.57   \n",
       "60       0.32         0.31  19.88       21.65      18.12  0.1088       1.78   \n",
       "61       0.35         0.35  19.41       20.85      17.97  0.1110       1.67   \n",
       "62       0.22         0.22  17.13       19.53      14.72  0.1119       1.42   \n",
       "63       0.35         0.35  16.15       18.28      14.02  0.1066       1.37   \n",
       "64       0.29         0.29  19.62       21.00      18.23  0.1047       1.78   \n",
       "65       0.29         0.29  19.43       20.84      18.02  0.1016       1.72   \n",
       "66       0.33         0.33  21.16       22.96      19.36  0.1116       1.86   \n",
       "67       0.38         0.38  21.52       23.18      19.87  0.1124       1.84   \n",
       "68       0.34         0.34  22.29       23.90      20.68  0.1155       1.88   \n",
       "69       0.30         0.30  22.78       24.45      21.11  0.1152       1.90   \n",
       "70       0.31         0.31  23.86       24.95      22.76  0.1127       1.98   \n",
       "71       0.33         0.32  25.80       28.09      23.50  0.1046       2.13   \n",
       "72       0.30         0.30  26.13       27.94      24.32  0.0987       2.01   \n",
       "73       0.35         0.35  26.48       27.53      25.43  0.0997       2.01   \n",
       "74       0.35         0.35  25.95       27.15      24.75  0.1000       1.94   \n",
       "75       0.51         0.50  25.40       27.10      23.69  0.1152       1.89   \n",
       "76      -1.35        -1.35  24.84       26.27      23.41 -0.0106       1.95   \n",
       "77      -0.13        -0.13  26.63       28.68      24.57 -0.0519       2.47   \n",
       "78       0.25         0.25  23.35       27.33      19.37 -0.0636       2.15   \n",
       "79       0.65         0.66  28.14       31.49      24.79 -0.0575       2.56   \n",
       "80      -0.01        -0.01  29.57       32.05      27.10  0.0715       2.81   \n",
       "81       0.30         0.30  31.04       33.02      29.06  0.1186       3.14   \n",
       "82       0.22         0.22   0.00        0.00       0.00  0.1226       0.00   \n",
       "83       0.40         0.39  30.29       32.38      28.19  0.0980       3.27   \n",
       "84       0.07         0.07  30.55       31.84      29.25  0.1121       3.52   \n",
       "85       0.14         0.13  28.66       30.54      26.79  0.0958       3.34   \n",
       "86       0.21         0.21  25.59       27.59      23.58  0.0951       2.99   \n",
       "87      -1.14        -1.14  21.07       24.89      17.25 -0.0861       2.40   \n",
       "\n",
       "    P/E ratio  Cumulative dividends per share  Dividend payout ratio  \\\n",
       "0       25.22                            0.61                 0.4321   \n",
       "1       21.24                            0.68                 0.4308   \n",
       "2       19.64                            0.76                 0.4302   \n",
       "3       17.61                            0.85                 0.4304   \n",
       "4       30.80                            0.93                 0.4327   \n",
       "5       30.15                            1.02                 0.4300   \n",
       "6       24.96                            1.11                 0.4296   \n",
       "7       21.14                            1.21                 0.4300   \n",
       "8       34.58                            1.31                 0.4343   \n",
       "9       33.11                            1.41                 0.4345   \n",
       "10      32.72                            1.51                 0.4359   \n",
       "11      29.24                            1.62                 0.4389   \n",
       "12      37.22                            1.74                 0.4443   \n",
       "13      37.72                            1.86                 0.4451   \n",
       "14      37.64                            1.97                 0.4454   \n",
       "15      44.22                            2.11                 0.4471   \n",
       "16      45.01                            2.24                 0.4422   \n",
       "17      46.41                            2.38                 0.4414   \n",
       "18      47.28                            2.52                 0.4421   \n",
       "19      43.90                            2.68                 0.4435   \n",
       "20      30.50                            2.84                 0.4680   \n",
       "21      30.65                            3.00                 0.4651   \n",
       "22      33.38                            3.16                 0.4766   \n",
       "23      30.64                            3.34                 0.4791   \n",
       "24      17.51                            3.52                 0.4962   \n",
       "25      20.00                            3.70                 0.4919   \n",
       "26      23.13                            3.88                 0.4789   \n",
       "27      23.86                            4.07                 0.5149   \n",
       "28      19.43                            4.26                 0.5050   \n",
       "29      18.77                            4.45                 0.5359   \n",
       "30      18.80                            4.64                 0.5618   \n",
       "31      17.99                            4.84                 0.5150   \n",
       "32      22.71                            5.04                 0.5163   \n",
       "33      21.28                            5.24                 0.5254   \n",
       "34      20.34                            5.44                 0.5252   \n",
       "35      20.21                            5.66                 0.5114   \n",
       "36      21.57                            5.88                 0.5068   \n",
       "37      20.79                            6.10                 0.4988   \n",
       "38      20.91                            6.32                 0.4934   \n",
       "39      20.29                            6.57                 0.5742   \n",
       "40      23.28                            6.82                 0.5739   \n",
       "41      21.30                            7.07                 0.5783   \n",
       "42      20.99                            7.32                 0.5863   \n",
       "43      20.43                            7.60                 0.5107   \n",
       "44      19.65                            7.88                 0.5225   \n",
       "45      19.19                            8.16                 0.5244   \n",
       "46      17.70                            8.44                 0.5198   \n",
       "47      16.71                            8.75                 0.5257   \n",
       "48       8.71                            9.06                 0.5401   \n",
       "49      11.94                            9.37                 0.5587   \n",
       "50      14.83                            9.68                 0.6048   \n",
       "51      16.76                            9.99                 0.7199   \n",
       "52       8.97                           10.30                 0.8033   \n",
       "53       9.04                           10.40                 0.8009   \n",
       "54       9.52                           10.50                 0.7569   \n",
       "55      10.69                           10.60                 0.6039   \n",
       "56      16.88                           10.70                 0.4323   \n",
       "57      18.46                           10.80                 0.4145   \n",
       "58      15.85                           10.92                 0.4547   \n",
       "59      18.75                           11.06                 0.4321   \n",
       "60      18.75                           11.20                 0.4145   \n",
       "61      16.18                           11.35                 0.4330   \n",
       "62      13.49                           11.50                 0.4437   \n",
       "63      12.33                           11.67                 0.4922   \n",
       "64      15.95                           11.84                 0.5293   \n",
       "65      16.06                           12.01                 0.5713   \n",
       "66      18.40                           12.18                 0.5372   \n",
       "67      17.08                           12.37                 0.5391   \n",
       "68      17.28                           12.56                 0.5319   \n",
       "69      17.00                           12.75                 0.5407   \n",
       "70      17.67                           12.94                 0.5625   \n",
       "71      19.40                           13.16                 0.6149   \n",
       "72      20.57                           13.38                 0.6601   \n",
       "73      21.53                           13.60                 0.6602   \n",
       "74      20.27                           13.82                 0.6646   \n",
       "75      19.24                           14.05                 0.5867   \n",
       "76      16.56                           14.28                 0.0000   \n",
       "77       0.00                           14.51                 0.0000   \n",
       "78       0.00                           14.74                 0.0000   \n",
       "79       0.00                           14.97                 0.0000   \n",
       "80       0.00                           15.20                 1.2153   \n",
       "81      40.31                           15.43                 0.7567   \n",
       "82       0.00                           15.66                 0.7656   \n",
       "83      25.89                           15.90                 1.0157   \n",
       "84      33.94                           16.14                 0.9304   \n",
       "85      29.24                           16.38                 1.1295   \n",
       "86      31.59                           16.62                 1.1667   \n",
       "87      26.34                           16.74                 0.0000   \n",
       "\n",
       "    Long-term debt to equity ratio  Net margin  Asset turnover  \\\n",
       "0                           1.7870      0.0933            0.33   \n",
       "1                           1.6990      0.0942            0.32   \n",
       "2                           1.6267      0.0932            0.32   \n",
       "3                           1.5822      0.0919            0.32   \n",
       "4                           1.5219      0.0905            0.32   \n",
       "5                           1.4797      0.0903            0.32   \n",
       "6                           1.4097      0.0909            0.31   \n",
       "7                           1.3532      0.0903            0.32   \n",
       "8                           1.2610      0.0902            0.32   \n",
       "9                           1.3704      0.0903            0.32   \n",
       "10                          1.5454      0.0911            0.31   \n",
       "11                          1.5345      0.0925            0.30   \n",
       "12                          1.5596      0.0937            0.30   \n",
       "13                          1.5362      0.0952            0.29   \n",
       "14                          1.6596      0.0959            0.29   \n",
       "15                          1.6784      0.0960            0.29   \n",
       "16                          1.6498      0.0950            0.30   \n",
       "17                          1.6898      0.0953            0.30   \n",
       "18                          1.5879      0.0958            0.30   \n",
       "19                          1.6266      0.0981            0.30   \n",
       "20                          1.5755      0.0976            0.30   \n",
       "21                          1.5051      0.1022            0.30   \n",
       "22                          1.5119      0.1051            0.29   \n",
       "23                          1.4557      0.1087            0.27   \n",
       "24                          1.7016      0.1081            0.26   \n",
       "25                          2.1320      0.1112            0.25   \n",
       "26                          2.1529      0.1147            0.25   \n",
       "27                          2.2075      0.1072            0.24   \n",
       "28                          2.3464      0.1111            0.23   \n",
       "29                          2.3483      0.1062            0.23   \n",
       "30                          2.2476      0.1022            0.22   \n",
       "31                          2.1471      0.1118            0.22   \n",
       "32                          1.9595      0.1111            0.22   \n",
       "33                          1.7873      0.1091            0.21   \n",
       "34                          1.8308      0.1082            0.22   \n",
       "35                          1.9190      0.1100            0.22   \n",
       "36                          1.8865      0.1102            0.22   \n",
       "37                          1.9541      0.1115            0.22   \n",
       "38                          1.9260      0.1128            0.23   \n",
       "39                          1.9413      0.1112            0.21   \n",
       "40                          2.0737      0.1157            0.22   \n",
       "41                          2.1779      0.1189            0.22   \n",
       "42                          2.1810      0.1212            0.22   \n",
       "43                          2.3221      0.1275            0.24   \n",
       "44                          2.4093      0.1262            0.24   \n",
       "45                          2.4165      0.1273            0.24   \n",
       "46                          2.7583      0.1299            0.23   \n",
       "47                          2.7606      0.1286            0.23   \n",
       "48                          2.9886      0.1259            0.22   \n",
       "49                          2.9671      0.1207            0.22   \n",
       "50                          2.9371      0.1110            0.22   \n",
       "51                          3.1536      0.0954            0.22   \n",
       "52                          3.2435      0.0887            0.22   \n",
       "53                          3.0273      0.0782            0.22   \n",
       "54                          3.0468      0.0711            0.21   \n",
       "55                          2.8836      0.0684            0.20   \n",
       "56                          2.7571      0.0636            0.20   \n",
       "57                          2.6217      0.0672            0.20   \n",
       "58                          2.6617      0.0651            0.20   \n",
       "59                          2.4662      0.0755            0.20   \n",
       "60                          2.3284      0.0844            0.20   \n",
       "61                          2.1695      0.0898            0.20   \n",
       "62                          2.1555      0.0925            0.20   \n",
       "63                          2.0909      0.0891            0.20   \n",
       "64                          1.9479      0.0888            0.20   \n",
       "65                          1.9132      0.0843            0.20   \n",
       "66                          1.9149      0.0916            0.21   \n",
       "67                          1.9190      0.0926            0.21   \n",
       "68                          1.8939      0.0960            0.21   \n",
       "69                          1.8879      0.0971            0.21   \n",
       "70                          1.8491      0.0955            0.22   \n",
       "71                          1.6977      0.0894            0.22   \n",
       "72                          1.6763      0.0863            0.22   \n",
       "73                          1.6181      0.0884            0.22   \n",
       "74                          1.5791      0.0905            0.22   \n",
       "75                          1.5638      0.1025            0.23   \n",
       "76                          1.7541      0.0000            0.22   \n",
       "77                          1.6927      0.0000            0.22   \n",
       "78                          1.6187      0.0000            0.22   \n",
       "79                          1.4786      0.0000            0.20   \n",
       "80                          1.4513      0.0633            0.22   \n",
       "81                          1.4415      0.0971            0.24   \n",
       "82                          1.4129      0.0947            0.26   \n",
       "83                          1.3859      0.0661            0.31   \n",
       "84                          1.3374      0.0720            0.33   \n",
       "85                          1.3983      0.0614            0.33   \n",
       "86                          1.4134      0.0577            0.34   \n",
       "87                          1.6897      0.0000            0.33   \n",
       "\n",
       "    Free cash flow per share ticker  Relative Return DJIA  \n",
       "0                       0.06     GE                   inf  \n",
       "1                      -0.20     GE              1.057477  \n",
       "2                       0.11     GE              3.823103  \n",
       "3                       1.19     GE              8.703655  \n",
       "4                       0.07     GE              2.860977  \n",
       "5                       0.30     GE             14.200151  \n",
       "6                      -0.00     GE              3.288564  \n",
       "7                       0.46     GE             -0.593827  \n",
       "8                      -0.02     GE             10.824884  \n",
       "9                      -0.19     GE              1.680509  \n",
       "10                      0.93     GE            -14.126668  \n",
       "11                      0.76     GE             14.495922  \n",
       "12                      0.27     GE              6.499350  \n",
       "13                      0.29     GE             10.840433  \n",
       "14                      0.21     GE             -6.095984  \n",
       "15                      0.79     GE             10.309037  \n",
       "16                     -0.11     GE             -5.209130  \n",
       "17                     -0.00     GE             -4.464226  \n",
       "18                      0.27     GE              1.967382  \n",
       "19                      1.41     GE              1.233964  \n",
       "20                      0.20     GE             -9.469013  \n",
       "21                      0.66     GE              5.942787  \n",
       "22                      0.35     GE            -18.528940  \n",
       "23                      1.20     GE             11.632211  \n",
       "24                      0.19     GE              3.264052  \n",
       "25                      0.33     GE            -12.422979  \n",
       "26                      0.76     GE            -21.586480  \n",
       "27                      0.94     GE              9.157952  \n",
       "28                      0.17     GE             -4.604385  \n",
       "29                      0.60     GE             11.065568  \n",
       "30                      0.86     GE              3.086155  \n",
       "31                      0.90     GE             11.139218  \n",
       "32                      0.55     GE             -0.500979  \n",
       "33                      0.51     GE              0.682274  \n",
       "34                      0.63     GE             -3.568084  \n",
       "35                      1.08     GE              6.523767  \n",
       "36                      0.65     GE             -2.570958  \n",
       "37                      0.23     GE             -2.232469  \n",
       "38                      1.02     GE              2.821215  \n",
       "39                      0.87     GE              1.393139  \n",
       "40                      0.35     GE              3.544488  \n",
       "41                      0.39     GE              0.293211  \n",
       "42                      0.71     GE              4.532616  \n",
       "43                      0.56     GE              6.288540  \n",
       "44                      0.37     GE             -0.727654  \n",
       "45                      1.11     GE              7.863396  \n",
       "46                      0.97     GE              3.454183  \n",
       "47                      1.15     GE             -4.784306  \n",
       "48                      0.63     GE             -8.639528  \n",
       "49                      0.91     GE             -7.671903  \n",
       "50                      0.88     GE             -4.370962  \n",
       "51                      1.84     GE            -23.561686  \n",
       "52                     -0.17     GE            -15.896465  \n",
       "53                      0.52     GE              9.824917  \n",
       "54                      0.76     GE             12.897010  \n",
       "55                      0.99     GE              6.812167  \n",
       "56                      0.69     GE              4.423824  \n",
       "57                      0.81     GE            -11.080510  \n",
       "58                      0.86     GE              9.295916  \n",
       "59                      0.79     GE              6.939804  \n",
       "60                      0.59     GE              6.189944  \n",
       "61                      0.55     GE              0.738461  \n",
       "62                      0.73     GE            -13.870858  \n",
       "63                      0.64     GE             10.617426  \n",
       "64                      0.47     GE              7.741934  \n",
       "65                      0.44     GE             -2.586918  \n",
       "66                      0.46     GE              4.234566  \n",
       "67                      0.77     GE             -2.524092  \n",
       "68                      0.26     GE             10.149276  \n",
       "69                      0.54     GE              2.242432  \n",
       "70                      0.38     GE              1.501969  \n",
       "71                      0.89     GE              8.810388  \n",
       "72                      0.28     GE             -0.710277  \n",
       "73                      0.37     GE              2.205994  \n",
       "74                      0.49     GE              1.249135  \n",
       "75                      1.46     GE              4.356110  \n",
       "76                      0.42     GE             -0.286166  \n",
       "77                      0.61     GE             -0.816783  \n",
       "78                      0.03     GE             -8.319882  \n",
       "79                      0.53     GE              6.749346  \n",
       "80                     -0.07     GE              1.521322  \n",
       "81                     -0.46     GE              1.415581  \n",
       "82                      0.13     GE              1.065528  \n",
       "83                      0.06     GE                   inf  \n",
       "84                     -0.04     GE              4.367149  \n",
       "85                      0.20     GE              3.153225  \n",
       "86                      0.10     GE              4.603687  \n",
       "87                      0.66     GE              9.185031  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "financial_data_relative_return[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
